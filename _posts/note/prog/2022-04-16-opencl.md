---
layout: post
title: "编程与调试 C++ -- OpenCL & CUDA"
author:
location: "珠海"
categories: ["编程与调试"]
tags: ["编程", "C/C++", "OpenCL"]
toc: true
toclistyle:
comments:
visibility:
mathjax:
mermaid:
glslcanvas:
codeprint:
---

并行计算。
1. CPU
    * 首先是指令集优化：SIMD (SSE, AVX, NEON)
    * 然后多线程跑：
        * OpenMP 标准
        * intel 搞的 TBB, oneTBB 等。
2. GPU
    * NVIDIA 搞的 CUDA
    * [OpenCL {% include relref_khronos.html %}](https://www.khronos.org/opencl/) 标准
        * Unlike 'GPU-only' APIs, such as Vulkan, OpenCL enables use of a diverse range of accelerators including multi-core CPUs, GPUs, DSPs, FPGAs and dedicated hardware such as inferencing engines.
    * DirectX 搞的 DirectCompute
    * 微软尝试从 C++ 语言级别搞的 C++ AMP
3. 机器学习多为 CUDA 而 挖矿程序多为 CUDA 和 OpenCL。


## CL_TARGET_OPENCL_VERSION

```cpp
/* Detect which version to target */
#if !defined(CL_TARGET_OPENCL_VERSION)
#pragma message("cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 300 (OpenCL 3.0)")
#define CL_TARGET_OPENCL_VERSION 300
#endif

/* cl_device_type - bitfield */
#define CL_DEVICE_TYPE_DEFAULT                      (1 << 0)
#define CL_DEVICE_TYPE_CPU                          (1 << 1)
#define CL_DEVICE_TYPE_GPU                          (1 << 2)
#define CL_DEVICE_TYPE_ACCELERATOR                  (1 << 3)
#ifdef CL_VERSION_1_2
#define CL_DEVICE_TYPE_CUSTOM                       (1 << 4)
#endif
#define CL_DEVICE_TYPE_ALL                          0xFFFFFFFF
```

* OpenCL 2.0 异构计算 [第三版] （Heterogeneous Computing with OpenCL 2.0）
    * <https://www.bookstack.cn/read/Heterogeneous-Computing-with-OpenCL-2.0/content-chapter2-2.2-chinese.md>
* OPENCL：并行世界的桥梁
    * <https://www.mql5.com/zh/articles/405>
* OPENCL：从朴素到更具深度的编程
    * <https://www.mql5.com/zh/articles/407>
{% include image.html url="/assets/images/220416-opencl/20220422120039.png" %}

```cpp
void test() {

    const cl_uint num_entries = 100;
    const size_t param_value_size = 100;

    cl_platform_id platforms[num_entries];
    cl_uint num_platforms = 0;
    clGetPlatformIDs(num_entries, platforms, &num_platforms);

    for (int i = 0; i < num_platforms; i++) {
        cl_platform_id platform = platforms[i];
        cl_platform_info param_name = CL_PLATFORM_NAME;

        char param_valuep[param_value_size];
        size_t param_value_size_ret = 0;

        clGetPlatformInfo(platform,
            param_name,
            param_value_size,
            param_valuep,
            &param_value_size_ret);

        cl_device_type device_type = CL_DEVICE_TYPE_ALL;
        cl_device_id devices[num_entries];
        cl_uint num_devices = 0;

        clGetDeviceIDs(platform,
            device_type,
            num_entries,
            devices,
            &num_devices);

        for (int j = 0; j < num_devices; j++) {

            cl_device_id device = devices[j];
            cl_device_info param_name = CL_DEVICE_NAME;

            char param_valued[param_value_size];
            size_t param_value_size_ret = 0;

            clGetDeviceInfo(device,
                param_name,
                param_value_size,
                param_valued,
                &param_value_size_ret);
            param_value_size_ret = 0; //
        }

    }
}
```

Intel(R) OpenCL HD Graphics
Intel(R) HD Graphics 630


## 存储模型

OpenCL 将设备中的内部存储器抽象成四层结构的存储模型：
1. **全局内存（global memory）**：同一个工作空间内的所有工作节点都可以进行读写，宿主机可以对其进行初始化，特点是存储容量大、访问速度慢。
2. **常量内存（constant memory）**：工作空间内所有工作节点都可以进行读操作，却不能进行写操作。由宿主机进行初始化，在 kernel 执行过程中保持不变。
3. **本地内存（local memory）**：同一个工作组中所有的工作节点都可以进行读写操作，对其他工作组内的工作节点不可见，不可以通过宿主机进行初始化。
4. **私有内存（private memory）**：工作节点的专属内存，对其他工作节点完全不可见，只能通过内核程序分配。

下表描述了宿主机和设备对内存的的分配和访问规则。
{% include image.html url="/assets/images/220416-opencl/20190225170333694.png" %}
{% include image.html url="/assets/images/220416-opencl/20190225170549160.png" %}

在运行 OpenCL 应用时，宿主机需要将待处理的数据送到 OpenCL 设备，OpenCL 设备运算完成后需要把结构返回给宿主机，这就需要在宿主机与 OpenCL 设备之间进行数据交互，这种交互有两种方式：拷贝数据法和内存映射法。
OpenCL 规定了一个松散的内存模型，它不保证所有的工作节点访问的内存状态是一致的，它只规定在一个工作节点内部访问内存必须是一致的；在工作组内，可以通过同步点来保证组内节点的内存访问一致性。在不同工作组的访问内存一致性上，OpenCL 不提供任何保证。


## OpenCL vs CUDA

OpenCL 和 nVidia CUDA 很像。

CUDA               | OpenCL
---- | ----
Thread             | Work-item
Thread block       | Work-group
global memory      | global memory
constant memory    | constant memory
shared memory      | local memory
local memory       | private memory
Grid size          | Global range
Block size         | Local range
\_\_global\_\_     | kernel
gridDim.x          | get_num_groups(0)
blockDim.x         | get_local_size(0)
blockIdx.x         | get_group_id(0)
threadIds.x        | get_local_id(0)
\_\_syncthreads    | barrier()
warp               | no equivalent


## OpenCL 基本原理

1. 准备 OpenCL 源码（C99）然后给 OpenCL。
2. OpenCL 针对目标设备，编译源码。
3. 向目标设备传输数据。（内存 到 显存）
4. 在数据上运行 kernel。（GPU 运行）
5. 把数据拖回来。（显存 到 内存）


## OpenCL C++ 伪码

```cpp
#include <cl/cl.hpp>
#include <vector>

using namespace cl;
using namespace std;

int main(int, char**)
{
    Platform platform = Platform::getDefault();

    vector<Device> devices;
    platform.getDevices(CL_DEVICE_TYPE_ALL, &devices);

    Context context(devices[0]);
    CommandQueue queue(context, devices[0]);

    Program program(context, "OpenCL C code goes here...");
    program.build();

    auto kernel = make_kernel<Buffer, Buffer>(program, "example_kernel");

    const static int Size = 1000000;
    vector<int> inputData(Size, 0), outputData(Size, 0);

    Buffer inputBuffer(context, CL_MEM_READ_ONLY, Size * sizeof(int));
    Buffer outputBuffer(context, CL_MEM_WRITE_ONLY, Size * sizeof(int));

    // 发送数据
    queue.enqueueWriteBuffer(inputBuffer, false, 0, Size * sizeof(int), inputData.data());

    // 运行 kernel
    kernel(EnqueueArgs(queue, NDRange(Size)), inputBuffer, outputBuffer);

    // 拖回数据
    queue.enqueueReadBuffer(outputBuffer, false, 0, Size * sizeof(int), outputData.data());

    queue.finish();

    return 0;
}
```


## 内核程序 OpenCL C

```c
kernel void example_kernel(global int * input, global int * output)
{
    int worker_id = get_global_id(0);

    output[worker_id] = input[worker_id] + 10 + worker_id;
}

#define MACRO(a, b) a + b

bool function(int a)
{
    float4 vector_type(0, 1, 2, 3); // 有的支持 SIMD

    vector_type *= 2;

    float v = vector_type.x; // 和 OpenGL 有点类似
    float2 v2; float8 v8; float16 v16;
    uchar uc; uint ui;

    local bool local_buffer[256]; // local 内存不能初始化。
    int lid = get_local_id(0);
    if (lid < 256)
        local_buffer[lid] = (uc8.S1 == uc);

    barrier(CLK_LOCAL_MEM_FENCE);

    if (lid < 256 && lid > 1)
        return local_buffer[lid - 1];

    return false;
}
```


## OpenCL 性能优化

* 提前编译一次，再多次使用。
* 数据传输，只传必要的，只需要的时候才传。
* 运算能力往往高于带宽，kernel 拷贝到 local array 快于读取 global memory。
    * **Global data access** Devices generally have more compute power than they have global memory bandwith
        kernels that read multiple values from global memory can be accelerated by copying the data in a local array
* 简单的多个 kernels 序列，性能不如一个大 kernel 直接一次运算。超大 kernel 可能会超出 硬件能力。
    * A sequence of simple kernels will perform less than one kernel doing all the calculations at once.
        But a very big kernel can suffer from private or local memory exhaustion on some devices and will have less performance
        (this is usually not a problem except for very complex algorithms).

* GPU 内存与 CPU 内存有所区别。利用 OpenCL 进行程序性能优化的主要目标，是确保最大化带宽，而非像在 CPU 上一样缩短延迟。
* 存储访问的本质，对于总线利用的效率影响巨大。总线使用率低即意味着运行速度低。
* 要改善代码的性能，存储访问最好是相干的。此外，最好也要避免库冲突。
* 硬件规格（总线宽度、存储库数量，以及可以合并为单一相干访问的线程数量）请见供应商提供的相关文档。


## 性能对比

实现图片模糊，对比性能。

E:\kpdf\fastimage\fastpdf-turbo\gpuip\build\test\Release\test_performance.exe
E:\kpdf\fastimage\fastpdf-turbo\gpuip\examples\images\bridge.exr
E:\kpdf\fastimage\fastpdf-turbo\gpuip\examples\kernels\

```
---------------------------------------------------------------
|                  LERP                                       |
---------------------------------------------------------------
CPU:    33.6 ms.
CPU MT: 216.8 ms.
OpenCL: 4.5 ms, Process 0.8 ms (16.8%), Copy 3.7 ms (83.2%)
GLSL:   23.8 ms, Process 0.7 ms (2.8%), Copy 23.1 ms (97.2%)
---------------------------------------------------------------
|                  BOX BLUR                                   |
---------------------------------------------------------------
CPU:    3491.4 ms.
CPU MT: 983.1 ms.
OpenCL: 5.7 ms, Process 4.9 ms (85.5%), Copy 0.8 ms (14.5%)
GLSL:   5.1 ms, Process -1.0 ms (-19.5%), Copy 6.1 ms (119.5%)
---------------------------------------------------------------
|                  GAUSSIAN BLUR                              |
---------------------------------------------------------------
CPU:    4601.4 ms.
CPU MT: 178.7 ms.
OpenCL: 9.2 ms, Process 8.5 ms (91.5%), Copy 0.8 ms (8.5%)
GLSL:   11.4 ms, Process -1.0 ms (-8.7%), Copy 12.4 ms (108.7%)
---------------------------------------------------------------
|                  SEPARABLE GAUSSIAN BLUR                    |
---------------------------------------------------------------
CPU:    838.0 ms.
CPU MT: 624.9 ms.
OpenCL: 1.8 ms, Process 1.1 ms (59.4%), Copy 0.7 ms (40.6%)
GLSL:   1.7 ms, Process -1.0 ms (-57.5%), Copy 2.7 ms (157.5%)
```

简单的事情 CPU MT 比 CPU 更慢；
GLSL 传输比 OpenCL 更慢。（也有可能是程序没写好 [惭愧]-_-）

普通指令集 vs 增强指令集

* LERP

CPU |    20.0 ms. | 9.4 ms.
CPU MT | 946.8 ms. | 660.6 ms.
OpenCL | 6.5 ms, Process 0.8 ms (12.0%), Copy 5.7 ms (88.0%) | 2.2 ms, Process 0.7 ms (31.1%), Copy 1.5 ms (68.9%)

* BOX BLUR

CPU |    1207.2 ms. | 1341.0 ms.
CPU MT | 619.2 ms. | 872.4 ms.
OpenCL | 5.9 ms, Process 5.0 ms (85.3%), Copy 0.9 ms (14.7%) | 5.6 ms, Process 4.8 ms (87.2%), Copy 0.7 ms (12.8%)

* GAUSSIAN BLUR

CPU |    1983.4 ms. | 3481.0 ms.
CPU MT | 96.4 ms. | 233.0 ms.
OpenCL | 9.4 ms, Process 8.6 ms (91.0%), Copy 0.8 ms (9.0%) | 9.4 ms, Process 8.6 ms (90.9%), Copy 0.9 ms (9.1%)

* SEPARABLE GAUSSIAN BLUR

CPU |    385.2 ms. | 813.4 ms.
CPU MT | 667.9 ms. | 773.2 ms.
OpenCL | 1.9 ms, Process 1.1 ms (59.2%), Copy 0.8 ms (40.8%) | 1.9 ms, Process 1.1 ms (56.7%), Copy 0.8 ms (43.3%)


## Notes

* C++ Wrapper for OpenCL : www.khronos.org/registry/cl/specs/opencl-cplusplus-1.2.pdf
* OpenCL Reference : www.khronos.org/registry/cl/sdk/1.2/docs/man/xhtml/
* Quick reference card : www.khronos.org/registry/cl/sdk/1.2/docs/OpenCL-1.2-refcard.pdf
* Complete OpenCL tutorial : www.cmsoft.com.br/index.php?option=com_content&view=category&layout=blog&id=41&Itemid=75



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2022-04-16-opencl.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://www.khronos.org/opencl/]({% include relrefx.html url="/backup/2022-04-16-opencl.md/www.khronos.org/299e65e9.html" %})
- [https://www.bookstack.cn/read/Heterogeneous-Computing-with-OpenCL-2.0/content-chapter2-2.2-chinese.md]({% include relrefx.html url="/backup/2022-04-16-opencl.md/www.bookstack.cn/52bc81a0.html" %})
- [https://www.mql5.com/zh/articles/405]({% include relrefx.html url="/backup/2022-04-16-opencl.md/www.mql5.com/6159139e.html" %})
- [https://www.mql5.com/zh/articles/407]({% include relrefx.html url="/backup/2022-04-16-opencl.md/www.mql5.com/2054dd8f.html" %})

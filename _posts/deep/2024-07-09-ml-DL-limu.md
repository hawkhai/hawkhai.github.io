---
layout: post
title: "机器学习 -- 跟李沐学 AI【论文精读】（进行中）"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle:
comments:
visibility:
mathjax: true
mermaid:
glslcanvas:
codeprint:
cluster: "机器学习课程"
---

AI 论文精读
<https://www.bilibili.com/video/BV1H44y1t75x/>

深度学习经典、新论文逐段精读
<https://github.com/mli/paper-reading>


## 如何读论文 06:39

1. title
2. abs
3. intro
4. method
5. exp
6. conclusion


## 9 年后重读深度学习奠基作之一：AlexNet【上】 18:59


## 9 年后重读深度学习奠基作之一：AlexNet【下】 55:21

ReLU 函数其实是分段线性函数，把所有的负值都变为 0，而正值不变，这种操作被成为单侧抑制。
可别小看这个简单的操作，正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。

因为中间断开了链接，所以泛化的结果是两部分解决不同的功能，提取不同类别的特征。
这么训应该不止颜色上面分开，应该有很多方面都是分开的，只是做视觉容易去关注颜色上的特征。


## 撑起计算机视觉半边天的 ResNet【上】 11:50


## ResNet 论文逐段精读 53:46

{% include image.html url="/assets/images/240709-ml-dl-limu/20240711001136.png" %}

{% include image.html url="/assets/images/240709-ml-dl-limu/20240711004306.png" caption="梯度保持的很好，训练更快。" %}

正常情况下，梯度应该在 0 附近的高斯分布。
有点像泰勒展开啊。


## Transformer 论文逐段精读

1:27:05


## 零基础多图详解图神经网络（GNN/GCN）

1:06:19
不知道这玩意有啥用。

需要有一定的机器学习的背景。
图是一个非常强大的东西，但是它的强大也带来了很多问题：很难在图上做出优化，图一般比较稀疏，有效的在 CPU、GPU、加速器上计算是一件比较难的事情；图神经网络对超参数比较敏感。
图神经网络门槛比较高，这些年吸引了很多人对他的研究，但在工业界上的应用还需要时间的积累。
很多图是交互图（既是优点（非常漂亮）又是缺点（门槛太高，有时用一个公式可以清晰的将其表达出来））。
1. 什么是图？图的属性应该用向量来进行表示。对于顶点、边、全局都用向量来表示它的属性。
2. 现实生活中的现象怎么表示成图，怎么对顶点、边、全局进行预测？
3. 机器学习的算法用到图上有什么挑战？
4. 定义 GNN：GNN 就是对属性做变换，但不改变图的结构。
5. 属性有缺失可以做聚合操作，把边的数据拿到结点上面，补足缺失的数据。
6. GNN：每一层里面通过汇聚操作，把信息传递过来，每个顶点看邻接顶点的信息；每个顶点看邻接边的信息或全局的信息。在每一层上如果能对信息进行充分的汇聚，那么 GNN 可以对图的结构进行一个发掘。
7. 实验：做了很多实验，可以看到参数对实验结果的影响。


## GAN 论文逐段精读

[数学基础 —— 生成模型必备知识 {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Hw4m117Ka/)

46:18
刺猬摸电线，卷麻了。
这么多年了，GAN、SVM 和 强化学习 的 数学部分，还是没有彻底吃透。

经典论文 GAN。一个分布，通过学习一个模型去近似，收敛多多少少要看运气。判断两个统计是否同一个分布，训练一个二分类分类器即可。

[生成对抗网络（GAN）的数学原理全解 {% include relref_weixin.html %}](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247528921&idx=2&sn=ba9d1de7a493d7f5492a3897a28d3406&chksm=ebb7490ddcc0c01b191f778ddff7fc2508cbd66ade755c4fdab87e61c6afb4f5c64905e31d31&scene=27)

训练一个 GAN 可能很困难，它经常会遇到各种问题，其中最主要的问题有以下三点：

* 消失梯度：这种情况经常发生，特别是当判别器太好时，这会阻碍生辰器的改进。使用最佳的判别器时，由于梯度的消失，训练可能失败，因此无法提供足够的信息给生成器改进。
* 模式塌缩：这是指生成器开始反复产生相同的输出（或一小组输出）的现象。如果判别器陷入局部最小值，那么下一个生成器迭代就很容易找到判别器最合理的输出。判别器永远无法学会走出陷阱。
* 收敛失败：由于许多因素（已知和未知），GANs 经常无法收敛。

* [生成对抗网络 —— 原理解释和数学推导 {% include relref_github.html %}](https://alberthg.github.io/2018/05/05/introduction-gan/)
* [大白话 10 分钟讲明白 GAN 的数学原理 {% include relref_bili.html %}](https://www.bilibili.com/video/BV1jh411472S/)

{% include image.html url="/assets/images/240709-ml-dl-limu/20240712200220.png" %}

{% include image.html url="/assets/images/240709-ml-dl-limu/20240714205735.jpg" %}

$$ \min_G \max_D V(D, G) = E_{x \sim p_{data}(x)} [\log D(x)] + E_{z \sim p_z(z)} [\log (1 - D(G(z)))] $$

- $V(D^*, G) = -\log(4)$，取其对应极大值。

$$
V(G, D) = \int_x p_{data}(x) \log D(x) dx + \int_z p_z(z) \log (1 - D(G(z))) dz \\
        = \int_x p_{data}(x) \log D(x) dx + p_g(x) \log (1 - D(x)) dx
$$

令 $f(y) = a \log(y) + b \log(1 - y)$

求导 $\frac{\partial}{\partial y} (a \log(y) + b \log(1 - y)) = 0$
$$
\frac{a}{y} + \frac{b}{1 - y} (-1) = 0 \\
a - ay = by \\
y = \frac{a}{a + b}
$$

所以 $D(x)^* = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$

当 $p_{data}(x) = p_g(x)$ 时，$D(x)^* = \frac{1}{2}$

二 .
$$
V(D, G) = E_{x \sim p_{data}(x)} [\log \frac{1}{2}] + E_{z \sim p_z(z)} [\log \frac{1}{2}] \\
        = - \log(2) - \log(2) = - 2 \log(2) \quad \text{ 取其对应极大值。}
$$

三 . 定义 KL 散度。任意两个分布 $P$ 和 $Q$ 的差异度量。

$$
D_{KL}(P||Q) = E_{x \sim P} [\log (\frac{P(x)}{Q(x)})] \\
            = E_{x \sim P} [\log (\frac{P(x)}{Q(x)})] - \log 2
$$

四 . 代入后，得：

$$
V(G, D) = - \log(4) + KL(\frac{p_{data} + p_g}{2} || \frac{p_{data} + p_g}{2}) + KL(\frac{p_g}{2} || \frac{p_{data} + p_g}{2}) \\
        \text{ 设 JS 散度 } JSD(P||Q) = \frac{1}{2} D_{KL}(P||\frac{P+Q}{2}) + \frac{1}{2} D_{KL}(Q||\frac{P+Q}{2}) \\
        \text{ 可从容地看出散度表示了这两个分布的重合度量。}
$$

$$
V(G, D) = - \log(4) + 2 JSD({p_{data}} || {p_g})
$$

最终目标，$\min_G V(D, G) = - \log(4)$，当且仅当 $p_{data} = p_g$。


## BERT 论文逐段精读

45:49

$$
P\left(w_i \mid w_1, \ldots, w_{i-1}, w_{i+1}, \ldots, w_n\right)
$$
$$
P\left(w_i \mid w_1, \ldots w_{i-1}\right) \text { 和 } P\left(w_i \mid w_{i+1}, \ldots w_n\right)
$$
{% include image.html url="/assets/images/240709-ml-dl-limu/20240720011254.png" %}


## ViT 论文逐段精读

1:11:31

感觉关键的是不要对原始数据做过多的处理，让 AI 自己去学习规律。
想法不值钱，没钱的 lab 也没法搞，所以最近几年好的模型都是谷歌微软推进的。

CNN 1. translation equivariance 2. locality.

一篇论文，内容如此饱满，做实验都要做好久好久。
{% include image.html url="/assets/images/240709-ml-dl-limu/v2-ecbd322402f99693c13d5ff7e9c66de0_1440w.png" %}

真的是 **力大飞砖** 。
Vision Transformer 挑战了 CNN 在 CV 中绝对的统治地位。
Vision Transformer 得出的结论是如果在足够多的数据上做预训练，
在不依赖 CNN 的基础上，直接用自然语言上的 Transformer 也能 CV 问题解决得很好。
Transformer 打破了 CV、NLP 之间的壁垒。


## MAE 论文逐段精读

{% include image.html url="/assets/images/240709-ml-dl-limu/20240801234818.png" %}

47:04
{% include image.html url="/assets/images/240709-ml-dl-limu/20240801224324.png" %}
Mu Li is scalable and efficient tutor

经典论文 2022 MAE。前沿 AI 距离普通人越来越远，以 MAE 这个“便宜”论文为例：
每次训练都需要使用 128 个 TPUv3 的核，训练一天以上，换算大概几千美金；
整个论文大概要做几十上百个实验，整体下来大概几十万美金。
AI 每次改进都有点反直觉，都是靠做实验进步的。

{% include image.html url="/assets/images/240709-ml-dl-limu/20240801232654.png" %}

增强不增强我不好说，但是我认为可以抑制过拟合。

* ImageNet，正态分布。
* JFT 数据集，长尾分布。


## 如何找研究想法 1

05:35
何恺明
涵盖 ResNet、Faster RCNN、Mask RCNN、MoCO 和 MAE。
- Residual Layer 是 LLM 的基本组成部分。
- Faster/Mask R-CNN 是图像分割和机器人感知领域的行业标准。
- Panoptic segmentation 重新定义了视觉研究的一个子领域。
- Mask AutoEncoder (MAE) 是最优秀的通用自监督算法，适用于计算机视觉等多个领域。
- 在 MAE 之前，Momentum Contrast (MoCo) 是最先进的对比学习技术。
- SlowFast 网络是视频学习的默认核心技术之一，直到 ViTs 的出现才慢慢取代了它。


### 暗通道先验去雾算法

暗通道去雾 <https://zhuanlan.zhihu.com/p/418174496>

年轻的时候我们有太多的纠结，好像走错了一步棋自己的人生就可能满盘皆输。
但其实我们中的大多数人都只是普通人，花了很多的力气来思考来做抉择，最终还是只能度过平凡的一生。
我们处在某个时间点上，总是想看到未来，却忘了最该做的是走好现在的路。
我在读研期间看书的时候，压根就没想到将来面试的时候考官会问到专业问题，但如果我没有认真学习专业知识，就一定回答不出考官的问题。
也许读完研你会觉得它没有卵用，但是不读研你未必就能成为一个有多少用的人。


## MoCo 论文逐段精读

1:24:11
{% include image.html url="/assets/images/240709-ml-dl-limu/20240919004358.png" %}
$$
\mathcal{L}_q=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^K \exp \left(q \cdot k_i / \tau\right)}
$$

**Momentum Contrast**
From the above perspective, contrastive learning is a wayof building a discrete dictionary on high-dimensional con.tinuous inputs such as images. The dictionary is dynamic inthe sense that the keys are randomly sampled, and that thekey encoder evolves during training. Our hypothesis is thatgood features can be learned by a large dictionary that cov-ers a rich set of negative samples, while the encoder for thedictionary keys is kept as consistent as possible despite itsevolution. Based on this motivation, we present MomentumContrast as described next.


## 对比学习论文综述

1:32:01

1. 百花齐放
2. CV 双雄，MoCo vs SimCLR。
3. 不用负样本
4. Transformer

InstDisc
{% include image.html url="/assets/images/240709-ml-dl-limu/20240919235259.png" %}


## AlphaFold2 论文精读预告

03:28


## Deepmind 用机器学习指导数学直觉论文逐段精读

52:51

发表在 Nature 的封面文章
AI 辅助直觉（AI-guided intuition）希望通过机器学习辅助发现纯数学的猜想和定理。

* 对于两个数学对象 X(z), Y(z) ，如果 机器学习 能够学到 f 使得 f(X(z)) 约等于 Y(z) ，说明 X 与 Y 之间有一定关系。
* 其中利用 归因技术（Attribution Techniques） 来辅助发现哪些特征更加重要。（归因技术：计算输入关于输出的梯度，梯度大说明该输入（特征）重要，梯度小说明该输入（特征）不重要）


## Swin Transformer 论文精读

Swin transformer 是 2021 年 ICCV 的 best paper。
才过去三年，AI 像过了一个世纪，现在看论文都有点考古的感觉了。
基础性的工作基本都是 Meta、Google、Microsoft、OpenAI 推动的，它们真的改变了世界。

<https://zhuanlan.zhihu.com/p/659117181>
[Swin Transformer 学习笔记 {% include relref_csdn.html %}](https://blog.csdn.net/qq_36936443/article/details/124296075)

1:00:22
{% include image.html url="/assets/images/240709-ml-dl-limu/v2-b79b20e96721887b381b8d2de9e23e21_720w.webp" caption="网络结构对比" %}

上图从左到右，我们可以看到：
* CNN：金字塔结构，网络越深，feature map 尺寸越小，channel 数越多。
* ViT：柱状结构，单一的 feature map 尺寸。
* Swin Transformer：金字塔结构，网络越深，feature map 尺寸越小，channel 数越多。

The Question about the mask of window attention #38
<https://github.com/microsoft/Swin-Transformer/issues/38>
{% include image.html url="/assets/images/240709-ml-dl-limu/145946639-f9da3139-a793-46aa-ba71-aadaa45706a7.png" %}

RegNet 由 Facebook 用 NAS 搜出的模型，Efficient Net 是 google 用 NAS 搜索出的模型性能都很好。之前的 VIT 由于没有用好的数据增强，而且缺少偏置归纳所以效果较差。
而 DeiT 用了更好的数据增强和模型蒸馏的方法效果更好。而 Swin 效果更好一些。
但与 Efficient Net 不分伯仲，虽然效果胜于 EffNet 但参数量计算速度不如 EffNet b 展示，先在更大的数据集上做预训练，在在 imageNet-1k 上微调做测试，会提升其性能。

* 局部性：同一个物体的不同部位或者是语义相近的不同物体还是大概率会出现在相连的地方。所以在 local 一个小范围的窗口去计算自注意力也是差不多够用的，全局计算对于视觉任务是有些浪费资源的。
* Patch Merging 十分类似于 Pixel Shuffle 的上采样的反过程。Pixel Shuffle 是 low level 任务中常用的上采样。

* NAS 神经网络架构搜索
    * 神经网络架构搜索是一种自动化的方法，用于搜索最佳的神经网络架构来解决特定的任务。
        它通过在给定的搜索空间中评估和优化不同的网络结构，来找到最适合的架构。
        神经网络架构搜索的目标是在给定的计算资源限制下，找到一个性能最佳的神经网络架构。
        它可以通过遗传算法、强化学习、贝叶斯优化等方法进行搜索。

在进行神经网络架构搜索时，以下是一些常见的步骤和考虑因素：

1. 定义搜索空间：确定神经网络架构的可选组件和参数范围，例如层数、节点数、卷积核大小等。
2. 评估策略：定义一个性能指标，用于评估每个网络结构的表现，例如准确率、速度等。
3. 搜索算法：选择合适的搜索算法来遍历搜索空间并找到最佳架构。常用的方法包括遗传算法、强化学习和贝叶斯优化等。
4. 训练和评估：对每个候选架构进行训练和评估，以确定它们在给定任务上的性能。
5. 迭代优化：根据评估结果调整搜索策略，并进行多次迭代以找到最佳架构。

[数学基础 —— 生成模型必备知识 {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Hw4m117Ka/)
<https://pytorch.org/vision/stable/models.html>
ConvNext
RegNet
EfficientNet

某种程度上可以说，Swin Transformer 就是披着 Transformer 皮的 CNN。
Swin Transformer 像 CNN 一样具有层级式结构，而且它的计算复杂度是跟输入图像的大小呈线性增长的。这种层级式结构使得它在下游任务上的表现非常好，如 Swin Transformerr 在 COCO 和 ADE20K 上的效果都非常的好，远远超越了之前最好的方法。

最大的创新就是使用了基于 Shifted Window 的自注意力，可以看到，相比于全局自注意力，它在有效地减少了的计算量的同时，还保持了很好的效果，因此对很多视觉的任务，尤其是对下游密集预测型的任务是非常有帮助的。

但是如果 Shifted Window 操作不能用到 NLP 领域里，其实在模型大一统上论据就不是那么强了，所以作者说接下来他们的未来工作就是要把 Shifted Windows 用到 NLP 里面（看了下 github，现在还没有实现），如果真的能做到这一点，那 Swin Transformer 真的就是一个里程碑式的工作了，NLP 和 CV 模型真正大一统的未来也就来了。


## 如何判断（你自己的）研究工作的价值

09:00
* 用有新意的方法
* 有效的解决
* 一个研究问题


## AlphaFold 2 论文精读

1:15:29
MSA row-wise gated self-attention with pair bias.

好复杂的模型。

突然想起来中科院张真人说过的一句话：如果你看到了科学的突破进展，只能说明你还没有搞懂发展历史。真正的科学进步都是一点一点堆起来的。


## 你（被）吐槽过论文不够 novel 吗？

14:11


## CLIP 论文逐段精读

1:38:26
我个人感觉，4 亿张图片的训练集就肯定或多或少包括了那 30 类数据集中相似的图片，就像 LLM 那种，我感觉这些 zero-shot 就其实都是早就 train 过的原因。

<https://blog.csdn.net/zcyzcyjava/article/details/127006287>
* Zero-shot learning -- 零样本学习
* One-shot learning -- 单样本学习
    *  Zero-shot learning 指的是我们之前没有这个类别的训练样本。但是我们可以学习到一个映射 $X->Y$。
        如果这个映射足够好的话，我们就可以处理没有看到的类了。
        One-shot learning 指的是我们在训练样本很少，甚至只有一个的情况下，依旧能做预测。
        这是如何做到呢？可以在一个大数据集上学到 general knowledge（具体的说，也可以是 $X->Y$ 的映射），
        然后再到小数据上有技巧的 update。
* Few-shot learning -- 小样本学习
    * 如果训练集中，不同类别的样本只有少量，则成为 Few-shot learning.
        就是给模型待预测类别的少量样本，然后让模型通过查看该类别的其他样本来预测该类别。
        比如：给小孩子看一张熊猫的照片，那么小孩子到动物园看见熊猫的照片之后，就可以识别出那是熊猫。

CLIP 为 CV 研究者打开了一片非常非常广阔的天地，原因在于 CLIP 真的把自然语言级别的抽象概念带到计算机视觉里了。当然肯定有其它类似工作，但 CLIP 跨过了奇点。

{% include image.html url="/assets/images/240709-ml-dl-limu/1_uFmAE4VGnCYa_aJe0CYdYw.webp" caption="数据集" %}

作者在 [Prompt_Engineering_for_ImageNet.ipynb {% include relref_github.html %}](https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb)
列出了使用的这 80 个 context prompts，比如有 "a photo of many {}" 适合包含多个物体的情况，"a photo of the hard to see {}" 可能适合一些小目标或比较难辨认的目标。
{% include image.html url="/assets/images/240709-ml-dl-limu/v2-d1f81a0700468720d77a061a005ab1e4_1440w.webp" %}

<ol>
<li>itap of a {}.</li>
<li>a bad photo of the {}.</li>
<li>a origami {}.</li>
<li>a photo of the large {}.</li>
<li>a {} in a video game.</li>
<li>art of the {}.</li>
<li>a photo of the small {}.</li>
</ol>

上面的 prompt template 是 a photo of a {}
Prompt engineering：避免歧义和与 NLP 那边保持一致，避免 distribution gap（分布误差）
Prompt ensembling：多用提示的模板 prompt template，然后综合一下选最高的
这个就是咒语。

Torch 计算 cross_entropy 时直接输入 label 的 id，它会先转成一个 one-hot 矩阵，np.range(n) 就会因此转为一个 n 维的单位矩阵，正好跟文本表征和图片表征的相似矩阵计算交叉熵。
[一文读懂三篇少样本微调 CLIP 的论文及代码实现细节 {% include relref_zhihu.html %}](https://www.zhihu.com/tardis/bd/art/670622153?source_id=1001)

**局限**
1. 还是没有 SOTA（跟领域最好的，包括有监督）
2. 某些数据集不好，有 bias
3. MNIST 很差，因为 4 亿里没有
4. 虽然 CLIP 可以做 zero-shot 的分类任务，但它还是在你给定的这些类别中去做选择
5. 4 亿还是太夸张了，自监督和伪标签
6. 调参和 tricks 还是用了很多
7. 通过自然语言引导图像分类器是一种灵活和通用的接口，但它有自己的局限性。
    许多复杂的任务和视觉概念可能很难仅仅通过文本来指导，即使用语言也无法描述。
8. 实验还是在监督的数据集做，对于 zero-shot 来说还是不好
9. 政治、性别、宗教
10. 当从 zero-shot 转换到设置 few-shot 时，当 one-shot、two-shot、four-shot 时反而不如 zero-shot，
    不提供训练样本时反而比提供少量训练样本时差了，这与人类的表现明显不同，
    人类的表现显示了从 zero-shot 到 one-shot 大幅增加。
    今后需要开展工作，让 CLIP 既在 zero-shot 表现很好，也能在 few-shot 表现很好。
    作者：鸡笋 Jeasun <https://www.bilibili.com/read/cv25773666/> 出处：bilibili

[中文字幕] OpenAI CLIP 论文解读
<https://www.bilibili.com/video/BV1Cv411h72S/>


## 双流网络论文逐段精读

52:57
视频理解。
Two-Stream Convolutional Networks for Action Recognition in Videos
* 空间流
* 时间流


## GPT，GPT-2，GPT-3 论文精读

1:29:59
{% include image.html url="/assets/images/240709-ml-dl-limu/f1d4be3d8e495ce8be0cb94c5511e762b7b22f20.jpg" %}

沐神关于做研究的启发：
做研究不要一条路走到黑，做过程你可以一条路走到黑，但是在做研究的时候，你要灵活一些，不要一条路走到黑。你需要尝试从一个新的角度来看问题。

gpt2 还是做语言模型，但是在做到下游任务的时候，会用一个叫做 zero-shot 的设定，zero-shot 是说，在做到下游任务的时候，不需要下游任务的任何标注信息，那么也不需要去重新训练已经预训练好的模型。这样子的好处是我只要训练好一个模型，在任何地方都可以用。
如果作者就是在 gpt1 的基础上用一个更大的数据集训练一个更大的模型，说我的结果比 Bert 好一些，可能也就好那么一点点，不是好那么多的情况下，大家会觉得 gpt2 这篇文章就没什么意思了，工程味特别重。那么我换一个角度，选择一个更难的问题，我说做 zero-shot。虽然结果可能没那么厉害了，没那么有优势，但是新意度一下就来了。


## OpenAI Codex 论文精读

47:59


## DeepMind AlphaCode 论文精读

44:01


## 斯坦福 2022 年 AI 指数报告精读

1:19:56


## I3D 论文精读

52:31


## 视频理解论文串讲（上）

51:15


## 参数服务器（Parameter Server）逐段精读

1:37:40


## 视频理解论文串讲（下）

1:08:32


## Pathways 论文精读

1:02:14


## GPipe 论文精读

58:48


## Megatron LM 论文精读

56:08


## DETR 论文精读

54:23


## Zero 论文精读

52:21


## 一 · 跟读者建立联系【论文写作】

45:02


## 明白问题的重要性【研究的艺术 · 二】

1:03:40


## DALL·E 2

1:27:55


## 讲好故事、论点【研究的艺术 · 三】

43:57


## 理由、论据和担保【研究的艺术 · 四】

44:14


## ViLT 论文精读

1:03:26


## CLIP 改进工作串讲（上）

1:15:43


## CLIP 改进工作串讲（下）

1:04:26


## Chain of Thought 论文、代码和资源

33:23


## OpenAI Whisper 精读

1:12:16


## 多模态论文串讲 · 上

1:12:25


## Neural Corpus Indexer 文档检索

55:47


## InstructGPT 论文精读

1:07:11


## 多模态论文串讲 · 下

1:03:29


## HELM 全面语言模型评测

1:23:38


## Anthropic LLM 论文精读【论文精读 ·51】

1:01:52


## 大模型时代下做科研的四个思路【论文精读 ·52】

1:06:29


## GPT-4 论文精读【论文精读 ·53】

1:20:39


## 深度学习论文精读


### 录制完成的论文

| 日期 | 标题 | 时长 | 视频（播放数） |
| --: | -- | -- | --: | -- |
| 3/30/23 | [GPT-4](https://openai.com/research/gpt-4) | 1:20:38 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1vM4y1U7b5) |
| 3/23/23 | 大模型时代下做科研的四个思路 | 1:06:29 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1oX4y1d7X6) |
| 3/10/23 | [Anthropic LLM] | 1:01:51 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1XY411B7nM) |
| 1/20/23 | [Helm] 全面语言模型评测 | 1:23:37 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1z24y1B7uX) |
| 1/11/23 | 多模态论文串讲 · 下 | 1:03:29 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1fA411Z772) |
| 12/29/22 | [Instruct GPT] | 1:07:10 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1hd4y187CR) |
| 12/19/22 | [Neural Corpus Indexer] 文档检索 | 55:47 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Se411w7Sn) |
| 12/12/22 | 多模态论文串讲 · 上 | 1:12:27 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Vd4y1v77v) |
| 11/14/22 | [OpenAI Whisper](https://cdn.openai.com/papers/whisper.pdf) 精读 | 1:12:16 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1VG4y1t74x) |
| 11/07/22 | 在讲 OpenAI Whisper 前先做了一个剪视频小工具 | 23:39 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Pe4y1t7de) |
| 10/23/22 | [Chain of Thought] 论文、代码和资源 | 33:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1t8411e7Ug) |
| 9/17/22 | CLIP 改进工作串讲（下） | 1:04:26 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1gg411U7n4) |
| 9/2/22 | CLIP 改进工作串讲（上） | 1:14:43 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1FV4y1p7Lm) |
| 7/29/22 | [ViLT] 论文精读 | 1:03:26 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV14r4y1j74y) |
| 7/22/22 | 理由、论据和担保【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 四】 | 44:14 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1SB4y1a75c) |
| 7/15/22 | 如何讲好故事、故事里的论点【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 三】 | 43:56 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1WB4y1v7ST) |
| 7/8/22 | [DALL·E 2] 逐段精读 | 1:27:54 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV17r4y1u77B) |
| 7/1/22 | 明白问题的重要性【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 二】 | 1:03:40 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV11S4y1v7S2/) |
| 6/24/22 | 跟读者建立联系【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 一】 | 45:01 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1hY411T7vy/) |
| 6/17/22 | [Zero] 逐段精读 | 52:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1tY411g7ZT/) |
| 6/10/22 | [DETR] 逐段精读 | 54:22 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1GB4y1X72R/) |
| 6/3/22 | [Megatron LM] 逐段精读 | 56:07 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1nB4y1R7Yz/) |
| 5/27/22 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) 逐段精读 | 58:47 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1v34y1E7zu/) |
| 5/5/22 | [Pathways] 逐段精读 | 1:02:13 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1xB4y1m7Xi/) |
| 4/28/22 | [视频理解论文串讲]（下） | 1:08:32 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV11Y411P7ep/) |
| 4/21/22 | [参数服务器（Parameter Server）](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) 逐段精读 | 1:37:40 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1YA4y197G8/) |
| 4/14/22 | [视频理解论文串讲]（上） | 51:15 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1fL4y157yA/) |
| 3/31/22 | [I3D] 论文精读 | 52:31 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1tY4y1p7hq/) |
| 3/24/22 | 斯坦福 2022 年 [AI 指数报告](https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf) 精读 | 1:19:56 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1s44y1N7eu/) |
| 3/17/22 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) 论文精读 | 44:00 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1ab4y1s7rc/) |
| 3/10/22 | [OpenAI Codex] 论文精读 | 47:58 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1iY41137Zi/) |
| 3/3/22 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3] 精读 | 1:29:58 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1AF411b7xQ/) |
| 2/24/22 | [Two-Stream](https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf) 逐段精读 | 52:57 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1mq4y1x7RU/) |
| 2/10/22 | [CLIP](https://openai.com/blog/clip/) 逐段精读 | 1:38:25 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1SL4y1s7LQ/) |
| 2/6/22 | 你（被）吐槽过 [论文不够 novel](https://perceiving-systems.blog/en/post/novelty-in-science) 吗？ | 14:11 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1ea41127Bq/) |
| 1/23/22 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) 精读 | 1:15:28 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1oR4y1K7Xr/) |
| 1/18/22 | 如何判断（你自己的）研究工作的价值 | 9:59 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1oL411c7Us/) |
| 1/15/22 | [Swin Transformer] 精读 | 1:00:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV13L4y1475U/) |
| 1/7/22 | [指导数学直觉](https://www.nature.com/articles/s41586-021-04086-x.pdf) | 52:51 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1YZ4y1S72j/) |
| 1/5/22 | AlphaFold 2 预告 | 03:28 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Eu411U7Te/) |
| 12/20/21 | [对比学习](#contrastive_learning) 论文综述 | 1:32:01 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV19S4y1M7hm/) |
| 12/15/21 | [MoCo] 逐段精读 | 1:24:11 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1C3411s7t9/) |
| 12/9/21 | 如何找研究想法 1 | 5:34 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1qq4y1z7F2/) |
| 12/8/21 | [MAE] 逐段精读 | 47:04 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1sq4y1q77t/) |
| 11/29/21 | [ViT] 逐段精读 | 1:11:30 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV15P4y137jb/) |
| 11/18/21 | [BERT] 逐段精读 | 45:49 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1PL411M7eQ/) |
| 11/9/21 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) 逐段精读 | 46:16 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1rb4y187vD/) |
| 11/3/21 | 零基础多图详解 [图神经网络](https://distill.pub/2021/gnn-intro/)（GNN/GCN） | 1:06:19 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1iT4y1d7zP/) |
| 10/27/21 | [Transformer] 逐段精读<br> （视频中提到的文献 [^transformer]) | 1:27:05 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1pu411o7BE/) |
| 10/22/21 | [ResNet] 论文逐段精读 | 53:46 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1P3411y7nn/) |
| 10/21/21 | 撑起计算机视觉半边天的 [ResNet] | 11:50 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Fb4y1h73E/) |
| 10/15/21 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) 论文逐段精读 | 55:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1hq4y157t1/) |
| 10/14/21 | 9 年后重读深度学习奠基作之一：[AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | 19:59 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1ih411J7Kz/) |
| 10/06/21 | 如何读论文 | 06:39 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1H44y1t75x/) |

[^transformer]: 1 [斯坦福 100+ 作者的 200+ 页综述]，2 [对 LayerNorm 的新研究]，3 [对 Attention 在 Transformer 里面作用的研究]


### 所有论文

包括已经录制完成和之后将要介绍的论文。选取的原则是 10 年内深度学习里有影响力文章（必读文章），或者近期比较有意思的文章。当然这十年里重要的工作太多了，不可能一一过一遍。在选取的时候我会偏向一些之前 [直播课](https://c.d2l.ai/zh-v2/) 中没讲到过的。 欢迎大家在 [讨论区 {% include relref_github.html %}](https://github.com/mli/paper-reading/discussions) 里提供建（点）议（歌）。

总论文数 67，录制完成数 32

（这里引用采用的是 semanticscholar，是因为它提供 [API](https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper) 可以自动获取，不用手动更新。）

#### 计算机视觉 - CNN

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2012 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | 深度学习热潮的奠基作 |
| | 2014 | [VGG] | 使用 3x3 卷积构造更深的网络 |
| | 2014 | [GoogleNet] | 使用并行架构构造更深的网络 |
| ✅ | 2015 | [ResNet] | 构建深层网络都要有的残差连接。 |
| | 2017 | [MobileNet] | 适合终端设备的小 CNN |
| | 2019 | [EfficientNet] | 通过架构搜索得到的 CNN |
| | 2021 | [Non-deep networks] | 让不深的网络也能在 ImageNet 刷到 SOTA |

#### 计算机视觉 - Transformer

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2020 | [ViT] | Transformer 杀入 CV 界 |
| ✅ | 2021 | [Swin Transformer] | 多层次的 Vision Transformer |
| | 2021 | [MLP-Mixer] | 使用 MLP 替换 self-attention |
| ✅ | 2021 | [MAE] | BERT 的 CV 版 |

#### 生成模型

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2014 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) | 生成模型的开创工作 |
| | 2015 | [DCGAN] | 使用 CNN 的 GAN |
| | 2016 | [pix2pix] | |
| | 2016 | [SRGAN] | 图片超分辨率 |
| | 2017 | [WGAN] | 训练更加容易 |
| | 2017 | [CycleGAN] | |
| | 2018 | [StyleGAN] | |
| | 2019 | [StyleGAN2] | |
| | 2020 | [DDPM] | Diffusion Models |
| | 2021 | [Improved DDPM] | 改进的 DDPM |
| | 2021 | [Guided Diffusion Models] | 号称超越 GAN |
| | 2021 | [StyleGAN3] | |
| ✅ | 2022 | [DALL.E 2] | CLIP + Diffusion models，文本生成图像新高度 |

#### 计算机视觉 - Object Detection

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| | 2014 | [R-CNN] | Two-stage |
| | 2015 | [Fast R-CNN] | |
| | 2015 | [Faster R-CNN] | |
| | 2016 | [SSD] | Single stage |
| | 2016 | [YOLO] | |
| | 2017 | [Mask R-CNN] | |
| | 2017 | [YOLOv2] | |
| | 2018 | [YOLOv3] | |
| | 2019 | [CenterNet] | Anchor free |
| ✅ | 2020 | [DETR] | Transformer |

<a name="contrastive_learning"></a>

#### 计算机视觉 - 对比学习

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2018 | [InstDisc] | 提出实例判别和 memory bank 做对比学习 |
| ✅ | 2018 | [CPC] | 对比预测编码，图像语音文本强化学习全都能做 |
| ✅ | 2019 | [InvaSpread] | 一个编码器的端到端对比学习 |
| ✅ | 2019 | [CMC] | 多视角下的对比学习 |
| ✅ | 2019 | [MoCov1] | 无监督训练效果也很好 |
| ✅ | 2020 | [SimCLRv1] | 简单的对比学习（数据增强 + MLP head + 大 batch 训练久） |
| ✅ | 2020 | [MoCov2] | MoCov1 + improvements from SimCLRv1 |
| ✅ | 2020 | [SimCLRv2] | 大的自监督预训练模型很适合做半监督学习 |
| ✅ | 2020 | [BYOL] | 不需要负样本的对比学习 |
| ✅ | 2020 | [SWaV] | 聚类对比学习 |
| ✅ | 2020 | [SimSiam] | 化繁为简的孪生表征学习 |
| ✅ | 2021 | [MoCov3] | 如何更稳定的自监督训练 ViT |
| ✅ | 2021 | [DINO] | transformer 加自监督在视觉也很香 |

#### 计算机视觉 - 视频理解

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2014 | [DeepVideo](https://cs.stanford.edu/people/karpathy/deepvideo/) | 提出 sports1M 数据集，用深度学习做视频理解 |
| ✅ | 2014 | [Two-stream] | 引入光流做时序建模，神经网络首次超越手工特征 |
| ✅ | 2014 | [C3D] | 比较深的 3D-CNN 做视频理解 |
| ✅ | 2015 | [Beyond-short-snippets] | 尝试使用 LSTM |
| ✅ | 2016 | [Convolutional fusion] | 做 early fusion 来加强时空间建模 |
| ✅ | 2016 | [TSN] | 超级有效的视频分段建模，bag of tricks in video |
| ✅ | 2017 | [I3D] | 提出 Kinetics 数据集，膨胀 2D 网络到 3D，开启 3D-CNN 时代 |
| ✅ | 2017 | [R2+1D] | 拆分 3D 卷积核，使 3D 网络容易优化 |
| ✅ | 2017 | [Non-local] | 引入自注意力做视觉问题 |
| ✅ | 2018 | [SlowFast] | 快慢两支提升效率 |
| ✅ | 2021 | [TimeSformer] | 视频中第一个引入 transformer，开启 video transformer 时代 |

#### 多模态学习

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2021 | [CLIP](https://openai.com/blog/clip/) | 图片和文本之间的对比学习 |
| ✅ | 2021 | [ViLT] | 第一个摆脱了目标检测的视觉文本模型 |
| ✅ | 2021 | [ViLD] | CLIP 蒸馏帮助开集目标检测 |
| ✅ | 2021 | [GLIP] | 联合目标检测和文本定位 |
| ✅ | 2021 | [CLIP4Clip] | 拿 CLIP 直接做视频文本 retrieval |
| ✅ | 2021 | [ActionCLIP] | 用多模态对比学习有监督的做视频动作分类 |
| ✅ | 2021 | [PointCLIP] | 3D 变 2D，巧妙利用 CLIP 做点云 |
| ✅ | 2022 | [LSeg] | 有监督的开集分割 |
| ✅ | 2022 | [GroupViT] | 只用图像文本对也能无监督做分割 |
| ✅ | 2022 | [CLIPasso] | CLIP 跨界生成简笔画 |
| ✅ | 2022 | [DepthCLIP] | 用文本跨界估计深度 |

#### 自然语言处理 - Transformer

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2017 | [Transformer] | 继 MLP、CNN、RNN 后的第四大类架构 |
| ✅ | 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | 使用 Transformer 解码器来做预训练 |
| ✅ | 2018 | [BERT] | Transformer 一统 NLP 的开始 |
| ✅ | 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | 更大的 GPT 模型，朝着 zero-shot learning 迈了一大步 |
| ✅ | 2020 | [GPT-3] | 100 倍更大的 GPT-2，few-shot learning 效果显著 |

#### 系统

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2014 | [参数服务器](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) | 支持千亿参数的传统机器学习模型 |
| ✅ | 2018 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) | 流水线（Pipeline）并行 |
| ✅ | 2019 | [Megatron-LM] | 张量（Tensor）并行 |
| ✅ | 2019 | [Zero] | 参数分片 |
| ✅ | 2022 | [Pathways] | 将 Jax 拓展到上千 TPU 核上 |

#### 图神经网络

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2021 | [图神经网络介绍](https://distill.pub/2021/gnn-intro/) | GNN 的可视化介绍 |

#### 优化算法

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| | 2014 | [Adam] | 深度学习里最常用的优化算法之一 |
| | 2016 | [为什么超大的模型泛化性不错] | |
| | 2017 | [为什么 Momentum 有效](https://distill.pub/2017/momentum/) | Distill 的可视化介绍 |

#### 新领域应用

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| | 2016 | [AlphaGo](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) | 强化学习出圈 |
| | 2020 | [AlphaFold](https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf) | 赢得比赛的的蛋白质 3D 结构预测 |
| ✅ | 2021 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) | 原子级别精度的蛋白质 3D 结构预测 |
| ✅ | 2021 | [Codex] | 使用注释生成代码 |
| ✅ | 2021 | [指导数学直觉](https://www.nature.com/articles/s41586-021-04086-x.pdf) | 分析不同数学物体之前的联系来帮助发现新定理 |
| ✅ | 2022 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) | 媲美一般程序员的编程解题水平 |



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2024-07-09-ml-DL-limu.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://www.bilibili.com/video/BV1H44y1t75x/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b19d9f0f.html" %})
- [https://github.com/mli/paper-reading]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/github.com/3a0488b8.html" %})
- [https://www.bilibili.com/video/BV1Hw4m117Ka/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/9d6cf4b5.html" %})
- [https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247528921&idx=2&sn=ba9d1de7a493d7f5492a3897a28d3406&chksm=ebb7490ddcc0c01b191f778ddff7fc2508cbd66ade755c4fdab87e61c6afb4f5c64905e31d31&scene=27]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/mp.weixin.qq.com/0689d438.html" %})
- [https://alberthg.github.io/2018/05/05/introduction-gan/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/alberthg.github.io/1304b71e.html" %})
- [https://www.bilibili.com/video/BV1jh411472S/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/14422865.html" %})
- [https://zhuanlan.zhihu.com/p/418174496]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/zhuanlan.zhihu.com/bec1f496.html" %})
- [https://zhuanlan.zhihu.com/p/659117181]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/zhuanlan.zhihu.com/3b8c64ee.html" %})
- [https://blog.csdn.net/qq_36936443/article/details/124296075]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/blog.csdn.net/09a11fba.html" %})
- [https://github.com/microsoft/Swin-Transformer/issues/38]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/github.com/d2642ed4.html" %})
- [https://pytorch.org/vision/stable/models.html]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/pytorch.org/8a94c8e4.html" %})
- [https://blog.csdn.net/zcyzcyjava/article/details/127006287]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/blog.csdn.net/30d6e725.html" %})
- [https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/github.com/4a098bdc.html" %})
- [https://www.zhihu.com/tardis/bd/art/670622153?source_id=1001]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.zhihu.com/0183fd92.html" %})
- [https://www.bilibili.com/read/cv25773666/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/bb90199b.html" %})
- [https://www.bilibili.com/video/BV1Cv411h72S/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/56293f2a.html" %})
- [https://openai.com/research/gpt-4]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/openai.com/a7961cae.html" %})
- [https://www.bilibili.com/video/BV1vM4y1U7b5]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c5fd7092.html" %})
- [https://www.bilibili.com/video/BV1oX4y1d7X6]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ba59fe1a.html" %})
- [https://www.bilibili.com/video/BV1XY411B7nM]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/4e2e011e.html" %})
- [https://www.bilibili.com/video/BV1z24y1B7uX]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/2c99292e.html" %})
- [https://www.bilibili.com/video/BV1fA411Z772]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ae6a3bb0.html" %})
- [https://www.bilibili.com/video/BV1hd4y187CR]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/070d7954.html" %})
- [https://www.bilibili.com/video/BV1Se411w7Sn]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/23ef66e6.html" %})
- [https://www.bilibili.com/video/BV1Vd4y1v77v]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/57ef66ef.html" %})
- [https://cdn.openai.com/papers/whisper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/cdn.openai.com/1ceb7c19.pdf" %})
- [https://www.bilibili.com/video/BV1VG4y1t74x]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/84094d7f.html" %})
- [https://www.bilibili.com/video/BV1Pe4y1t7de]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/a52a6652.html" %})
- [https://www.bilibili.com/video/BV1t8411e7Ug]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c07ebeee.html" %})
- [https://www.bilibili.com/video/BV1gg411U7n4]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/6d1ba03e.html" %})
- [https://www.bilibili.com/video/BV1FV4y1p7Lm]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/28e710fa.html" %})
- [https://www.bilibili.com/video/BV14r4y1j74y]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c5de9331.html" %})
- [https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/press.uchicago.edu/f62063ba.html" %})
- [https://www.bilibili.com/video/BV1SB4y1a75c]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/0f40e50e.html" %})
- [https://www.bilibili.com/video/BV1WB4y1v7ST]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c235ced7.html" %})
- [https://www.bilibili.com/video/BV17r4y1u77B]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/5b34aaf5.html" %})
- [https://www.bilibili.com/video/BV11S4y1v7S2/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ddcb7fae.html" %})
- [https://www.bilibili.com/video/BV1hY411T7vy/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/4aebc97a.html" %})
- [https://www.bilibili.com/video/BV1tY411g7ZT/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/261a36a5.html" %})
- [https://www.bilibili.com/video/BV1GB4y1X72R/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/2b5641b8.html" %})
- [https://www.bilibili.com/video/BV1nB4y1R7Yz/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/d6818f7d.html" %})
- [https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/proceedings.neurips.cc/e76803d4.pdf" %})
- [https://www.bilibili.com/video/BV1v34y1E7zu/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/d78f7174.html" %})
- [https://www.bilibili.com/video/BV1xB4y1m7Xi/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c797a1cb.html" %})
- [https://www.bilibili.com/video/BV11Y411P7ep/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/f8dfe44c.html" %})
- [https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.usenix.org/7bcee388.pdf" %})
- [https://www.bilibili.com/video/BV1YA4y197G8/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b0af4605.html" %})
- [https://www.bilibili.com/video/BV1fL4y157yA/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/217bd23f.html" %})
- [https://www.bilibili.com/video/BV1tY4y1p7hq/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ea99e42b.html" %})
- [https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/aiindex.stanford.edu/2c703978.pdf" %})
- [https://www.bilibili.com/video/BV1s44y1N7eu/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/eae7392e.html" %})
- [https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/storage.googleapis.com/41f5b475.pdf" %})
- [https://www.bilibili.com/video/BV1ab4y1s7rc/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/cfb701f2.html" %})
- [https://www.bilibili.com/video/BV1iY41137Zi/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/8050dc90.html" %})
- [https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/s3-us-west-2.amazonaws.com/96f09d50.pdf" %})
- [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/d4mucfpksywv.cloudfront.net/f5d4df52.pdf" %})
- [https://www.bilibili.com/video/BV1AF411b7xQ/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/90946f3b.html" %})
- [https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/proceedings.neurips.cc/96ca1767.pdf" %})
- [https://www.bilibili.com/video/BV1mq4y1x7RU/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/5253c75b.html" %})
- [https://openai.com/blog/clip/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/openai.com/10c417e2.html" %})
- [https://www.bilibili.com/video/BV1SL4y1s7LQ/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/4a9ea43b.html" %})
- [https://perceiving-systems.blog/en/post/novelty-in-science]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/perceiving-systems.blog/79692033.html" %})
- [https://www.bilibili.com/video/BV1ea41127Bq/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ac52a389.html" %})
- [https://www.nature.com/articles/s41586-021-03819-2.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.nature.com/a0b50e86.pdf" %})
- [https://www.bilibili.com/video/BV1oR4y1K7Xr/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/58aaede3.html" %})
- [https://www.bilibili.com/video/BV1oL411c7Us/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/fd36e0a8.html" %})
- [https://www.bilibili.com/video/BV13L4y1475U/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b9f72f91.html" %})
- [https://www.nature.com/articles/s41586-021-04086-x.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.nature.com/2053d51e.pdf" %})
- [https://www.bilibili.com/video/BV1YZ4y1S72j/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/cb76fb63.html" %})
- [https://www.bilibili.com/video/BV1Eu411U7Te/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/22e8db1f.html" %})
- [https://www.bilibili.com/video/BV19S4y1M7hm/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ada14565.html" %})
- [https://www.bilibili.com/video/BV1C3411s7t9/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/8f6ba93f.html" %})
- [https://www.bilibili.com/video/BV1qq4y1z7F2/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/724f621b.html" %})
- [https://www.bilibili.com/video/BV1sq4y1q77t/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/7e198c62.html" %})
- [https://www.bilibili.com/video/BV15P4y137jb/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/367433d9.html" %})
- [https://www.bilibili.com/video/BV1PL411M7eQ/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ceb3402d.html" %})
- [https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/papers.nips.cc/316cc9d3.pdf" %})
- [https://www.bilibili.com/video/BV1rb4y187vD/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/082bafb9.html" %})
- [https://distill.pub/2021/gnn-intro/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/distill.pub/1c86bfde.html" %})
- [https://www.bilibili.com/video/BV1iT4y1d7zP/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/10c5ee8f.html" %})
- [https://www.bilibili.com/video/BV1pu411o7BE/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/d4534f87.html" %})
- [https://www.bilibili.com/video/BV1P3411y7nn/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/600ca294.html" %})
- [https://www.bilibili.com/video/BV1Fb4y1h73E/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b97dae66.html" %})
- [https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/papers.nips.cc/29d938f1.pdf" %})
- [https://www.bilibili.com/video/BV1hq4y157t1/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/1a08ef6e.html" %})
- [https://www.bilibili.com/video/BV1ih411J7Kz/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/0363cb0e.html" %})
- [https://c.d2l.ai/zh-v2/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/c.d2l.ai/f774f1a4.html" %})
- [https://github.com/mli/paper-reading/discussions]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/github.com/4e37b659.html" %})
- [https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/api.semanticscholar.org/28134a4f.html" %})
- [https://cs.stanford.edu/people/karpathy/deepvideo/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/cs.stanford.edu/9df4fabd.html" %})
- [https://distill.pub/2017/momentum/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/distill.pub/5bd8c0cb.html" %})
- [https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/storage.googleapis.com/321c4317.pdf" %})
- [https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/discovery.ucl.ac.uk/e47bd3a4.pdf" %})

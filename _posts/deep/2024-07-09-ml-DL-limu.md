---
layout: post
title: "机器学习 -- 跟李沐学 AI【论文精读】（进行中）"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle:
comments:
visibility:
mathjax: true
mermaid:
glslcanvas:
codeprint:
cluster: "机器学习课程"
---

AI 论文精读
<https://www.bilibili.com/video/BV1H44y1t75x/>

深度学习经典、新论文逐段精读
<https://github.com/mli/paper-reading>


## 如何读论文 06:39

1. title
2. abs
3. intro
4. method
5. exp
6. conclusion


## 9 年后重读深度学习奠基作之一：AlexNet【上】 18:59


## 9 年后重读深度学习奠基作之一：AlexNet【下】 55:21

ReLU 函数其实是分段线性函数，把所有的负值都变为 0，而正值不变，这种操作被成为单侧抑制。
可别小看这个简单的操作，正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。

因为中间断开了链接，所以泛化的结果是两部分解决不同的功能，提取不同类别的特征。
这么训应该不止颜色上面分开，应该有很多方面都是分开的，只是做视觉容易去关注颜色上的特征。


## 撑起计算机视觉半边天的 ResNet【上】 11:50


## ResNet 论文逐段精读 53:46

{% include image.html url="/assets/images/240709-ml-dl-limu/20240711001136.png" %}

{% include image.html url="/assets/images/240709-ml-dl-limu/20240711004306.png" caption="梯度保持的很好，训练更快。" %}

正常情况下，梯度应该在 0 附近的高斯分布。
有点像泰勒展开啊。


## Transformer 论文逐段精读

1:27:05


## 零基础多图详解图神经网络（GNN/GCN）

1:06:19
不知道这玩意有啥用。

需要有一定的机器学习的背景。
图是一个非常强大的东西，但是它的强大也带来了很多问题：很难在图上做出优化，图一般比较稀疏，有效的在 CPU、GPU、加速器上计算是一件比较难的事情；图神经网络对超参数比较敏感。
图神经网络门槛比较高，这些年吸引了很多人对他的研究，但在工业界上的应用还需要时间的积累。
很多图是交互图（既是优点（非常漂亮）又是缺点（门槛太高，有时用一个公式可以清晰的将其表达出来））。
1. 什么是图？图的属性应该用向量来进行表示。对于顶点、边、全局都用向量来表示它的属性。
2. 现实生活中的现象怎么表示成图，怎么对顶点、边、全局进行预测？
3. 机器学习的算法用到图上有什么挑战？
4. 定义 GNN：GNN 就是对属性做变换，但不改变图的结构。
5. 属性有缺失可以做聚合操作，把边的数据拿到结点上面，补足缺失的数据。
6. GNN：每一层里面通过汇聚操作，把信息传递过来，每个顶点看邻接顶点的信息；每个顶点看邻接边的信息或全局的信息。在每一层上如果能对信息进行充分的汇聚，那么 GNN 可以对图的结构进行一个发掘。
7. 实验：做了很多实验，可以看到参数对实验结果的影响。


## GAN 论文逐段精读

46:18
刺猬摸电线，卷麻了。
这么多年了，GAN、SVM 和 强化学习 的 数学部分，还是没有彻底吃透。

经典论文 GAN。一个分布，通过学习一个模型去近似，收敛多多少少要看运气。判断两个统计是否同一个分布，训练一个二分类分类器即可。

[生成对抗网络（GAN）的数学原理全解 {% include relref_weixin.html %}](https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247528921&idx=2&sn=ba9d1de7a493d7f5492a3897a28d3406&chksm=ebb7490ddcc0c01b191f778ddff7fc2508cbd66ade755c4fdab87e61c6afb4f5c64905e31d31&scene=27)

训练一个 GAN 可能很困难，它经常会遇到各种问题，其中最主要的问题有以下三点：

* 消失梯度：这种情况经常发生，特别是当判别器太好时，这会阻碍生辰器的改进。使用最佳的判别器时，由于梯度的消失，训练可能失败，因此无法提供足够的信息给生成器改进。
* 模式塌缩：这是指生成器开始反复产生相同的输出（或一小组输出）的现象。如果判别器陷入局部最小值，那么下一个生成器迭代就很容易找到判别器最合理的输出。判别器永远无法学会走出陷阱。
* 收敛失败：由于许多因素（已知和未知），GANs 经常无法收敛。

* [生成对抗网络 —— 原理解释和数学推导 {% include relref_github.html %}](https://alberthg.github.io/2018/05/05/introduction-gan/)
* [大白话 10 分钟讲明白 GAN 的数学原理 {% include relref_bili.html %}](https://www.bilibili.com/video/BV1jh411472S/)

{% include image.html url="/assets/images/240709-ml-dl-limu/20240712200220.png" %}

{% include image.html url="/assets/images/240709-ml-dl-limu/20240714205735.jpg" %}

$$ \min_G \max_D V(D, G) = E_{x \sim p_{data}(x)} [\log D(x)] + E_{z \sim p_z(z)} [\log (1 - D(G(z)))] $$

- $V(D^*, G) = -\log(4)$，取其对应极大值。

$$
V(G, D) = \int_x p_{data}(x) \log D(x) dx + \int_z p_z(z) \log (1 - D(G(z))) dz \\
        = \int_x p_{data}(x) \log D(x) dx + p_g(x) \log (1 - D(x)) dx
$$

令 $f(y) = a \log(y) + b \log(1 - y)$

求导 $\frac{\partial}{\partial y} (a \log(y) + b \log(1 - y)) = 0$
$$
\frac{a}{y} + \frac{b}{1 - y} (-1) = 0 \\
a - ay = by \\
y = \frac{a}{a + b}
$$

所以 $D(x)^* = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$

当 $p_{data}(x) = p_g(x)$ 时，$D(x)^* = \frac{1}{2}$

二 .
$$
V(D, G) = E_{x \sim p_{data}(x)} [\log \frac{1}{2}] + E_{z \sim p_z(z)} [\log \frac{1}{2}] \\
        = - \log(2) - \log(2) = - 2 \log(2) \quad \text{ 取其对应极大值。}
$$

三 . 定义 KL 散度。任意两个分布 $P$ 和 $Q$ 的差异度量。

$$
D_{KL}(P||Q) = E_{x \sim P} [\log (\frac{P(x)}{Q(x)})] \\
            = E_{x \sim P} [\log (\frac{P(x)}{Q(x)})] - \log 2
$$

四 . 代入后，得：

$$
V(G, D) = - \log(4) + KL(\frac{p_{data} + p_g}{2} || \frac{p_{data} + p_g}{2}) + KL(\frac{p_g}{2} || \frac{p_{data} + p_g}{2}) \\
        \text{ 设 JS 散度 } JSD(P||Q) = \frac{1}{2} D_{KL}(P||\frac{P+Q}{2}) + \frac{1}{2} D_{KL}(Q||\frac{P+Q}{2}) \\
        \text{ 可从容地看出散度表示了这两个分布的重合度量。}
$$

$$
V(G, D) = - \log(4) + 2 JSD({p_{data}} || {p_g})
$$

最终目标，$\min_G V(D, G) = - \log(4)$，当且仅当 $p_{data} = p_g$。


## BERT 论文逐段精读

45:49

$$
P\left(w_i \mid w_1, \ldots, w_{i-1}, w_{i+1}, \ldots, w_n\right)
$$
$$
P\left(w_i \mid w_1, \ldots w_{i-1}\right) \text { 和 } P\left(w_i \mid w_{i+1}, \ldots w_n\right)
$$
{% include image.html url="/assets/images/240709-ml-dl-limu/20240720011254.png" %}


## ViT 论文逐段精读

1:11:31

感觉关键的是不要对原始数据做过多的处理，让 AI 自己去学习规律。
想法不值钱，没钱的 lab 也没法搞，所以最近几年好的模型都是谷歌微软推进的。

CNN 1. translation equivariance 2. locality.

一篇论文，内容如此饱满，做实验都要做好久好久。
{% include image.html url="/assets/images/240709-ml-dl-limu/v2-ecbd322402f99693c13d5ff7e9c66de0_1440w.png" %}

真的是 **力大飞砖** 。
Vision Transformer 挑战了 CNN 在 CV 中绝对的统治地位。
Vision Transformer 得出的结论是如果在足够多的数据上做预训练，
在不依赖 CNN 的基础上，直接用自然语言上的 Transformer 也能 CV 问题解决得很好。
Transformer 打破了 CV、NLP 之间的壁垒。


## MAE 论文逐段精读

{% include image.html url="/assets/images/240709-ml-dl-limu/20240801234818.png" %}

47:04
{% include image.html url="/assets/images/240709-ml-dl-limu/20240801224324.png" %}
Mu Li is scalable and efficient tutor

经典论文 2022 MAE。前沿 AI 距离普通人越来越远，以 MAE 这个“便宜”论文为例：
每次训练都需要使用 128 个 TPUv3 的核，训练一天以上，换算大概几千美金；
整个论文大概要做几十上百个实验，整体下来大概几十万美金。
AI 每次改进都有点反直觉，都是靠做实验进步的。

{% include image.html url="/assets/images/240709-ml-dl-limu/20240801232654.png" %}

增强不增强我不好说，但是我认为可以抑制过拟合。

* ImageNet，正态分布。
* JFT 数据集，长尾分布。


## 如何找研究想法 1

05:35
何恺明
涵盖 ResNet、Faster RCNN、Mask RCNN、MoCO 和 MAE。
- Residual Layer 是 LLM 的基本组成部分。
- Faster/Mask R-CNN 是图像分割和机器人感知领域的行业标准。
- Panoptic segmentation 重新定义了视觉研究的一个子领域。
- Mask AutoEncoder (MAE) 是最优秀的通用自监督算法，适用于计算机视觉等多个领域。
- 在 MAE 之前，Momentum Contrast (MoCo) 是最先进的对比学习技术。
- SlowFast 网络是视频学习的默认核心技术之一，直到 ViTs 的出现才慢慢取代了它。


### 暗通道先验去雾算法

暗通道去雾 <https://zhuanlan.zhihu.com/p/418174496>

年轻的时候我们有太多的纠结，好像走错了一步棋自己的人生就可能满盘皆输。
但其实我们中的大多数人都只是普通人，花了很多的力气来思考来做抉择，最终还是只能度过平凡的一生。
我们处在某个时间点上，总是想看到未来，却忘了最该做的是走好现在的路。
我在读研期间看书的时候，压根就没想到将来面试的时候考官会问到专业问题，但如果我没有认真学习专业知识，就一定回答不出考官的问题。
也许读完研你会觉得它没有卵用，但是不读研你未必就能成为一个有多少用的人。


## MoCo 论文逐段精读

1:24:11
{% include image.html url="/assets/images/240709-ml-dl-limu/20240919004358.png" %}
$$
\mathcal{L}_q=-\log \frac{\exp \left(q \cdot k_{+} / \tau\right)}{\sum_{i=0}^K \exp \left(q \cdot k_i / \tau\right)}
$$

**Momentum Contrast**
From the above perspective, contrastive learning is a wayof building a discrete dictionary on high-dimensional con.tinuous inputs such as images. The dictionary is dynamic inthe sense that the keys are randomly sampled, and that thekey encoder evolves during training. Our hypothesis is thatgood features can be learned by a large dictionary that cov-ers a rich set of negative samples, while the encoder for thedictionary keys is kept as consistent as possible despite itsevolution. Based on this motivation, we present MomentumContrast as described next.


## 对比学习论文综述

1:32:01

1. 百花齐放
2. CV 双雄，MoCo vs SimCLR。
3. 不用负样本
4. Transformer

InstDisc
{% include image.html url="/assets/images/240709-ml-dl-limu/20240919235259.png" %}


## AlphaFold2 论文精读预告

03:28


## Deepmind 用机器学习指导数学直觉论文逐段精读

52:51

发表在 Nature 的封面文章
AI 辅助直觉（AI-guided intuition）希望通过机器学习辅助发现纯数学的猜想和定理。

* 对于两个数学对象 X(z), Y(z) ，如果 机器学习 能够学到 f 使得 f(X(z)) 约等于 Y(z) ，说明 X 与 Y 之间有一定关系。
* 其中利用 归因技术（Attribution Techniques） 来辅助发现哪些特征更加重要。（归因技术：计算输入关于输出的梯度，梯度大说明该输入（特征）重要，梯度小说明该输入（特征）不重要）


## Swin Transformer 论文精读

1:00:22


## 如何判断（你自己的）研究工作的价值

09:00


## AlphaFold 2 论文精读

1:15:29


## 你（被）吐槽过论文不够 novel 吗？

14:11


## CLIP 论文逐段精读

1:38:26


## 双流网络论文逐段精读

52:57


## GPT，GPT-2，GPT-3 论文精读

1:29:59


## OpenAI Codex 论文精读

47:59


## DeepMind AlphaCode 论文精读

44:01


## 斯坦福 2022 年 AI 指数报告精读

1:19:56


## I3D 论文精读

52:31


## 视频理解论文串讲（上）

51:15


## 参数服务器（Parameter Server）逐段精读

1:37:40


## 视频理解论文串讲（下）

1:08:32


## Pathways 论文精读

1:02:14


## GPipe 论文精读

58:48


## Megatron LM 论文精读

56:08


## DETR 论文精读

54:23


## Zero 论文精读

52:21


## 一 · 跟读者建立联系【论文写作】

45:02


## 明白问题的重要性【研究的艺术 · 二】

1:03:40


## DALL·E 2

1:27:55


## 讲好故事、论点【研究的艺术 · 三】

43:57


## 理由、论据和担保【研究的艺术 · 四】

44:14


## ViLT 论文精读

1:03:26


## CLIP 改进工作串讲（上）

1:15:43


## CLIP 改进工作串讲（下）

1:04:26


## Chain of Thought 论文、代码和资源

33:23


## OpenAI Whisper 精读

1:12:16


## 多模态论文串讲 · 上

1:12:25


## Neural Corpus Indexer 文档检索

55:47


## InstructGPT 论文精读

1:07:11


## 多模态论文串讲 · 下

1:03:29


## HELM 全面语言模型评测

1:23:38


## Anthropic LLM 论文精读【论文精读 ·51】

1:01:52


## 大模型时代下做科研的四个思路【论文精读 ·52】

1:06:29


## GPT-4 论文精读【论文精读 ·53】

1:20:39


## 深度学习论文精读


### 录制完成的论文

| 日期 | 标题 | 时长 | 视频（播放数） |
| --: | -- | -- | --: | -- |
| 3/30/23 | [GPT-4](https://openai.com/research/gpt-4) | 1:20:38 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1vM4y1U7b5) |
| 3/23/23 | 大模型时代下做科研的四个思路 | 1:06:29 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1oX4y1d7X6) |
| 3/10/23 | [Anthropic LLM] | 1:01:51 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1XY411B7nM) |
| 1/20/23 | [Helm] 全面语言模型评测 | 1:23:37 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1z24y1B7uX) |
| 1/11/23 | 多模态论文串讲 · 下 | 1:03:29 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1fA411Z772) |
| 12/29/22 | [Instruct GPT] | 1:07:10 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1hd4y187CR) |
| 12/19/22 | [Neural Corpus Indexer] 文档检索 | 55:47 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Se411w7Sn) |
| 12/12/22 | 多模态论文串讲 · 上 | 1:12:27 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Vd4y1v77v) |
| 11/14/22 | [OpenAI Whisper](https://cdn.openai.com/papers/whisper.pdf) 精读 | 1:12:16 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1VG4y1t74x) |
| 11/07/22 | 在讲 OpenAI Whisper 前先做了一个剪视频小工具 | 23:39 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Pe4y1t7de) |
| 10/23/22 | [Chain of Thought] 论文、代码和资源 | 33:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1t8411e7Ug) |
| 9/17/22 | CLIP 改进工作串讲（下） | 1:04:26 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1gg411U7n4) |
| 9/2/22 | CLIP 改进工作串讲（上） | 1:14:43 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1FV4y1p7Lm) |
| 7/29/22 | [ViLT] 论文精读 | 1:03:26 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV14r4y1j74y) |
| 7/22/22 | 理由、论据和担保【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 四】 | 44:14 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1SB4y1a75c) |
| 7/15/22 | 如何讲好故事、故事里的论点【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 三】 | 43:56 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1WB4y1v7ST) |
| 7/8/22 | [DALL·E 2] 逐段精读 | 1:27:54 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV17r4y1u77B) |
| 7/1/22 | 明白问题的重要性【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 二】 | 1:03:40 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV11S4y1v7S2/) |
| 6/24/22 | 跟读者建立联系【[研究的艺术](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)· 一】 | 45:01 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1hY411T7vy/) |
| 6/17/22 | [Zero] 逐段精读 | 52:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1tY411g7ZT/) |
| 6/10/22 | [DETR] 逐段精读 | 54:22 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1GB4y1X72R/) |
| 6/3/22 | [Megatron LM] 逐段精读 | 56:07 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1nB4y1R7Yz/) |
| 5/27/22 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) 逐段精读 | 58:47 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1v34y1E7zu/) |
| 5/5/22 | [Pathways] 逐段精读 | 1:02:13 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1xB4y1m7Xi/) |
| 4/28/22 | [视频理解论文串讲]（下） | 1:08:32 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV11Y411P7ep/) |
| 4/21/22 | [参数服务器（Parameter Server）](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) 逐段精读 | 1:37:40 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1YA4y197G8/) |
| 4/14/22 | [视频理解论文串讲]（上） | 51:15 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1fL4y157yA/) |
| 3/31/22 | [I3D] 论文精读 | 52:31 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1tY4y1p7hq/) |
| 3/24/22 | 斯坦福 2022 年 [AI 指数报告](https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf) 精读 | 1:19:56 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1s44y1N7eu/) |
| 3/17/22 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) 论文精读 | 44:00 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1ab4y1s7rc/) |
| 3/10/22 | [OpenAI Codex] 论文精读 | 47:58 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1iY41137Zi/) |
| 3/3/22 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3] 精读 | 1:29:58 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1AF411b7xQ/) |
| 2/24/22 | [Two-Stream](https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf) 逐段精读 | 52:57 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1mq4y1x7RU/) |
| 2/10/22 | [CLIP](https://openai.com/blog/clip/) 逐段精读 | 1:38:25 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1SL4y1s7LQ/) |
| 2/6/22 | 你（被）吐槽过 [论文不够 novel](https://perceiving-systems.blog/en/post/novelty-in-science) 吗？ | 14:11 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1ea41127Bq/) |
| 1/23/22 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) 精读 | 1:15:28 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1oR4y1K7Xr/) |
| 1/18/22 | 如何判断（你自己的）研究工作的价值 | 9:59 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1oL411c7Us/) |
| 1/15/22 | [Swin Transformer] 精读 | 1:00:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV13L4y1475U/) |
| 1/7/22 | [指导数学直觉](https://www.nature.com/articles/s41586-021-04086-x.pdf) | 52:51 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1YZ4y1S72j/) |
| 1/5/22 | AlphaFold 2 预告 | 03:28 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Eu411U7Te/) |
| 12/20/21 | [对比学习](#contrastive_learning) 论文综述 | 1:32:01 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV19S4y1M7hm/) |
| 12/15/21 | [MoCo] 逐段精读 | 1:24:11 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1C3411s7t9/) |
| 12/9/21 | 如何找研究想法 1 | 5:34 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1qq4y1z7F2/) |
| 12/8/21 | [MAE] 逐段精读 | 47:04 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1sq4y1q77t/) |
| 11/29/21 | [ViT] 逐段精读 | 1:11:30 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV15P4y137jb/) |
| 11/18/21 | [BERT] 逐段精读 | 45:49 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1PL411M7eQ/) |
| 11/9/21 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) 逐段精读 | 46:16 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1rb4y187vD/) |
| 11/3/21 | 零基础多图详解 [图神经网络](https://distill.pub/2021/gnn-intro/)（GNN/GCN） | 1:06:19 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1iT4y1d7zP/) |
| 10/27/21 | [Transformer] 逐段精读<br> （视频中提到的文献 [^transformer]) | 1:27:05 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1pu411o7BE/) |
| 10/22/21 | [ResNet] 论文逐段精读 | 53:46 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1P3411y7nn/) |
| 10/21/21 | 撑起计算机视觉半边天的 [ResNet] | 11:50 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1Fb4y1h73E/) |
| 10/15/21 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) 论文逐段精读 | 55:21 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1hq4y157t1/) |
| 10/14/21 | 9 年后重读深度学习奠基作之一：[AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | 19:59 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1ih411J7Kz/) |
| 10/06/21 | 如何读论文 | 06:39 | [video {% include relref_bili.html %}](https://www.bilibili.com/video/BV1H44y1t75x/) |

[^transformer]: 1 [斯坦福 100+ 作者的 200+ 页综述]，2 [对 LayerNorm 的新研究]，3 [对 Attention 在 Transformer 里面作用的研究]


### 所有论文

包括已经录制完成和之后将要介绍的论文。选取的原则是 10 年内深度学习里有影响力文章（必读文章），或者近期比较有意思的文章。当然这十年里重要的工作太多了，不可能一一过一遍。在选取的时候我会偏向一些之前 [直播课](https://c.d2l.ai/zh-v2/) 中没讲到过的。 欢迎大家在 [讨论区 {% include relref_github.html %}](https://github.com/mli/paper-reading/discussions) 里提供建（点）议（歌）。

总论文数 67，录制完成数 32

（这里引用采用的是 semanticscholar，是因为它提供 [API](https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper) 可以自动获取，不用手动更新。）

#### 计算机视觉 - CNN

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2012 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | 深度学习热潮的奠基作 |
| | 2014 | [VGG] | 使用 3x3 卷积构造更深的网络 |
| | 2014 | [GoogleNet] | 使用并行架构构造更深的网络 |
| ✅ | 2015 | [ResNet] | 构建深层网络都要有的残差连接。 |
| | 2017 | [MobileNet] | 适合终端设备的小 CNN |
| | 2019 | [EfficientNet] | 通过架构搜索得到的 CNN |
| | 2021 | [Non-deep networks] | 让不深的网络也能在 ImageNet 刷到 SOTA |

#### 计算机视觉 - Transformer

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2020 | [ViT] | Transformer 杀入 CV 界 |
| ✅ | 2021 | [Swin Transformer] | 多层次的 Vision Transformer |
| | 2021 | [MLP-Mixer] | 使用 MLP 替换 self-attention |
| ✅ | 2021 | [MAE] | BERT 的 CV 版 |

#### 生成模型

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2014 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) | 生成模型的开创工作 |
| | 2015 | [DCGAN] | 使用 CNN 的 GAN |
| | 2016 | [pix2pix] | |
| | 2016 | [SRGAN] | 图片超分辨率 |
| | 2017 | [WGAN] | 训练更加容易 |
| | 2017 | [CycleGAN] | |
| | 2018 | [StyleGAN] | |
| | 2019 | [StyleGAN2] | |
| | 2020 | [DDPM] | Diffusion Models |
| | 2021 | [Improved DDPM] | 改进的 DDPM |
| | 2021 | [Guided Diffusion Models] | 号称超越 GAN |
| | 2021 | [StyleGAN3] | |
| ✅ | 2022 | [DALL.E 2] | CLIP + Diffusion models，文本生成图像新高度 |

#### 计算机视觉 - Object Detection

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| | 2014 | [R-CNN] | Two-stage |
| | 2015 | [Fast R-CNN] | |
| | 2015 | [Faster R-CNN] | |
| | 2016 | [SSD] | Single stage |
| | 2016 | [YOLO] | |
| | 2017 | [Mask R-CNN] | |
| | 2017 | [YOLOv2] | |
| | 2018 | [YOLOv3] | |
| | 2019 | [CenterNet] | Anchor free |
| ✅ | 2020 | [DETR] | Transformer |

<a name="contrastive_learning"></a>

#### 计算机视觉 - 对比学习

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2018 | [InstDisc] | 提出实例判别和 memory bank 做对比学习 |
| ✅ | 2018 | [CPC] | 对比预测编码，图像语音文本强化学习全都能做 |
| ✅ | 2019 | [InvaSpread] | 一个编码器的端到端对比学习 |
| ✅ | 2019 | [CMC] | 多视角下的对比学习 |
| ✅ | 2019 | [MoCov1] | 无监督训练效果也很好 |
| ✅ | 2020 | [SimCLRv1] | 简单的对比学习（数据增强 + MLP head + 大 batch 训练久） |
| ✅ | 2020 | [MoCov2] | MoCov1 + improvements from SimCLRv1 |
| ✅ | 2020 | [SimCLRv2] | 大的自监督预训练模型很适合做半监督学习 |
| ✅ | 2020 | [BYOL] | 不需要负样本的对比学习 |
| ✅ | 2020 | [SWaV] | 聚类对比学习 |
| ✅ | 2020 | [SimSiam] | 化繁为简的孪生表征学习 |
| ✅ | 2021 | [MoCov3] | 如何更稳定的自监督训练 ViT |
| ✅ | 2021 | [DINO] | transformer 加自监督在视觉也很香 |

#### 计算机视觉 - 视频理解

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2014 | [DeepVideo](https://cs.stanford.edu/people/karpathy/deepvideo/) | 提出 sports1M 数据集，用深度学习做视频理解 |
| ✅ | 2014 | [Two-stream] | 引入光流做时序建模，神经网络首次超越手工特征 |
| ✅ | 2014 | [C3D] | 比较深的 3D-CNN 做视频理解 |
| ✅ | 2015 | [Beyond-short-snippets] | 尝试使用 LSTM |
| ✅ | 2016 | [Convolutional fusion] | 做 early fusion 来加强时空间建模 |
| ✅ | 2016 | [TSN] | 超级有效的视频分段建模，bag of tricks in video |
| ✅ | 2017 | [I3D] | 提出 Kinetics 数据集，膨胀 2D 网络到 3D，开启 3D-CNN 时代 |
| ✅ | 2017 | [R2+1D] | 拆分 3D 卷积核，使 3D 网络容易优化 |
| ✅ | 2017 | [Non-local] | 引入自注意力做视觉问题 |
| ✅ | 2018 | [SlowFast] | 快慢两支提升效率 |
| ✅ | 2021 | [TimeSformer] | 视频中第一个引入 transformer，开启 video transformer 时代 |

#### 多模态学习

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2021 | [CLIP](https://openai.com/blog/clip/) | 图片和文本之间的对比学习 |
| ✅ | 2021 | [ViLT] | 第一个摆脱了目标检测的视觉文本模型 |
| ✅ | 2021 | [ViLD] | CLIP 蒸馏帮助开集目标检测 |
| ✅ | 2021 | [GLIP] | 联合目标检测和文本定位 |
| ✅ | 2021 | [CLIP4Clip] | 拿 CLIP 直接做视频文本 retrieval |
| ✅ | 2021 | [ActionCLIP] | 用多模态对比学习有监督的做视频动作分类 |
| ✅ | 2021 | [PointCLIP] | 3D 变 2D，巧妙利用 CLIP 做点云 |
| ✅ | 2022 | [LSeg] | 有监督的开集分割 |
| ✅ | 2022 | [GroupViT] | 只用图像文本对也能无监督做分割 |
| ✅ | 2022 | [CLIPasso] | CLIP 跨界生成简笔画 |
| ✅ | 2022 | [DepthCLIP] | 用文本跨界估计深度 |

#### 自然语言处理 - Transformer

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2017 | [Transformer] | 继 MLP、CNN、RNN 后的第四大类架构 |
| ✅ | 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | 使用 Transformer 解码器来做预训练 |
| ✅ | 2018 | [BERT] | Transformer 一统 NLP 的开始 |
| ✅ | 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | 更大的 GPT 模型，朝着 zero-shot learning 迈了一大步 |
| ✅ | 2020 | [GPT-3] | 100 倍更大的 GPT-2，few-shot learning 效果显著 |

#### 系统

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2014 | [参数服务器](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) | 支持千亿参数的传统机器学习模型 |
| ✅ | 2018 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) | 流水线（Pipeline）并行 |
| ✅ | 2019 | [Megatron-LM] | 张量（Tensor）并行 |
| ✅ | 2019 | [Zero] | 参数分片 |
| ✅ | 2022 | [Pathways] | 将 Jax 拓展到上千 TPU 核上 |

#### 图神经网络

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| ✅ | 2021 | [图神经网络介绍](https://distill.pub/2021/gnn-intro/) | GNN 的可视化介绍 |

#### 优化算法

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| | 2014 | [Adam] | 深度学习里最常用的优化算法之一 |
| | 2016 | [为什么超大的模型泛化性不错] | |
| | 2017 | [为什么 Momentum 有效](https://distill.pub/2017/momentum/) | Distill 的可视化介绍 |

#### 新领域应用

| 已录制 | 年份 | 名字 | 简介 |
| ---- | ---- | ---- | ---- | ---- |
| | 2016 | [AlphaGo](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) | 强化学习出圈 |
| | 2020 | [AlphaFold](https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf) | 赢得比赛的的蛋白质 3D 结构预测 |
| ✅ | 2021 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) | 原子级别精度的蛋白质 3D 结构预测 |
| ✅ | 2021 | [Codex] | 使用注释生成代码 |
| ✅ | 2021 | [指导数学直觉](https://www.nature.com/articles/s41586-021-04086-x.pdf) | 分析不同数学物体之前的联系来帮助发现新定理 |
| ✅ | 2022 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) | 媲美一般程序员的编程解题水平 |



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2024-07-09-ml-DL-limu.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://www.bilibili.com/video/BV1H44y1t75x/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b19d9f0f.html" %})
- [https://github.com/mli/paper-reading]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/github.com/3a0488b8.html" %})
- [https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247528921&idx=2&sn=ba9d1de7a493d7f5492a3897a28d3406&chksm=ebb7490ddcc0c01b191f778ddff7fc2508cbd66ade755c4fdab87e61c6afb4f5c64905e31d31&scene=27]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/mp.weixin.qq.com/0689d438.html" %})
- [https://alberthg.github.io/2018/05/05/introduction-gan/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/alberthg.github.io/1304b71e.html" %})
- [https://www.bilibili.com/video/BV1jh411472S/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/14422865.html" %})
- [https://zhuanlan.zhihu.com/p/418174496]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/zhuanlan.zhihu.com/bec1f496.html" %})
- [https://openai.com/research/gpt-4]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/openai.com/a7961cae.html" %})
- [https://www.bilibili.com/video/BV1vM4y1U7b5]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c5fd7092.html" %})
- [https://www.bilibili.com/video/BV1oX4y1d7X6]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ba59fe1a.html" %})
- [https://www.bilibili.com/video/BV1XY411B7nM]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/4e2e011e.html" %})
- [https://www.bilibili.com/video/BV1z24y1B7uX]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/2c99292e.html" %})
- [https://www.bilibili.com/video/BV1fA411Z772]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ae6a3bb0.html" %})
- [https://www.bilibili.com/video/BV1hd4y187CR]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/070d7954.html" %})
- [https://www.bilibili.com/video/BV1Se411w7Sn]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/23ef66e6.html" %})
- [https://www.bilibili.com/video/BV1Vd4y1v77v]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/57ef66ef.html" %})
- [https://cdn.openai.com/papers/whisper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/cdn.openai.com/1ceb7c19.pdf" %})
- [https://www.bilibili.com/video/BV1VG4y1t74x]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/84094d7f.html" %})
- [https://www.bilibili.com/video/BV1Pe4y1t7de]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/a52a6652.html" %})
- [https://www.bilibili.com/video/BV1t8411e7Ug]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c07ebeee.html" %})
- [https://www.bilibili.com/video/BV1gg411U7n4]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/6d1ba03e.html" %})
- [https://www.bilibili.com/video/BV1FV4y1p7Lm]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/28e710fa.html" %})
- [https://www.bilibili.com/video/BV14r4y1j74y]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c5de9331.html" %})
- [https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/press.uchicago.edu/f62063ba.html" %})
- [https://www.bilibili.com/video/BV1SB4y1a75c]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/0f40e50e.html" %})
- [https://www.bilibili.com/video/BV1WB4y1v7ST]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c235ced7.html" %})
- [https://www.bilibili.com/video/BV17r4y1u77B]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/5b34aaf5.html" %})
- [https://www.bilibili.com/video/BV11S4y1v7S2/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ddcb7fae.html" %})
- [https://www.bilibili.com/video/BV1hY411T7vy/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/4aebc97a.html" %})
- [https://www.bilibili.com/video/BV1tY411g7ZT/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/261a36a5.html" %})
- [https://www.bilibili.com/video/BV1GB4y1X72R/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/2b5641b8.html" %})
- [https://www.bilibili.com/video/BV1nB4y1R7Yz/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/d6818f7d.html" %})
- [https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/proceedings.neurips.cc/e76803d4.pdf" %})
- [https://www.bilibili.com/video/BV1v34y1E7zu/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/d78f7174.html" %})
- [https://www.bilibili.com/video/BV1xB4y1m7Xi/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/c797a1cb.html" %})
- [https://www.bilibili.com/video/BV11Y411P7ep/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/f8dfe44c.html" %})
- [https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.usenix.org/7bcee388.pdf" %})
- [https://www.bilibili.com/video/BV1YA4y197G8/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b0af4605.html" %})
- [https://www.bilibili.com/video/BV1fL4y157yA/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/217bd23f.html" %})
- [https://www.bilibili.com/video/BV1tY4y1p7hq/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ea99e42b.html" %})
- [https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/aiindex.stanford.edu/2c703978.pdf" %})
- [https://www.bilibili.com/video/BV1s44y1N7eu/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/eae7392e.html" %})
- [https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/storage.googleapis.com/41f5b475.pdf" %})
- [https://www.bilibili.com/video/BV1ab4y1s7rc/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/cfb701f2.html" %})
- [https://www.bilibili.com/video/BV1iY41137Zi/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/8050dc90.html" %})
- [https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/s3-us-west-2.amazonaws.com/96f09d50.pdf" %})
- [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/d4mucfpksywv.cloudfront.net/f5d4df52.pdf" %})
- [https://www.bilibili.com/video/BV1AF411b7xQ/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/90946f3b.html" %})
- [https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/proceedings.neurips.cc/96ca1767.pdf" %})
- [https://www.bilibili.com/video/BV1mq4y1x7RU/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/5253c75b.html" %})
- [https://openai.com/blog/clip/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/openai.com/10c417e2.html" %})
- [https://www.bilibili.com/video/BV1SL4y1s7LQ/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/4a9ea43b.html" %})
- [https://perceiving-systems.blog/en/post/novelty-in-science]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/perceiving-systems.blog/79692033.html" %})
- [https://www.bilibili.com/video/BV1ea41127Bq/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ac52a389.html" %})
- [https://www.nature.com/articles/s41586-021-03819-2.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.nature.com/a0b50e86.pdf" %})
- [https://www.bilibili.com/video/BV1oR4y1K7Xr/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/58aaede3.html" %})
- [https://www.bilibili.com/video/BV1oL411c7Us/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/fd36e0a8.html" %})
- [https://www.bilibili.com/video/BV13L4y1475U/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b9f72f91.html" %})
- [https://www.nature.com/articles/s41586-021-04086-x.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.nature.com/2053d51e.pdf" %})
- [https://www.bilibili.com/video/BV1YZ4y1S72j/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/cb76fb63.html" %})
- [https://www.bilibili.com/video/BV1Eu411U7Te/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/22e8db1f.html" %})
- [https://www.bilibili.com/video/BV19S4y1M7hm/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ada14565.html" %})
- [https://www.bilibili.com/video/BV1C3411s7t9/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/8f6ba93f.html" %})
- [https://www.bilibili.com/video/BV1qq4y1z7F2/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/724f621b.html" %})
- [https://www.bilibili.com/video/BV1sq4y1q77t/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/7e198c62.html" %})
- [https://www.bilibili.com/video/BV15P4y137jb/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/367433d9.html" %})
- [https://www.bilibili.com/video/BV1PL411M7eQ/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/ceb3402d.html" %})
- [https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/papers.nips.cc/316cc9d3.pdf" %})
- [https://www.bilibili.com/video/BV1rb4y187vD/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/082bafb9.html" %})
- [https://distill.pub/2021/gnn-intro/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/distill.pub/1c86bfde.html" %})
- [https://www.bilibili.com/video/BV1iT4y1d7zP/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/10c5ee8f.html" %})
- [https://www.bilibili.com/video/BV1pu411o7BE/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/d4534f87.html" %})
- [https://www.bilibili.com/video/BV1P3411y7nn/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/600ca294.html" %})
- [https://www.bilibili.com/video/BV1Fb4y1h73E/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/b97dae66.html" %})
- [https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/papers.nips.cc/29d938f1.pdf" %})
- [https://www.bilibili.com/video/BV1hq4y157t1/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/1a08ef6e.html" %})
- [https://www.bilibili.com/video/BV1ih411J7Kz/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/www.bilibili.com/0363cb0e.html" %})
- [https://c.d2l.ai/zh-v2/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/c.d2l.ai/f774f1a4.html" %})
- [https://github.com/mli/paper-reading/discussions]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/github.com/4e37b659.html" %})
- [https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/api.semanticscholar.org/28134a4f.html" %})
- [https://cs.stanford.edu/people/karpathy/deepvideo/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/cs.stanford.edu/9df4fabd.html" %})
- [https://distill.pub/2017/momentum/]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/distill.pub/5bd8c0cb.html" %})
- [https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/storage.googleapis.com/321c4317.pdf" %})
- [https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf]({% include relrefx.html url="/backup/2024-07-09-ml-DL-limu.md/discovery.ucl.ac.uk/e47bd3a4.pdf" %})

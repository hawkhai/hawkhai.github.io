---
layout: post
title: "机器学习 -- 大型语言模型简史：从 Transformers (2017) 到 DeepSeek-R1(2025)"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle:
comments:
visibility:
mathjax:
mermaid:
glslcanvas:
codeprint:
cluster: "机器学习课程"
---

[大型语言模型简史：从 Transformers (2017) 到 DeepSeek-R1(2025)](https://medium.com/@lmpo/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8F%B2-%E4%BB%8Etransformer-2017-%E5%88%B0deepseek-r1-2025-cc54d658fb43)

{% include image.html url="/assets/images/250220-llm-history-transformer~a8/1_mtohu_WU9ykYZUztwmDTjg.webp" %}

> GRPO 人工智能的每次发展，基本原理感觉都很简单，那些完成严格数学表达，完整代码实现的人，应该都是天才。
> 在尝试了大量可能后，找到了那条最优美的路。简洁的像诗歌，璀璨如星辰。
> 每当深夜阅读这些文章，总会令人惊叹与震撼，感受那纯粹的美妙。
> 不管什么奇奇怪怪的模型结构，不管多少维度的 Tensor 都能算梯度，并完成正向反向传播。
> 这些极度聪明的人推动了人工智能的发展。而我，不够聪明，无法参与其中。


## GRPO（Group Relative Policy Optimization）

DeepSeek-R1 GRPO 算法揭秘
<https://www.bilibili.com/video/BV15zNyeXEVP/>

{% include image.html url="/assets/images/250220-llm-history-transformer~a8/bc1c0ea5c7052810b5f2ea6482be5b75.png" %}
<https://blog.csdn.net/v_JULY_v/article/details/136656918>

{% include image.html url="/assets/images/250220-llm-history-transformer~a8/1e5e5964c5cf44759136de53a5a89808.png" %}

<https://huggingface.co/docs/trl/main/en/grpo_trainer>



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2025-02-20-llm-history-transformers-2017-to-deepseek-r1-2025.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://medium.com/@lmpo/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8F%B2-%E4%BB%8Etransformer-2017-%E5%88%B0deepseek-r1-2025-cc54d658fb43]({% include relrefx.html url="/backup/2025-02-20-llm-history-transformers-2017-to-deepseek-r1-2025.md/medium.com/32ba340f.html" %})
- [https://www.bilibili.com/video/BV15zNyeXEVP/]({% include relrefx.html url="/backup/2025-02-20-llm-history-transformers-2017-to-deepseek-r1-2025.md/www.bilibili.com/28741d25.html" %})
- [https://blog.csdn.net/v_JULY_v/article/details/136656918]({% include relrefx.html url="/backup/2025-02-20-llm-history-transformers-2017-to-deepseek-r1-2025.md/blog.csdn.net/d3167ade.html" %})
- [https://huggingface.co/docs/trl/main/en/grpo_trainer]({% include relrefx.html url="/backup/2025-02-20-llm-history-transformers-2017-to-deepseek-r1-2025.md/huggingface.co/fe7b7c63.html" %})

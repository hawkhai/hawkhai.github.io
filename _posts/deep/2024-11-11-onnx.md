---
layout: post
title: "机器学习 -- ONNX 问题总结"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle: none
comments:
visibility:
mathjax:
mermaid:
glslcanvas:
codeprint:
---

**支持 win7 系统的 onnxruntime**
项目用到了 AI 相关的东西，依赖于 onnxruntime，在官网下载了编译好的版本，成功跑通。
实际部署后发现不支持 win7，提示缺失各种 api-ms-core-… 之类的 dll，于是只能自行魔改 onnxruntime。

**You can implement it yourself to support Windows 7. The issue is resolved here:**
**可以自己实现，以支持 win7，这里解决了：<https://github.com/yycmagic/onnxruntime-for-win7>**


## PyTorch 模型存储—转化为 ONNX

<https://blog.csdn.net/Leo_whj/article/details/109736449>


## api-ms-win-core-heap-l2-1-0.dll missing

<https://github.com/microsoft/onnxruntime/issues/15025>
@skottmckay @fdwr "Windows builds are not compatible with Windows 8.x in this release. Please use v1.11 for now." great news, I looked dependency walker with onnxruntime.dll v1.11.1, it seems it doesn't depends on api-ms-win-core-heap-l2-1-0.dll.

<https://github.com/microsoft/onnxruntime/pull/10796>
**You can implement it yourself to support Windows 7. The issue is resolved here:**
**可以自己实现，以支持 win7，这里解决了：<https://github.com/yycmagic/onnxruntime-for-win7>**


## onnxruntime pre-compiled libs

[静态编译 ONNX RUNTIME {% include relref_zhihu.html %}](https://zhuanlan.zhihu.com/p/614293644)
<https://github.com/csukuangfj/onnxruntime-libs>

onnxruntime 兼容 win7
<https://blog.csdn.net/weixin_40196536/article/details/134668960>
HLOCAL \_\_stdcall LocalAlloc(IN UINT uFlags, SIZE_T uBytes); // 17
LPVOID \_\_stdcall LocalLock(IN HLOCAL hMem); // 18
HLOCAL \_\_stdcall LocalFree(IN HLOCAL hMem);

```cpp
// Imports from api-ms-win-core-heap-l2-1-0.dll

// ; Exported entry  17. LocalAlloc
// LocalAlloc      db 'kernel32.LocalAlloc',0
// ; Exported entry  18. LocalFree
// LocalFree       db 'kernel32.LocalFree',0

HLOCAL __stdcall LocalFree(HLOCAL hMem)
{
    return __imp_LocalFree(hMem);
}
```

```cpp
#include <Windows.h>

HLOCAL WINAPI MyLocalFree(HLOCAL hMem) {
    if (hMem == NULL) {
        return NULL; // 如果指针为空，直接返回
    }

    // 使用 Windows 的 HeapFree 函数来释放内存
    HANDLE hHeap = GetProcessHeap(); // 获取默认的进程堆
    if (!HeapFree(hHeap, 0, hMem)) {
        SetLastError(ERROR_INVALID_HANDLE); // 设置错误码
        return hMem; // 返回非 NULL 表示释放失败
    }

    return NULL; // 返回 NULL 表示释放成功
}
```

从这里下载的：<https://github.com/microsoft/onnxruntime/issues/15025>
api-ms-win-core-heap-l2-1-0.dll


## float32 -> float16

在模型转换为 float16 后，输入和输出张量的数据类型也变为
ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16。需要确保代码正确处理此数据类型。
```cpp
#include <onnxruntime/core/session/onnxruntime_cxx_api.h>

// 示例修改输入数据类型
Ort::AllocatorWithDefaultOptions allocator;

// 获取模型输入信息
auto input_info = session.GetInputTypeInfo(0).GetTensorTypeAndShapeInfo();
auto input_type = input_info.GetElementType();

// 确保输入数据类型是 float16
if (input_type == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16) {
    std::cout << "Model input type is float16." << std::endl;
} else {
    throw std::runtime_error("Unexpected input type. Expected float16.");
}
```

准备输入数据。
ONNX Runtime 使用 uint16_t 表示 float16。
onnxruntime::utils::floatToHalf 或其他类似工具。
```cpp
#include <onnxruntime/core/common/float16.h>

// 示例 float32 转 float16
std::vector<float> input_data_float32 = {1.0f, 2.0f, 3.0f}; // 假设输入是 float32
std::vector<uint16_t> input_data_float16(input_data_float32.size());

for (size_t i = 0; i < input_data_float32.size(); ++i) {
    input_data_float16[i] = onnxruntime::math::floatToHalf(input_data_float32[i]);
}
```

```cpp
Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

Ort::Value input_tensor = Ort::Value::CreateTensor<uint16_t>(
    memory_info, input_data_float16.data(), input_data_float16.size(),
    input_dims.data(), input_dims.size());
```

处理推理结果。
```
std::vector<uint16_t> output_data_float16(output_size);
session.Run(Ort::RunOptions{nullptr}, input_names.data(), &input_tensor, 1,
            output_names.data(), &output_tensor, 1);

// 转换为 float32
std::vector<float> output_data_float32(output_size);
for (size_t i = 0; i < output_size; ++i) {
    output_data_float32[i] = onnxruntime::math::halfToFloat(output_data_float16[i]);
}
```



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2024-11-11-onnx.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://github.com/yycmagic/onnxruntime-for-win7]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/9b27ec7b.html" %})
- [https://blog.csdn.net/Leo_whj/article/details/109736449]({% include relrefx.html url="/backup/2024-11-11-onnx.md/blog.csdn.net/135f5db1.html" %})
- [https://github.com/microsoft/onnxruntime/issues/15025]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/365c11fb.html" %})
- [https://github.com/microsoft/onnxruntime/pull/10796]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/0ae599c1.html" %})
- [https://zhuanlan.zhihu.com/p/614293644]({% include relrefx.html url="/backup/2024-11-11-onnx.md/zhuanlan.zhihu.com/329ae5d2.html" %})
- [https://github.com/csukuangfj/onnxruntime-libs]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/f6222030.html" %})
- [https://blog.csdn.net/weixin_40196536/article/details/134668960]({% include relrefx.html url="/backup/2024-11-11-onnx.md/blog.csdn.net/0a729d1e.html" %})

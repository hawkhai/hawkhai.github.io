---
layout: post
title: "机器学习 -- ONNX 问题总结 & MNN"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle: none
comments:
visibility:
mathjax:
mermaid:
glslcanvas:
codeprint:
---

**支持 win7 系统的 onnxruntime**
项目用到了 AI 相关的东西，依赖于 onnxruntime，在官网下载了编译好的版本，成功跑通。
实际部署后发现不支持 win7，提示缺失各种 api-ms-core-… 之类的 dll，于是只能自行魔改 onnxruntime。

**You can implement it yourself to support Windows 7. The issue is resolved here:**
**可以自己实现，以支持 win7，这里解决了：<https://github.com/yycmagic/onnxruntime-for-win7>**


## PyTorch 模型存储—转化为 ONNX

<https://blog.csdn.net/Leo_whj/article/details/109736449>


## api-ms-win-core-heap-l2-1-0.dll missing

<https://github.com/microsoft/onnxruntime/issues/15025>
@skottmckay @fdwr "Windows builds are not compatible with Windows 8.x in this release. Please use v1.11 for now." great news, I looked dependency walker with onnxruntime.dll v1.11.1, it seems it doesn't depends on api-ms-win-core-heap-l2-1-0.dll.

<https://github.com/microsoft/onnxruntime/pull/10796>
**You can implement it yourself to support Windows 7. The issue is resolved here:**
**可以自己实现，以支持 win7，这里解决了：<https://github.com/yycmagic/onnxruntime-for-win7>**


## onnxruntime pre-compiled libs

[静态编译 ONNX RUNTIME {% include relref_zhihu.html %}](https://zhuanlan.zhihu.com/p/614293644)
<https://github.com/csukuangfj/onnxruntime-libs>

onnxruntime 兼容 win7
<https://blog.csdn.net/weixin_40196536/article/details/134668960>
HLOCAL \_\_stdcall LocalAlloc(IN UINT uFlags, SIZE_T uBytes); // 17
LPVOID \_\_stdcall LocalLock(IN HLOCAL hMem); // 18
HLOCAL \_\_stdcall LocalFree(IN HLOCAL hMem);

```cpp
// Imports from api-ms-win-core-heap-l2-1-0.dll

// ; Exported entry  17. LocalAlloc
// LocalAlloc      db 'kernel32.LocalAlloc',0
// ; Exported entry  18. LocalFree
// LocalFree       db 'kernel32.LocalFree',0

HLOCAL __stdcall LocalFree(HLOCAL hMem)
{
    return __imp_LocalFree(hMem);
}
```

```cpp
#include <Windows.h>

HLOCAL WINAPI MyLocalFree(HLOCAL hMem) {
    if (hMem == NULL) {
        return NULL; // 如果指针为空，直接返回
    }

    // 使用 Windows 的 HeapFree 函数来释放内存
    HANDLE hHeap = GetProcessHeap(); // 获取默认的进程堆
    if (!HeapFree(hHeap, 0, hMem)) {
        SetLastError(ERROR_INVALID_HANDLE); // 设置错误码
        return hMem; // 返回非 NULL 表示释放失败
    }

    return NULL; // 返回 NULL 表示释放成功
}
```

从这里下载的：<https://github.com/microsoft/onnxruntime/issues/15025>
api-ms-win-core-heap-l2-1-0.dll


## float32 -> float16

在模型转换为 float16 后，输入和输出张量的数据类型也变为
ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16。需要确保代码正确处理此数据类型。
```cpp
#include <onnxruntime/core/session/onnxruntime_cxx_api.h>

// 示例修改输入数据类型
Ort::AllocatorWithDefaultOptions allocator;

// 获取模型输入信息
auto input_info = session.GetInputTypeInfo(0).GetTensorTypeAndShapeInfo();
auto input_type = input_info.GetElementType();

// 确保输入数据类型是 float16
if (input_type == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16) {
    std::cout << "Model input type is float16." << std::endl;
} else {
    throw std::runtime_error("Unexpected input type. Expected float16.");
}
```

准备输入数据。
ONNX Runtime 使用 uint16_t 表示 float16。
onnxruntime::utils::floatToHalf 或其他类似工具。
```cpp
#include <onnxruntime/core/common/float16.h>

// 示例 float32 转 float16
std::vector<float> input_data_float32 = {1.0f, 2.0f, 3.0f}; // 假设输入是 float32
std::vector<uint16_t> input_data_float16(input_data_float32.size());

for (size_t i = 0; i < input_data_float32.size(); ++i) {
    input_data_float16[i] = onnxruntime::math::floatToHalf(input_data_float32[i]);
}
```

```cpp
Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

Ort::Value input_tensor = Ort::Value::CreateTensor<uint16_t>(
    memory_info, input_data_float16.data(), input_data_float16.size(),
    input_dims.data(), input_dims.size());
```

处理推理结果。
```
std::vector<uint16_t> output_data_float16(output_size);
session.Run(Ort::RunOptions{nullptr}, input_names.data(), &input_tensor, 1,
            output_names.data(), &output_tensor, 1);

// 转换为 float32
std::vector<float> output_data_float32(output_size);
for (size_t i = 0; i < output_size; ++i) {
    output_data_float32[i] = onnxruntime::math::halfToFloat(output_data_float16[i]);
}
```


## ImportError onnxruntime_pybind11_state

```
  File "D:\tempdemo\mylayout\Python310\lib\site-packages\rapid_table\table_structure\utils.py", line 26, in <module>
    from onnxruntime import (
  File "D:\tempdemo\mylayout\Python310\lib\site-packages\onnxruntime\__init__.py", line 61, in <module>
    raise import_capi_exception
  File "D:\tempdemo\mylayout\Python310\lib\site-packages\onnxruntime\__init__.py", line 24, in <module>
    from onnxruntime.capi._pybind_state import (
  File "D:\tempdemo\mylayout\Python310\lib\site-packages\onnxruntime\capi\_pybind_state.py", line 32, in <module>
    from .onnxruntime_pybind11_state import *  # noqa
ImportError: DLL load failed while importing onnxruntime_pybind11_state: 动态链接库（DLL）初始化例程失败。
```

Microsoft Visual C++ Redistributable 2019x64 官方下载地址
aka.ms/vs/16/release/VC_redist.x64.exe

Visual Studio 2015, 2017, 2019, and 2022
aka.ms/vs/17/release/vc_redist.x64.exe


## MNN

编译 Python：
```
set PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\Bin;%PATH%

python3 build_deps.py
#-G "Visual Studio 16 2019"
#msbuild MNN.sln /t:MNN;MNNConvertDeps /p:Configuration=Release
I:\ocr_data\Python\Python310-mineru\Python310-mineru\python.exe setup.py install

cd I:\ocr_inference
I:\ocr_data\Python\Python310-mineru\Python310-mineru\Scripts\pip.exe
I:\ocr_data\Python\Python310-mineru\Python310-mineru\python.exe main.py
```

-DCMAKE_CXX_FLAGS="-O3 -march=native"
-DCMAKE_CXX_FLAGS="-O3 -march=native"


### SSE4.1 构建（最低兼容）

```
cmake .. -G "Visual Studio 16 2019" -DMNN_BUILD_FOR_X86=ON ^
  -DMNN_USE_SSE=ON -DMNN_AVX512=OFF -DMNN_AVX2=OFF ^
  -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install_SSE4.1
```


### AVX2 构建（中间性能）

```
cmake .. -G "Visual Studio 16 2019" -DMNN_BUILD_FOR_X86=ON ^
  -DMNN_USE_SSE=ON -DMNN_AVX2=ON -DMNN_AVX512=OFF ^
  -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install_AVX2
```


### AVX512 构建（高性能）

```
cmake .. -G "Visual Studio 16 2019" -DMNN_BUILD_FOR_X86=ON ^
  -DMNN_USE_SSE=ON -DMNN_AVX2=ON -DMNN_AVX512=ON ^
  -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install_AVX512
```

mkdir buildx && cd buildx

```
cmake .. -DMNN_BUILD_FOR_X86=ON ^
  -G "Visual Studio 16 2019" -T v142 ^
  -A Win32 ^
  -DMNN_USE_SSE=ON -DMNN_AVX512=OFF -DMNN_AVX2=OFF ^
  -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install_SSE4.1
```

```
cmake .. ^
  -G "Visual Studio 16 2019" -T v142 ^
  -DCMAKE_BUILD_TYPE=Release ^
  -DMNN_BUILD_CONVERTER=OFF ^
  -DMNN_BUILD_TRAIN=OFF ^
  -DMNN_BUILD_DEMO=OFF ^
  -DMNN_BUILD_BENCHMARK=OFF ^
  -DMNN_BUILD_QUANTOOLS=OFF ^
  -DMNN_BUILD_TOOLS=OFF ^
  -DMNN_BUILD_TEST=OFF ^
  -DMNN_SEP_BUILD=ON ^
  -DMNN_OPENMP=ON ^
  -DMNN_AVX2=ON ^
  -DMNN_AVX512=ON ^
  -DMNN_USE_SSE=ON ^
  -DMNN_DISABLE_LOG=ON ^
  -DMNN_THREADS=ON -DMNN_BUILD_EXPRESS=ON ^
  -DCMAKE_CXX_FLAGS="/O2 /GL /arch:AVX2 /fp:fast /favor:INTEL64 /Ot /Oi /Ob2 /Oy /Gy /GF /Gw /GS- /GR- /Zc:inline /Zc:threadSafeInit- /Zc:strictStrings /Zc:preprocessor /permissive- /diagnostics:caret /GL /DNDEBUG -march=native" ^
  -DCMAKE_EXE_LINKER_FLAGS="/LTCG" -DCMAKE_INSTALL_PREFIX=install_AVX512
```

ninja
"C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\Bin\MSBuild.exe"



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2024-11-11-onnx.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://github.com/yycmagic/onnxruntime-for-win7]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/9b27ec7b.html" %})
- [https://blog.csdn.net/Leo_whj/article/details/109736449]({% include relrefx.html url="/backup/2024-11-11-onnx.md/blog.csdn.net/135f5db1.html" %})
- [https://github.com/microsoft/onnxruntime/issues/15025]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/365c11fb.html" %})
- [https://github.com/microsoft/onnxruntime/pull/10796]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/0ae599c1.html" %})
- [https://zhuanlan.zhihu.com/p/614293644]({% include relrefx.html url="/backup/2024-11-11-onnx.md/zhuanlan.zhihu.com/329ae5d2.html" %})
- [https://github.com/csukuangfj/onnxruntime-libs]({% include relrefx.html url="/backup/2024-11-11-onnx.md/github.com/f6222030.html" %})
- [https://blog.csdn.net/weixin_40196536/article/details/134668960]({% include relrefx.html url="/backup/2024-11-11-onnx.md/blog.csdn.net/0a729d1e.html" %})

---
layout: post
title: "机器学习 -- Attention is all you need"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle: none
comments:
visibility: hidden
mathjax:
mermaid:
glslcanvas:
codeprint:
---

Attention is all you need
transformer
bert
@vcubingx


## What does it mean for computers to understand language? \| LM1

<https://www.youtube.com/watch?v=1il-s4mgNdI>
<https://www.bilibili.com/video/BV1jx4y1t7Lq/>


## Why Recurrent Neural Networks are cursed \| LM2

<https://www.youtube.com/watch?v=rTz6hadM1Lg>
<https://www.bilibili.com/video/BV1qi421e7DH/>


## How did the Attention Mechanism start an AI frenzy? \| LM3

注意力机制被用于 Transformer 中，起源于修复 RNN 的问题。
<https://www.youtube.com/watch?v=lOrTlKrdmkQ>
<https://www.bilibili.com/video/BV1Dw4m1e74b/>



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2024-06-30-attention.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://www.youtube.com/watch?v=1il-s4mgNdI]({% include relrefx.html url="/backup/2024-06-30-attention.md/www.youtube.com/b22ed315.html" %})
- [https://www.bilibili.com/video/BV1jx4y1t7Lq/]({% include relrefx.html url="/backup/2024-06-30-attention.md/www.bilibili.com/dbbaac38.html" %})
- [https://www.youtube.com/watch?v=rTz6hadM1Lg]({% include relrefx.html url="/backup/2024-06-30-attention.md/www.youtube.com/337c0aa5.html" %})
- [https://www.bilibili.com/video/BV1qi421e7DH/]({% include relrefx.html url="/backup/2024-06-30-attention.md/www.bilibili.com/ce799314.html" %})
- [https://www.youtube.com/watch?v=lOrTlKrdmkQ]({% include relrefx.html url="/backup/2024-06-30-attention.md/www.youtube.com/49f0d0af.html" %})
- [https://www.bilibili.com/video/BV1Dw4m1e74b/]({% include relrefx.html url="/backup/2024-06-30-attention.md/www.bilibili.com/4ff97324.html" %})

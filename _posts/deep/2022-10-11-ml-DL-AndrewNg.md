---
layout: post
title: "机器学习 -- 吴恩达深度学习（进行中）"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle:
comments:
visibility:
mathjax:
mermaid:
glslcanvas:
codeprint:
cluster: "机器学习课程"
---

* 吴恩达机器学习 <https://www.bilibili.com/video/BV1Bq421A74G/>
* 吴恩达深度学习 <https://www.bilibili.com/video/BV1Gm421u73z/>
* [html2text](http://www.atoolbox.net/Tool.php?Id=715)


## 1 1.1 欢迎来到机器学习 ! 02:45 1.1 欢迎来到机器学习 !


## 2 1.2 机器学习的应用 04:29 1.2 机器学习的应用


## 3 2.1 什么是机器学习 05:36 2.1 什么是机器学习


## 4 2.2 监督学习 part 1 06:57 2.2 监督学习 part 1


## 5 2.3 监督学习 part 2 07:17 2.3 监督学习 part 2


## 6 2.4 非监督学习 part 1 08:54 2.4 非监督学习 part 1


## 7 2.5 非监督学习 part 2 03:40 2.5 非监督学习 part 2


## 8 2.6 Jupyter Notebooks 04:30 2.6 Jupyter Notebooks


## 9 3.1 线性回归模型 part 1 10:27 3.1 线性回归模型 part 1


## 10 3.2 线性回归模型 part 2 06:45 3.2 线性回归模型 part 2


## 11 3.3 代价函数 09:05 3.3 代价函数


## 12 3.4 代价函数的直观理解 15:47 3.4 代价函数的直观理解


## 13 3.5 可视化代价函数 08:34 3.5 可视化代价函数


## 14 3.6 可视化的例子 06:01 3.6 可视化的例子


## 15 4.1 梯度下降 08:04 4.1 梯度下降


## 16 4.2 实现梯度下降 10:00 4.2 实现梯度下降


## 17 4.3 梯度下降的直观理解 07:02 4.3 梯度下降的直观理解


## 18 4.4 学习率 09:04 4.4 学习率


## 19 4.5 线性回归中的梯度下降 06:37 4.5 线性回归中的梯度下降


## 20 4.6 运行梯度下降 05:49 4.6 运行梯度下降


## 21 5.1 多类特征 09:52 5.1 多类特征


## 22 5.2 向量化 part 1 06:55 5.2 向量化 part 1


## 23 5.3 向量化 part 2 06:53 5.3 向量化 part 2


## 24 5.4 多元线性回归的梯度下降法 07:47 5.4 多元线性回归的梯度下降法


## 25 6.1 特征缩放 part 1 06:36 6.1 特征缩放 part 1


## 26 6.2 特征缩放 part 2 07:35 6.2 特征缩放 part 2


## 27 6.3 检查梯度下降是否收敛 05:40 6.3 检查梯度下降是否收敛


## 28 6.4 学习率的选择 06:07 6.4 学习率的选择


## 29 6.5 特征工程 03:05 6.5 特征工程


## 30 6.6 多项式回归 05:53 6.6 多项式回归


## 31 7.1 Motivations 09:48 7.1 Motivations


## 32 7.2 逻辑 (logistic) 回归 09:49 7.2 逻辑 (logistic) 回归


## 33 7.3 决策边界 10:43 7.3 决策边界


## 34 8.1 逻辑回归的代价函数 12:00 8.1 逻辑回归的代价函数


## 35 8.2 逻辑回归的简化版代价函数 05:46 8.2 逻辑回归的简化版代价函数


## 36 9.1 梯度下降实现 06:32 9.1 梯度下降实现


## 37 10.1 过拟合的问题 11:53 10.1 过拟合的问题


## 38 10.2 解决过拟合 08:16 10.2 解决过拟合


## 39 10.3 正则化代价函数 09:04 10.3 正则化代价函数


## 40 10.4 正则化线性回归 08:53 10.4 正则化线性回归


## 41 10.5 正则化 logistic 回归 05:33 10.5 正则化 logistic 回归


## 42 1.1 欢迎来到第二部分 \_ 高级学习算法 02:54 1.1 欢迎来到第二部分 _ 高级学习算法


## 43 1.2 神经元和大脑 10:53 1.2 神经元和大脑


## 44 1.3 需求预测 16:24 1.3 需求预测


## 45 1.4 例子：图像识别 06:36 1.4 例子：图像识别


## 46 2.1 神经网络中的层 09:50 2.1 神经网络中的层


## 47 2.2 更复杂的神经网络 07:19 2.2 更复杂的神经网络


## 48 2.3 推理：做出预测（前向传播） 05:24 2.3 推理：做出预测（前向传播）


## 49 3.1 代码中的推理 07:13 3.1 代码中的推理


## 50 3.2 TensorFlow 中的数据 11:20 3.2 TensorFlow 中的数据


## 51 3.3 构建一个神经网络 08:21 3.3 构建一个神经网络


## 52 4.1 在一个单层中的前向传播 05:07 4.1 在一个单层中的前向传播


## 53 4.2 前向传播的一般实现 07:53 4.2 前向传播的一般实现


## 54 5.1 是否有路通向 AGI（通用人工智能） 10:35 5.1 是否有路通向 AGI（通用人工智能）


## 55 6.1 神经网络如何高效实现 04:23 6.1 神经网络如何高效实现


## 56 6.2 矩阵乘法 09:28 6.2 矩阵乘法


## 57 6.3 矩阵乘法的规则 09:33 6.3 矩阵乘法的规则


## 58 6.4 矩阵乘法代码 06:42 6.4 矩阵乘法代码


## 59 7.1 TensorFlow 实现 03:38 7.1 TensorFlow 实现


## 60 7.2 训练细节 13:41 7.2 训练细节


## 61 8.1 sigmoid 的替代品 05:30 8.1 sigmoid 的替代品


## 62 8.2 选择激活函数 08:25 8.2 选择激活函数


## 63 8.3 为什么我们需要激活函数 05:32 8.3 为什么我们需要激活函数


## 64 9.1 多类 03:29 9.1 多类


## 65 9.2 Softmax 11:33 9.2 Softmax


## 66 9.3 神经网络的 Softmax 输出 07:25 9.3 神经网络的 Softmax 输出


## 67 9.4 softmax 的改进实现 09:13 9.4 softmax 的改进实现


## 68 9.5 多个输出的分类 (Optional) 04:20 9.5 多个输出的分类 (Optional)


## 69 10.1 高级优化方法 06:26 10.1 高级优化方法


## 70 10.2 Additional Layer Types 08:56 10.2 Additional Layer Types


## 71 11.1 决定下一步做什么 03:42 11.1 决定下一步做什么


## 72 11.2 模型评估 10:26 11.2 模型评估


## 73 11.3 模型选择和训练交叉验证测试集 14:53 11.3 模型选择和训练交叉验证测试集


## 74 12.1 诊断偏差和方差 11:13 12.1 诊断偏差和方差


## 75 12.2 正则化和偏差或方差 10:37 12.2 正则化和偏差或方差


## 76 12.3 建立表现基准 09:26 12.3 建立表现基准


## 77 12.4 学习曲线 12:14 12.4 学习曲线


## 78 12.5 再次决定下一步做什么 08:47 12.5 再次决定下一步做什么


## 79 12.6 偏差或方差与神经网络 10:45 12.6 偏差或方差与神经网络


## 80 13.1 机器学习的迭代发展 07:43 13.1 机器学习的迭代发展


## 81 13.2 误差分析 08:21 13.2 误差分析


## 82 13.3 添加数据 14:24 13.3 添加数据


## 83 13.4 迁移学习：使用其他任务中的数据 12:11 13.4 迁移学习：使用其他任务中的数据


## 84 13.5 机器学习项目的完整周期 08:45 13.5 机器学习项目的完整周期


## 85 13.6 公平、偏见与伦理 09:57 13.6 公平、偏见与伦理


## 86 14.1 倾斜数据集的误差指标 11:36 14.1 倾斜数据集的误差指标


## 87 14.2 精确率与召回率的权衡 11:50 14.2 精确率与召回率的权衡


## 88 15.1 决策树模型 07:06 15.1 决策树模型


## 89 15.2 学习过程 11:21 15.2 学习过程


## 90 16.1 测量纯度 07:50 16.1 测量纯度


## 91 16.2 选择拆分信息增益 11:52 16.2 选择拆分信息增益


## 92 16.3 整合 09:29 16.3 整合


## 93 16.4 使用分类特征的一种独热编码 (One-Hot) 05:26 16.4 使用分类特征的一种独热编码 (One-Hot)


## 94 16.5 连续的有价值特征 06:55 16.5 连续的有价值特征


## 95 16.6 回归树 (optional) 09:52 16.6 回归树 (optional)


## 96 17.1 使用多个决策树 03:57 17.1 使用多个决策树


## 97 17.2 放回抽样 04:00 17.2 放回抽样


## 98 17.3 随机森林算法 06:23 17.3 随机森林算法


## 99 17.4 XGBoost 07:26 17.4 XGBoost


## 100 17.5 什么时候使用决策树 06:55 17.5 什么时候使用决策树


## 101 1.1 欢迎来到第三部分 无监督学习、推荐系统和强化学习 03:23 1.1 欢迎来到第三部分 无监督学习、推荐系统和强化学习


## 102 2.1 什么是聚类 04:13 2.1 什么是聚类


## 103 2.2 K-means 的直观理解 06:50 2.2 K-means 的直观理解


## 104 2.3 K-means 算法 09:51 2.3 K-means 算法


## 105 2.4 优化目标 11:14 2.4 优化目标


## 106 2.5 初始化 K-means 08:54 2.5 初始化 K-means


## 107 2.6 选择聚类的个数 07:58 2.6 选择聚类的个数


## 108 3.1 发现异常事件 11:55 3.1 发现异常事件


## 109 3.2 高斯（正态）分布 10:51 3.2 高斯（正态）分布


## 110 3.3 异常检测算法 12:09 3.3 异常检测算法


## 111 3.4 开发和评估异常检测系统 11:39 3.4 开发和评估异常检测系统


## 112 3.5 异常检测 vs. 监督学习 08:09 3.5 异常检测 vs. 监督学习


## 113 3.6 选择要使用的特征 14:58 3.6 选择要使用的特征


## 114 4.1 提出建议 05:33 4.1 提出建议


## 115 4.2 使用每项特征 11:23 4.2 使用每项特征


## 116 4.3 协同过滤算法 13:56 4.3 协同过滤算法


## 117 4.4 Binary labels- favs, likes and c 08:28 4.4 Binary labels- favs, likes and c


## 118 5.1 均值归一化 08:46 5.1 均值归一化


## 119 5.2 协同过滤的 TensorFlow 实现 11:39 5.2 协同过滤的 TensorFlow 实现


## 120 5.3 查找相关项目 06:34 5.3 查找相关项目


## 121 6.1 协同过滤 vs. 基于内容的过滤 09:46 6.1 协同过滤 vs. 基于内容的过滤


## 122 6.2 Deep learning for content-based 09:43 6.2 Deep learning for content-based


## 123 6.3 从大目录中推荐 07:53 6.3 从大目录中推荐


## 124 6.4 推荐系统的道德使用 10:49 6.4 推荐系统的道德使用


## 125 6.5 基于内容过滤的 TensorFlow 实现 04:49 6.5 基于内容过滤的 TensorFlow 实现


## 126 7.1 什么是强化学习 08:49 7.1 什么是强化学习


## 127 7.2 火星探测器示例 06:42 7.2 火星探测器示例


## 128 7.3 The Return in reinforcement lear 10:19 7.3 The Return in reinforcement lear


## 129 7.4 强化学习中的决策与策略制定 02:38 7.4 强化学习中的决策与策略制定


## 130 7.5 回顾关键概念 05:35 7.5 回顾关键概念


## 131 8.1 状态动作值函数定义 10:37 8.1 状态动作值函数定义


## 132 8.2 状态动作值函数示例 05:23 8.2 状态动作值函数示例


## 133 8.3 Bellman 方程 12:53 8.3 Bellman 方程


## 134 8.4 随机环境（可选） 08:25 8.4 随机环境（可选）


## 135 9.1 连续状态空间应用示例 06:25 9.1 连续状态空间应用示例


## 136 9.2 月球着陆器 05:55 9.2 月球着陆器


## 137 9.3 学习状态值函数 16:51 9.3 学习状态值函数


## 138 9.4 算法优化-改进的神经网络结构 03:02 9.4 算法优化-改进的神经网络结构


## 139 9.5 算法优化ϵ-贪婪策略 09:00 9.5 算法优化ϵ-贪婪策略


## 140 9.6 算法优化-小批量和软更新（可选） 11:44 9.6 算法优化-小批量和软更新（可选）


## 141 9.7 强化学习的状态 02:55 9.7 强化学习的状态


## 142 10.1 总结与感谢 03:12 10.1 总结与感谢


## 1 1.1.1 欢迎 05:33 1.1.1 欢迎


## 2 2.1.2 什么是神经网络 07:17 2.1.2 什么是神经网络


## 3 3.1.3 用神经网络进行监督学习 08:30 3.1.3 用神经网络进行监督学习


## 4 4.1.4 为什么深度学习会兴起？ 10:22 4.1.4 为什么深度学习会兴起？


## 5 5.1.5 关于这门课 02:29 5.1.5 关于这门课


## 6 6.1.6 课程资源 01:56 6.1.6 课程资源


## 7 7.2.1 二分分类 08:25 7.2.1 二分分类


## 8 8.2.2 logistic 回归 06:00 8.2.2 logistic 回归


## 9 9.2.3 logistic 回归损失函数 08:12 9.2.3 logistic 回归损失函数


## 10 10.2.4 梯度下降法 11:24 10.2.4 梯度下降法


## 11 11.2.5 导数 07:11 11.2.5 导数


## 12 12.2.6 更多导数的例子 10:28 12.2.6 更多导数的例子


## 13 13.2.7 计算图 03:34 13.2.7 计算图


## 14 14.2.8 使用计算图求导 14:35 14.2.8 使用计算图求导


## 15 15.2.9 logistic 回归中的梯度下降法 06:43 15.2.9 logistic 回归中的梯度下降法


## 16 16.2.10 m 个样本的梯度下降 08:01 16.2.10 m 个样本的梯度下降


## 17 17.2.11 向量化 08:05 17.2.11 向量化


## 18 18.2.12 向量化的更多例子 06:22 18.2.12 向量化的更多例子


## 19 19.2.13 向量化 logistic 回归 07:33 19.2.13 向量化 logistic 回归


## 20 20.2.14 向量化 logistic 回归的梯度输出 09:39 20.2.14 向量化 logistic 回归的梯度输出


## 21 21.2.15 Python 中的广播 11:07 21.2.15 Python 中的广播


## 22 22.2.16 关于 python \_ numpy 向量的说明 06:50 22.2.16 关于 python _ numpy 向量的说明


## 23 23.2.17 Jupyter \_ ipython 笔记本的快速指南 03:44 23.2.17 Jupyter _ ipython 笔记本的快速指南


## 24 24.2.18 （选修）logistic 损失函数的解释 07:16 24.2.18 （选修）logistic 损失函数的解释


## 25 25.3.1 神经网络概览 04:27 25.3.1 神经网络概览


## 26 26.3.2 神经网络表示 05:15 26.3.2 神经网络表示


## 27 27.3.3 计算神经网络的输出 09:59 27.3.3 计算神经网络的输出


## 28 28.3.4 多个样本的向量化 09:06 28.3.4 多个样本的向量化


## 29 29.3.5 向量化实现的解释 07:38 29.3.5 向量化实现的解释


## 30 30.3.6 激活函数 10:57 30.3.6 激活函数


## 31 31.3.7 为什么需要非线性激活函数？ 05:37 31.3.7 为什么需要非线性激活函数？


## 32 32.3.8 激活函数的导数 07:58 32.3.8 激活函数的导数


## 33 33.3.9 神经网络的梯度下降法 09:58 33.3.9 神经网络的梯度下降法


## 34 34.3.10 （选修）直观理解反向传播 15:49 34.3.10 （选修）直观理解反向传播


## 35 35.3.11 随机初始化 07:58 35.3.11 随机初始化


## 36 36.4.1 深层神经网络 05:52 36.4.1 深层神经网络


## 37 37.4.2 前向和反向传播 10:30 37.4.2 前向和反向传播


## 38 38.4.3 深层网络中的前向传播 07:16 38.4.3 深层网络中的前向传播


## 39 39.4.4 核对矩阵的维数 11:11 39.4.4 核对矩阵的维数


## 40 40.4.5 为什么使用深层表示 10:34 40.4.5 为什么使用深层表示


## 41 41.4.6 搭建深层神经网络块 08:34 41.4.6 搭建深层神经网络块


## 42 42.4.7 参数 VS 超参数 07:18 42.4.7 参数 VS 超参数


## 43 43.4.8 这和大脑有什么关系？ 03:18 43.4.8 这和大脑有什么关系？


## 44 44.1. 吴恩达采访 Geoffrey Hinton 40:23 44.1. 吴恩达采访 Geoffrey Hinton


## 45 45.2. 吴恩达采访 Pieter Abbeel 16:04 45.2. 吴恩达采访 Pieter Abbeel


## 46 46.3. 吴恩达采访 Ian Goodfellow 14:56 46.3. 吴恩达采访 Ian Goodfellow


## 47 47.1.1 训练 \_ 开发 \_ 测试集 12:05 47.1.1 训练 _ 开发 _ 测试集


## 48 48.1.2 偏差 \_ 方差 08:47 48.1.2 偏差 _ 方差


## 49 49.1.3 机器学习基础 06:22 49.1.3 机器学习基础


## 50 50.1.4 正则化 09:43 50.1.4 正则化


## 51 51.1.5 为什么正则化可以减少过拟合？ 07:10 51.1.5 为什么正则化可以减少过拟合？


## 52 52.1.6 Dropout 正则化 09:26 52.1.6 Dropout 正则化


## 53 53.1.7 理解 Dropout 07:05 53.1.7 理解 Dropout


## 54 54.1.8 其他正则化方法 08:25 54.1.8 其他正则化方法


## 55 55.1.9 归一化输入 05:31 55.1.9 归一化输入


## 56 56.1.10 梯度消失与梯度爆炸 06:08 56.1.10 梯度消失与梯度爆炸


## 57 57.1.11 神经网络的权重初始化 06:13 57.1.11 神经网络的权重初始化


## 58 58.1.12 梯度的数值逼近 06:36 58.1.12 梯度的数值逼近


## 59 59.1.13 梯度检验 06:35 59.1.13 梯度检验


## 60 60.1.14 关于梯度检验实现的注记 05:19 60.1.14 关于梯度检验实现的注记


## 61 61.2.1 Mini-batch 梯度下降法 11:30 61.2.1 Mini-batch 梯度下降法


## 62 62.2.2 理解 mini-batch 梯度下降法 08:15 62.2.2 理解 mini-batch 梯度下降法


## 63 63.2.3 指数加权平均 05:59 63.2.3 指数加权平均


## 64 64.2.4 理解指数加权平均 09:43 64.2.4 理解指数加权平均


## 65 65.2.5 指数加权平均的偏差修正 04:12 65.2.5 指数加权平均的偏差修正


## 66 66.2.6 动量梯度下降法 09:21 66.2.6 动量梯度下降法


## 67 67.2.7 RMSprop 07:42 67.2.7 RMSprop


## 68 68.2.8 Adam 优化算法 07:09 68.2.8 Adam 优化算法


## 69 69.2.9 学习率衰减 06:45 69.2.9 学习率衰减


## 70 70.2.10 局部最优的问题 05:24 70.2.10 局部最优的问题


## 71 71.3.1 调试处理 07:11 71.3.1 调试处理


## 72 72.3.2 为超参数选择合适的范围 08:51 72.3.2 为超参数选择合适的范围


## 73 73.3.3 超参数训练的实践：Pandas VS Caviar 06:52 73.3.3 超参数训练的实践：Pandas VS Caviar


## 74 74.3.4 正则化网络的激活函数 08:56 74.3.4 正则化网络的激活函数


## 75 75.3.5 将 Batch Norm 拟合进神经网络 12:56 75.3.5 将 Batch Norm 拟合进神经网络


## 76 76.3.6 Batch Norm 为什么奏效？ 11:40 76.3.6 Batch Norm 为什么奏效？


## 77 77.3.7 测试时的 Batch Norm 05:47 77.3.7 测试时的 Batch Norm


## 78 78.3.8 Softmax 回归 11:48 78.3.8 Softmax 回归


## 79 79.3.9 训练一个 Softmax 分类器 10:08 79.3.9 训练一个 Softmax 分类器


## 80 80.3.10 深度学习框架 04:16 80.3.10 深度学习框架


## 81 81.3.11 TensorFlow 16:08 81.3.11 TensorFlow


## 82 82.1. 吴恩达采访 Yoshua Bengio 25:49 82.1. 吴恩达采访 Yoshua Bengio


## 83 83.2. 吴恩达采访 林元庆 13:38 83.2. 吴恩达采访 林元庆


## 84 84.1.1 为什么是 ML 策略 02:44 84.1.1 为什么是 ML 策略


## 85 85.1.2 正交化 10:39 85.1.2 正交化


## 86 86.1.3 单一数字评估指标 07:17 86.1.3 单一数字评估指标


## 87 87.1.4 满足和优化指标 05:59 87.1.4 满足和优化指标


## 88 88.1.5 训练 \_ 开发 \_ 测试集划分 06:36 88.1.5 训练 _ 开发 _ 测试集划分


## 89 89.1.6 开发集合测试集的大小 05:40 89.1.6 开发集合测试集的大小


## 90 90.1.7 什么时候该改变开发 \_ 测试集和指标 11:08 90.1.7 什么时候该改变开发 _ 测试集和指标


## 91 91.1.8 为什么是人的表现 05:47 91.1.8 为什么是人的表现


## 92 92.1.9 可避免偏差 07:00 92.1.9 可避免偏差


## 93 93.1.10 理解人的表现 11:13 93.1.10 理解人的表现


## 94 94.1.11 超过人的表现 06:22 94.1.11 超过人的表现


## 95 95.1.12 改善你的模型的表现 04:35 95.1.12 改善你的模型的表现


## 96 96.2.1 进行误差分析 10:33 96.2.1 进行误差分析


## 97 97.2.2 清除标注错误的数据 13:06 97.2.2 清除标注错误的数据


## 98 98.2.3 快速搭建你的第一个系统，并进行迭代 06:02 98.2.3 快速搭建你的第一个系统，并进行迭代


## 99 99.2.4 在不同的划分上进行训练并测试 10:56 99.2.4 在不同的划分上进行训练并测试


## 100 100.2.5 不匹配数据划分的偏差和方差 18:17 100.2.5 不匹配数据划分的偏差和方差


## 101 101.2.6 解决数据不匹配 10:09 101.2.6 解决数据不匹配


## 102 102.2.7 迁移学习 11:18 102.2.7 迁移学习


## 103 103.2.8 多任务学习 13:00 103.2.8 多任务学习


## 104 104.2.9 什么是端到端的深度学习 11:48 104.2.9 什么是端到端的深度学习


## 105 105.2.10 是否要使用端到端的深度学习 10:20 105.2.10 是否要使用端到端的深度学习


## 106 106.1. 采访 Andrej Karpathy 15:11 106.1. 采访 Andrej Karpathy


## 107 107.2. 采访 Ruslan Salakhutdinov 17:09 107.2. 采访 Ruslan Salakhutdinov


## 108 108.1.1 计算机视觉 05:45 108.1.1 计算机视觉


## 109 109.1.2 边缘检测示例 11:32 109.1.2 边缘检测示例


## 110 110.1.3 更多边缘检测内容 07:58 110.1.3 更多边缘检测内容


## 111 111.1.4 Padding 09:50 111.1.4 Padding


## 112 112.1.5 卷积步长 09:03 112.1.5 卷积步长


## 113 113.1.6 三维卷积 10:45 113.1.6 三维卷积


## 114 114.1.7 单层卷积网络 16:11 114.1.7 单层卷积网络


## 115 115.1.8 简单卷积网络示例 08:33 115.1.8 简单卷积网络示例


## 116 116.1.9 池化层 10:26 116.1.9 池化层


## 117 117.1.10 卷积神经网络示例 12:38 117.1.10 卷积神经网络示例


## 118 118.1.11 为什么使用卷积？ 09:41 118.1.11 为什么使用卷积？


## 119 119.2.1 为什么要进行实例探究？ 03:09 119.2.1 为什么要进行实例探究？


## 120 120.2.2 经典网络 18:20 120.2.2 经典网络


## 121 121.2.3 残差网络 07:09 121.2.3 残差网络


## 122 122.2.4 残差网络为什么有用？ 09:13 122.2.4 残差网络为什么有用？


## 123 123.2.5 网络中的网络以及 1×1 卷积 06:41 123.2.5 网络中的网络以及 1×1 卷积


## 124 124.2.6 谷歌 Inception 网络简介 10:15 124.2.6 谷歌 Inception 网络简介


## 125 125.2.7 Inception 网络 08:47 125.2.7 Inception 网络


## 126 126.2.8 使用开源的实现方案 04:57 126.2.8 使用开源的实现方案


## 127 127.2.9 迁移学习 08:49 127.2.9 迁移学习


## 128 128.2.10 数据扩充 09:32 128.2.10 数据扩充


## 129 129.2.11 计算机视觉现状 12:39 129.2.11 计算机视觉现状


## 130 130.3.1 目标定位 11:55 130.3.1 目标定位


## 131 131.3.2 特征点检测 05:57 131.3.2 特征点检测


## 132 132.3.3 目标检测 05:50 132.3.3 目标检测


## 133 133.3.4 卷积的滑动窗口实现 11:09 133.3.4 卷积的滑动窗口实现


## 134 134.3.5 Bounding Box 预测 14:32 134.3.5 Bounding Box 预测


## 135 135.3.6 交并比 04:20 135.3.6 交并比


## 136 136.3.7 非极大值抑制 08:03 136.3.7 非极大值抑制


## 137 137.3.8 Anchor Boxes 09:44 137.3.8 Anchor Boxes


## 138 138.3.9 YOLO 算法 07:02 138.3.9 YOLO 算法


## 139 139.3.10 候选区域 06:28 139.3.10 候选区域


## 140 140.4.1 什么是人脸识别？ 04:38 140.4.1 什么是人脸识别？


## 141 141.4.2 One-Shot 学习 04:46 141.4.2  One-Shot 学习


## 142 142.4.3 Siamese 网络 04:36 142.4.3  Siamese 网络


## 143 143.4.4 Triplet 损失 15:31 143.4.4  Triplet 损失


## 144 144.4.5 面部验证与二分类 06:06 144.4.5  面部验证与二分类


## 145 145.4.6 什么是神经风格转换？ 02:03 145.4.6  什么是神经风格转换？


## 146 146.4.7 什么是深度卷积网络？ 07:58 146.4.7  什么是深度卷积网络？


## 147 147.4.8 代价函数 \_bilibili 03:51 147.4.8 代价函数 _bilibili


## 148 148.4.9 内容代价函数 03:38 148.4.9  内容代价函数


## 149 149.4.10 风格代价函数 17:02 149.4.10  风格代价函数


## 150 150.4.11 一维到三维推广 09:09 150.4.11 一维到三维推广


## 151 151.1.1 为什么选择序列模型 03:01 151.1.1 为什么选择序列模型


## 152 152.1.2 数学符号 09:16 152.1.2 数学符号


## 153 153.1.3 循环神经网络 16:32 153.1.3 循环神经网络


## 154 154.1.4 通过时间的反向传播 06:12 154.1.4 通过时间的反向传播


## 155 155.1.5 不同类型的循环神经网络 09:35 155.1.5 不同类型的循环神经网络


## 156 156.1.6 语言模型和序列生成 12:02 156.1.6 语言模型和序列生成


## 157 157.1.7 新序列采样 08:39 157.1.7 新序列采样


## 158 158.1.8 带有神经网络的梯度消失 06:29 158.1.8 带有神经网络的梯度消失


## 159 159.1.9 GRU 单元 17:07 159.1.9 GRU 单元


## 160 160.1.10 长短期记忆（LSTM） 09:54 160.1.10 长短期记忆（LSTM）


## 161 161.1.11 双向神经网络 08:20 161.1.11 双向神经网络


## 162 162.1.12 深层循环神经网络 05:17 162.1.12 深层循环神经网络


## 163 163.2.1 词汇表征 10:08 163.2.1 词汇表征


## 164 164.2.2 使用词嵌入 09:23 164.2.2 使用词嵌入


## 165 165.2.3 词嵌入的特性 11:55 165.2.3 词嵌入的特性


## 166 166.2.4 嵌入矩阵 05:58 166.2.4 嵌入矩阵


## 167 167.2.5 学习词嵌入 10:10 167.2.5 学习词嵌入


## 168 168.2.6 Word2Vec 12:48 168.2.6 Word2Vec


## 169 169.2.7 负采样 11:54 169.2.7 负采样


## 170 170.2.8 GloVe 词向量 11:09 170.2.8 GloVe 词向量


## 171 171.2.9 情绪分类 07:38 171.2.9 情绪分类


## 172 172.2.10 词嵌入除偏 11:09 172.2.10 词嵌入除偏


## 173 173.3.1 基础模型 06:19 173.3.1 基础模型


## 174 174.3.2 选择最可能的句子 08:57 174.3.2 选择最可能的句子


## 175 175.3.3 定向搜索 11:55 175.3.3 定向搜索


## 176 176.3.4 改进定向搜索 11:01 176.3.4 改进定向搜索


## 177 177.3.5 定向搜索的误差分析 09:44 177.3.5 定向搜索的误差分析


## 178 178.3.6 Bleu 得分（选修） 16:27 178.3.6 Bleu 得分（选修）


## 179 179.3.7 注意力模型直观理解 09:42 179.3.7 注意力模型直观理解


## 180 180.3.8 注意力模型 12:24 180.3.8 注意力模型


## 181 181.3.9 语音辨识 08:54 181.3.9 语音辨识


## 182 182.3.10 触发字检测 05:04 182.3.10 触发字检测


## 183 183.3.11 结论和致谢 02:45 183.3.11 结论和致谢



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2022-10-11-ml-DL-AndrewNg.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://www.bilibili.com/video/BV1Bq421A74G/]({% include relrefx.html url="/backup/2022-10-11-ml-DL-AndrewNg.md/www.bilibili.com/738a77eb.html" %})
- [https://www.bilibili.com/video/BV1Gm421u73z/]({% include relrefx.html url="/backup/2022-10-11-ml-DL-AndrewNg.md/www.bilibili.com/c8eafc49.html" %})
- [http://www.atoolbox.net/Tool.php?Id=715]({% include relrefx.html url="/backup/2022-10-11-ml-DL-AndrewNg.md/www.atoolbox.net/ecf02067.php" %})

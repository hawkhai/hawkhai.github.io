---
layout: post
title: "机器学习 -- NCNN 特性总结"
author:
location: "珠海"
categories: ["机器学习"]
tags: ["机器学习"]
toc: true
toclistyle: none
comments:
visibility:
mathjax:
mermaid:
glslcanvas:
codeprint:
---

* 整张图做计算，显存消耗太大，所以分块做，降低显存占用峰值。
* 做视频超分时，传统做法是将视频解码出很多图片，针对每张图片做超分，最后将全部图片合成为视频。
    通过多线程编解码，超分过程中使用多 GPU 和多 CPU 并行加速。
* CPU 驱动长时间无法调度资源用于 UI 渲染，一次性提交大量 GPU 任务，影响 GPU 显示，UI 卡顿的问题。解决办法，将任务拆小。
* GPU 模型加载问题。GPU 加载模型的时候有 shader 编译的过程，非常消耗资源和时间。
    有些算子的参数是一样的，对参数做 hash，作为 key，只有第一次使用该算子的时候进行 shader 编译，
    后面就可以直接复用编译好的 pipeline 对象，这样可以加速模型加载。
* 内存池复用技术。后面的算子使用前面释放出来的算子所占用的内存。
* 动态尺寸输入。输入多大图就计算多大图，无需 padding 到原图尺寸，节省时间。
* 动态任务分配。在多任务网络中，根据前面的推理结果决定下一步推理流程。
* 算子融合加速。两个运算合并成一个（比如 min 和 max），可以提高推理速度。
* 手机大小核调度。
    * 大核心 CPU：速度快、耗电高。通过线程池绑定的方式，将在前台跑的、实时性要求很高的任务绑定在大核心 CPU 上。
    * 小核心 CPU：速度慢、耗电低。放一个在后台偷偷跑的任务，不会让前台卡顿。
* OpenMP 里面的 busywait 过程。某个线程结束时并不会立即放弃 CPU，会使用自旋锁等待下一个任务分配，适合实时性要求较高，但是消耗 CPU 占用率。
    禁用之可以降低 CPU 占用率。
* 优化内存布局。
    * 推理框架的一般布局是（n,c,h,w）这样的布局下，遍历 channel 时指针是跳跃地访问地址的，有明显的访问延迟。
        改为（h,w,c），将 channel 维放在最内层，每个像素对应的 c 个通道在内存中地址是连续的便于快速访问。
    * 使用 FP16 tensor 和 BF16 tensor 替换 FP32 tesor 可以节约内存。
* 模型量化技术。浮点数做运算比整数更慢，功耗也更高。针对卷积层做量化处理，float32 转为 int8，
    再只用整数的乘法和加法实现卷积层，最后输出 int32，再反量化，转为浮点数。

[来源 {% include relref_bili.html %}](//www.bilibili.com/video/BV1P84y1F7Wz/)
[在线管线缓存加速 ncnn gpu 模型加载 {% include relref_zhihu.html %}](https://zhuanlan.zhihu.com/p/156796191)

那么，离线管线缓存呢？
* PC 上实验下来没有加速效果，因为驱动暗地里已经悄悄帮我缓存了。
* 离线缓存，需要存出一个二进制文件，要设计新的接口，调用方也要加新的代码，具体怎么存储和加载没想清楚。
* 最重要的问题，这个离线缓存如何保持兼容性？系统或驱动升级不兼容，换硬件不兼容，缓存文件损坏如何处理等等 ...

* 我当然是知道的，慢就慢在创建 pipeline 这里，vkCreateComputePipelines。
    ncnn 每个 gpu 算子都是用 shader 拼出来的，模型里有各种 op，所有 op 的所有 shader 最终都要经过这里，
    让驱动编译出 gpu 能跑的东西。shader 多了就慢了，而且因为某些驱动 bug，还不能多线程创建 pipeline。


## onnxruntime-directml 1.16.3

pip install onnxruntime-directml

<https://github.com/microsoft/DirectML>
<https://onnxruntime.ai/docs/execution-providers/DirectML-ExecutionProvider.html>
<https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx/fns_candy_style_transfer>
<https://github.com/microsoft/onnxruntime-inference-examples>

<https://github.com/fdwr/OnnxRuntimeDirectMLEPSample>

<https://www.nuget.org/packages/Microsoft.AI.DirectML/>
1.13.0
<https://www.nuget.org/packages/Microsoft.ML.OnnxRuntime.DirectML/>
1.16.3



<hr class='reviewline'/>
<p class='reviewtip'><script type='text/javascript' src='{% include relref.html url="/assets/reviewjs/blogs/2023-10-25-ncnn.md.js" %}'></script></p>
<font class='ref_snapshot'>参考资料快照</font>

- [https://zhuanlan.zhihu.com/p/156796191]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/zhuanlan.zhihu.com/56866568.html" %})
- [https://github.com/microsoft/DirectML]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/github.com/e2a8d35e.html" %})
- [https://onnxruntime.ai/docs/execution-providers/DirectML-ExecutionProvider.html]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/onnxruntime.ai/47d8363d.html" %})
- [https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx/fns_candy_style_transfer]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/github.com/6dd556aa.html" %})
- [https://github.com/microsoft/onnxruntime-inference-examples]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/github.com/b85af4eb.html" %})
- [https://github.com/fdwr/OnnxRuntimeDirectMLEPSample]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/github.com/16224c4d.html" %})
- [https://www.nuget.org/packages/Microsoft.AI.DirectML/]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/www.nuget.org/0b36c8c1.html" %})
- [https://www.nuget.org/packages/Microsoft.ML.OnnxRuntime.DirectML/]({% include relrefx.html url="/backup/2023-10-25-ncnn.md/www.nuget.org/9025f228.html" %})

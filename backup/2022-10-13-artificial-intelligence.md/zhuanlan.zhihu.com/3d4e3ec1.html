<html lang="zh" data-hairline="true" data-theme="light" data-rh="data-theme"><head><meta charset="utf-8"><title>机器学习面试干货精讲 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-rh="true" name="keywords" content="机器学习,数据挖掘,Kaggle"><meta data-rh="true" name="description" content="写于二零一八年二月十五日，农历大年三十 整体内容划分：推荐书单、公开课面试核心内容提纲本内容涉及模型核心数学公式，把本人面试中常被问到问题以及模型知识点的总结，起到提纲挈领作用，在准备的过程中抓住每…"><meta data-rh="true" property="og:title" content="机器学习面试干货精讲"><meta data-rh="true" property="og:url" content="https://zhuanlan.zhihu.com/p/32877396"><meta data-rh="true" property="og:description" content="写于二零一八年二月十五日，农历大年三十 整体内容划分：推荐书单、公开课面试核心内容提纲本内容涉及模型核心数学公式，把本人面试中常被问到问题以及模型知识点的总结，起到提纲挈领作用，在准备的过程中抓住每…"><meta data-rh="true" property="og:image" content="https://picd.zhimg.com/v2-379d54a5d9d4f5065efec52438a8a901_720w.jpg?source=172ae18b"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="og:site_name" content="知乎专栏"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.a53ae37b.png"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.a53ae37b.png" sizes="152x152"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.bbce8f18.png" sizes="120x120"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.cbade8f9.png" sizes="76x76"><link data-rh="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.8f6c52aa.png" sizes="60x60"><link crossorigin="" rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"><link crossorigin="" rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/heifetz/search.xml" title="知乎"><link rel="dns-prefetch" href="//static.zhimg.com"><link rel="dns-prefetch" href="//pica.zhimg.com"><link rel="dns-prefetch" href="//pic1.zhimg.com"><link rel="dns-prefetch" href="//pic2.zhimg.com"><link rel="dns-prefetch" href="//pic3.zhimg.com"><link rel="dns-prefetch" href="//pic4.zhimg.com"><link rel="dns-prefetch" href="//static.zhihu.com"><style data-emotion-css="1m4merm">.u-safeAreaInset-top{height:constant(safe-area-inset-top) !important;height:env(safe-area-inset-top) !important;}.u-safeAreaInset-bottom{height:constant(safe-area-inset-bottom) !important;height:env(safe-area-inset-bottom) !important;}</style><link href="https://static.zhihu.com/heifetz/column.app.216a26f4.da6bb3acea63642ce110.css" crossorigin="" rel="stylesheet"><script nonce="">!function(){"use strict";!function(e,n){var r=[];function t(e){return function(){r.push([e,arguments])}}n.Raven={captureException:t("captureException"),captureMessage:t("captureMessage"),captureBreadcrumb:t("captureBreadcrumb")};var a,o,c,i,s,u="undefined"!=typeof DOMError;function d(e){var n=e instanceof Error||e instanceof ErrorEvent||u&&e instanceof DOMError||e instanceof DOMException;Raven.captureException(n?e:new Error(e.message||e.reason))}n.addEventListener("unhandledrejection",d),n.addEventListener("error",d,!0),a=e.src,o=e,c=function(){r.forEach(function(e){var n;(n=Raven)[e[0]].apply(n,e[1])}),n.removeEventListener("unhandledrejection",d),n.removeEventListener("error",d,!0)},i=document.head||document.getElementsByTagName("head")[0],(s=document.createElement("script")).crossOrigin=o.crossOrigin,s.dataset.sentryConfig=o["data-sentry-config"],s.onload=c,s.src=a,i.appendChild(s)}({"defer":true,"crossOrigin":"anonymous","src":"https://unpkg.zhimg.com/@cfe/sentry-script@1.3.1/dist/init.js","data-sentry-config":"{\"dsn\":\"https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224\",\"sampleRate\":0.1,\"release\":\"201-e744d5ea\",\"ignoreErrorNames\":[\"NetworkError\",\"SecurityError\"],\"ignoreErrorsPreset\":\"ReactApp\",\"tags\":{\"app_name\":\"heifetz\"}}"},window)}();
</script><script crossorigin="anonymous" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;201-e744d5ea&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrorsPreset&quot;:&quot;ReactApp&quot;,&quot;tags&quot;:{&quot;app_name&quot;:&quot;heifetz&quot;}}" src="https://unpkg.zhimg.com/@cfe/sentry-script@1.3.1/dist/init.js"></script><style data-emotion-css="uzm3ri">.css-uzm3ri{position:fixed;top:0;right:0;left:0;z-index:101;display:none;height:2px;pointer-events:none;background:#056DE8;-webkit-transform:translateX(-100%);-ms-transform:translateX(-100%);transform:translateX(-100%);}</style><style data-emotion-css="1l12z7y">.css-1l12z7y{box-shadow:0px 16px 32px rgba(0,0,0,0.04);}</style><style data-emotion-css="1hlrcxk">.css-1hlrcxk{-webkit-transition-property:fill;transition-property:fill;-webkit-transition-duration:0.25s;transition-duration:0.25s;-webkit-transition-timing-function:ease-in;transition-timing-function:ease-in;}</style><style data-emotion-css="icip60">.css-icip60{border-radius:2px;}</style><style data-emotion-css="1tzrnvf">.css-1tzrnvf{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:30px;height:30px;border-radius:2px;}</style><style data-emotion-css="78p1r9">.css-78p1r9{box-sizing:border-box;margin:0;min-width:0;margin-left:auto;margin-right:auto;max-width:690px;margin-top:0;}@media screen and (min-width:40em){.css-78p1r9{margin-top:1em;}}</style><style data-emotion-css="m9mk0j">.css-m9mk0j{position:relative;padding-bottom:56.25%;height:0;border-radius:inherit;}</style><style data-emotion-css="vn9cxj">.css-vn9cxj{box-sizing:border-box;margin:0;min-width:0;position:relative;padding-bottom:56.25%;height:0;border-radius:inherit;}</style><style data-emotion-css="1ld0bim">.css-1ld0bim{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:inherit;}</style><style data-emotion-css="1ujtx97">.css-1ujtx97{object-fit:cover;background-color:#F6F6F6;}</style><style data-emotion-css="uodor8">.css-uodor8{border-radius:50%;}</style><style data-emotion-css="1y9jkzv">.css-1y9jkzv{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:38px;height:38px;border-radius:50%;}</style><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="1yuhvjn">.css-1yuhvjn{margin-top:16px;}</style><style data-emotion-css="79elbk">.css-79elbk{position:relative;}</style><style data-emotion-css="3jt6os">.css-3jt6os .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-3jt6os .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-3jt6os .FileLinkCard-info{margin-left:12px;}.css-3jt6os .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-3jt6os .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-3jt6os .FileLinkCard-source{white-space:pre;}</style><style data-emotion-css="1wr1m8">.css-1wr1m8 .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-1wr1m8 .LinkCard.new,.css-1wr1m8 .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1wr1m8 .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-1wr1m8 .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-1wr1m8 .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-1wr1m8 .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-1wr1m8 .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-1wr1m8 .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-1wr1m8 .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-1wr1m8 .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-1wr1m8 .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-1wr1m8 .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-1wr1m8 .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-1wr1m8 .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-1wr1m8 .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-1wr1m8 .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-1wr1m8 .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-1wr1m8 .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-1wr1m8 .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-1wr1m8 .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-1wr1m8 .LinkCard.old,.css-1wr1m8 .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-1wr1m8 .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-1wr1m8 .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}</style><style data-emotion-css="hypxot">.css-hypxot .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-hypxot .LinkCard.old,.css-hypxot .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-hypxot .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-hypxot .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-hypxot .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-hypxot .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-hypxot .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-hypxot .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-hypxot .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-hypxot .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-hypxot .LinkCard.new,.css-hypxot .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-hypxot .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-hypxot .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-hypxot .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-hypxot .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-hypxot .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-hypxot .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-hypxot .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-hypxot .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-hypxot .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-hypxot .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-hypxot .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-hypxot .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-hypxot .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-hypxot .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-hypxot .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-hypxot .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-hypxot .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-hypxot .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-hypxot .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-hypxot .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-hypxot .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-hypxot .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-hypxot .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-hypxot .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-hypxot .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-hypxot .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-hypxot .FileLinkCard-info{margin-left:12px;}.css-hypxot .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-hypxot .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-hypxot .FileLinkCard-source{white-space:pre;}</style><style data-emotion-css="4em6pe animation-1yvu044">.css-4em6pe{word-break:break-word;line-height:1.6;}.css-4em6pe > [data-first-child]{margin-top:0;}.css-4em6pe > :last-child{margin-bottom:0;}.css-4em6pe h1,.css-4em6pe h2{clear:left;margin-top:calc((1.4em * 2) / 1.2);margin-bottom:calc(1.4em / 1.2);font-size:1.2em;line-height:1.5;font-weight:600;}.css-4em6pe h3,.css-4em6pe h4,.css-4em6pe h5,.css-4em6pe h6{clear:left;margin-top:calc((1.4em * 1.5) / 1.1);margin-bottom:calc(1.4em / 1.1);font-size:1.1em;line-height:1.5;font-weight:600;}.css-4em6pe u{-webkit-text-decoration:none;text-decoration:none;border-bottom:1px solid #444444;}.css-4em6pe b{font-weight:600;}.css-4em6pe sup{font-size:0.8em;}.css-4em6pe sup[data-draft-type='reference']{color:#175199;}.css-4em6pe a:focus{outline:none;-webkit-transition:box-shadow 0.3s;transition:box-shadow 0.3s;}html[data-focus-visible] .css-4em6pe a:focus{box-shadow:0 0 0 2px #FFFFFF,0 0 0 4px rgba(5,109,232,0.3);}.css-4em6pe a.ztext-link,.css-4em6pe a.internal,.css-4em6pe a.external{-webkit-text-decoration:none;text-decoration:none;cursor:pointer;border-bottom:1px solid #808080;}.css-4em6pe a.ztext-link:hover,.css-4em6pe a.internal:hover,.css-4em6pe a.external:hover{color:#175199;border-bottom:1px solid #175199;}.css-4em6pe a.ztext-link > .ellipsis::after,.css-4em6pe a.internal > .ellipsis::after,.css-4em6pe a.external > .ellipsis::after{content:'...';}.css-4em6pe a.ztext-link > .invisible,.css-4em6pe a.internal > .invisible,.css-4em6pe a.external > .invisible{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;}.css-4em6pe a.ztext-link u,.css-4em6pe a.internal u,.css-4em6pe a.external u{border:none;}.css-4em6pe a.member_mention{color:#175199;}.css-4em6pe a.member_mention:hover{border-bottom:1px solid #175199;}.css-4em6pe a.UserLink-link{color:#175199;}.css-4em6pe a.UserLink-link:hover{border-bottom:1px solid #175199;}.css-4em6pe p{margin:1.4em 0;}.css-4em6pe p.ztext-empty-paragraph{margin:calc((2.8em- (1.4em * 2 + 1.6em)) / 2) 0;}.css-4em6pe p.ztext-empty-paragraph + .ztext-empty-paragraph{margin:1.4em 0;}.css-4em6pe hr{margin:4em auto;width:240px;max-width:100%;border:none;border-top:1px solid #D3D3D3;}.css-4em6pe img[eeimg]{max-width:100%;vertical-align:middle;}.css-4em6pe img[eeimg="1"]{margin:0 3px;max-width:calc(100% - 6px);display:inline-block;}.css-4em6pe img[eeimg="2"]{margin:1.4em auto;display:block;}.css-4em6pe blockquote{margin:1.4em 0;padding-left:1em;color:#646464;border-left:3px solid #D3D3D3;}.css-4em6pe ol,.css-4em6pe ul{margin:1.4em 0;padding:0;width:100%;}.css-4em6pe ol ol,.css-4em6pe ul ol,.css-4em6pe ol ul,.css-4em6pe ul ul{margin:0;}.css-4em6pe ol li::before,.css-4em6pe ul li::before{width:1em;}.css-4em6pe ol > ol,.css-4em6pe ul > ol,.css-4em6pe ol > ul,.css-4em6pe ul > ul{display:table-row;}.css-4em6pe ol > ol::before,.css-4em6pe ul > ol::before,.css-4em6pe ol > ul::before,.css-4em6pe ul > ul::before{display:table-cell;content:'';}.css-4em6pe ul{display:table;}.css-4em6pe ul>li{display:table-row;list-style:none;}.css-4em6pe ul>li::before{display:table-cell;content:'•  ';white-space:pre;}.css-4em6pe ol{display:table;counter-reset:ol;}.css-4em6pe ol > li{display:table-row;list-style:none;}.css-4em6pe ol > li::before{display:table-cell;text-align:right;counter-increment:ol;content:counter(ol) '. ';white-space:pre;}.css-4em6pe ol ol{counter-reset:ol2;}.css-4em6pe ol ol li::before{counter-increment:ol2;content:counter(ol2) '. ';}.css-4em6pe ol ol ol{counter-reset:ol3;}.css-4em6pe ol ol ol li::before{counter-increment:ol3;content:counter(ol3) '. ';}.css-4em6pe ol ol ol ol{counter-reset:ol4;}.css-4em6pe ol ol ol ol li::before{counter-increment:ol4;content:counter(ol4) '. ';}.css-4em6pe figure{margin:1.4em 0;}.css-4em6pe figure .content_image,.css-4em6pe figure .origin_image{margin:0 auto;}.css-4em6pe figure figcaption{margin-top:calc(0.6em / 0.9);padding:0 1em;font-size:0.9em;line-height:1.5;text-align:center;color:#999999;}.css-4em6pe figure + figure{margin-top:calc(1.4em * 1.6);}.css-4em6pe figure[data-size='small'],.css-4em6pe figure:not([data-size]) > [data-size='small']{clear:both;}.css-4em6pe figure[data-size='left'],.css-4em6pe figure:not([data-size]) > [data-size='left']{float:left;margin:0 20px 20px 0;max-width:33%;}.css-4em6pe figure[data-size='right'],.css-4em6pe figure:not([data-size]) > [data-size='right']{float:right;margin:0 0 20px 20px;max-width:33%;}.css-4em6pe figure[data-size='collapse']{margin-bottom:0;}.css-4em6pe figure[data-size='collapse'] + figure{margin-top:0;}.css-4em6pe .content_image,.css-4em6pe .origin_image{display:block;max-width:100%;height:auto;margin:1.4em auto;}.css-4em6pe .content_image[data-size='small'],.css-4em6pe .origin_image[data-size='small']{max-width:40%;}.css-4em6pe .content_image.zh-lightbox-thumb,.css-4em6pe .origin_image.zh-lightbox-thumb{cursor:-webkit-zoom-in;cursor:-moz-zoom-in;cursor:zoom-in;}.css-4em6pe code{margin:0 2px;padding:3px 4px;border-radius:3px;font-family:Menlo,Monaco,Consolas,'Andale Mono','lucida console','Courier New',monospace;font-size:0.9em;background-color:#F6F6F6;}.css-4em6pe pre{margin:1.4em 0;padding:calc(0.8em / 0.9);font-size:0.9em;word-break:initial;word-wrap:initial;white-space:pre;overflow:auto;-webkit-overflow-scrolling:touch;background:#F6F6F6;border-radius:4px;}.css-4em6pe pre code{margin:0;padding:0;font-size:inherit;border-radius:0;background-color:inherit;}.css-4em6pe li pre{white-space:pre-wrap;}.css-4em6pe table[data-draft-type='table']{border-collapse:collapse;font-size:15px;margin:1.4em auto;max-width:100%;table-layout:fixed;text-align:left;width:100%;}.css-4em6pe table[data-draft-type='table'][data-size='small']{min-width:260px;width:40%;}.css-4em6pe table[data-draft-type='table'][data-row-style='striped'] tr:nth-of-type(2n + 1){background:#F6F6F6;}.css-4em6pe table[data-draft-type='table'] td,.css-4em6pe table[data-draft-type='table'] th{border:1px solid #D3D3D3;line-height:24px;height:24px;padding:3px 12px;}.css-4em6pe table[data-draft-type='table'] th{background:#EBEBEB;color:#121212;font-weight:500;}.css-4em6pe .video-box,.css-4em6pe .link-box{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;margin:1.4em 0;overflow:auto;white-space:normal;cursor:pointer;border:solid 1px #EBEBEB;border-radius:4px;}.css-4em6pe .lazy[data-lazy-status]{background-color:#F6F6F6;}.css-4em6pe .lazy[data-lazy-status="ok"]{background-color:transparent;-webkit-animation:animation-1yvu044 0.5s ease-in;animation:animation-1yvu044 0.5s ease-in;}.css-4em6pe .highlight{margin:1em 0;}.css-4em6pe .highlight pre{margin:0;}.css-4em6pe .highlight .hll{background-color:#FDFDFD;}.css-4em6pe .highlight .c{font-style:italic;color:#999999;}.css-4em6pe .highlight .err{color:#F1403C;}.css-4em6pe .highlight .k{font-weight:600;}.css-4em6pe .highlight .o{font-weight:600;}.css-4em6pe .highlight .cm{font-style:italic;color:#999999;}.css-4em6pe .highlight .cp{font-weight:600;color:#999999;}.css-4em6pe .highlight .c1{font-style:italic;color:#999999;}.css-4em6pe .highlight .cs{font-style:italic;font-weight:600;color:#999999;}.css-4em6pe .highlight .gd{color:#FF3366;}.css-4em6pe .highlight .ge{font-style:italic;}.css-4em6pe .highlight .gr{color:#F1403C;}.css-4em6pe .highlight .gh{color:#999999;}.css-4em6pe .highlight .gi{color:#12b370;}.css-4em6pe .highlight .go{color:#808080;}.css-4em6pe .highlight .gp{color:#646464;}.css-4em6pe .highlight .gs{font-weight:600;}.css-4em6pe .highlight .gu{color:#999999;}.css-4em6pe .highlight .gt{color:#F1403C;}.css-4em6pe .highlight .kc{font-weight:600;}.css-4em6pe .highlight .kd{font-weight:600;}.css-4em6pe .highlight .kn{font-weight:600;}.css-4em6pe .highlight .kp{font-weight:600;}.css-4em6pe .highlight .kr{font-weight:600;}.css-4em6pe .highlight .kt{font-weight:600;color:#175199;}.css-4em6pe .highlight .m{color:#056DE8;}.css-4em6pe .highlight .s{color:#F1403C;}.css-4em6pe .highlight .na{color:#056DE8;}.css-4em6pe .highlight .nb{color:#056DE8;}.css-4em6pe .highlight .nc{font-weight:600;color:#175199;}.css-4em6pe .highlight .no{color:#056DE8;}.css-4em6pe .highlight .ni{color:#5555DD;}.css-4em6pe .highlight .ne{font-weight:600;color:#F1403C;}.css-4em6pe .highlight .nf{font-weight:600;color:#F1403C;}.css-4em6pe .highlight .nn{color:#646464;}.css-4em6pe .highlight .nt{color:#175199;}.css-4em6pe .highlight .nv{color:#056DE8;}.css-4em6pe .highlight .ow{font-weight:600;}.css-4em6pe .highlight .w{color:#BFBFBF;}.css-4em6pe .highlight .mf{color:#056DE8;}.css-4em6pe .highlight .mh{color:#056DE8;}.css-4em6pe .highlight .mi{color:#056DE8;}.css-4em6pe .highlight .mo{color:#056DE8;}.css-4em6pe .highlight .sb{color:#F1403C;}.css-4em6pe .highlight .sc{color:#F1403C;}.css-4em6pe .highlight .sd{color:#F1403C;}.css-4em6pe .highlight .s2{color:#F1403C;}.css-4em6pe .highlight .se{color:#F1403C;}.css-4em6pe .highlight .sh{color:#F1403C;}.css-4em6pe .highlight .si{color:#F1403C;}.css-4em6pe .highlight .sx{color:#F1403C;}.css-4em6pe .highlight .sr{color:#A5542F;}.css-4em6pe .highlight .s1{color:#F1403C;}.css-4em6pe .highlight .ss{color:#F1403C;}.css-4em6pe .highlight .bp{color:#999999;}.css-4em6pe .highlight .vc{color:#056DE8;}.css-4em6pe .highlight .vg{color:#056DE8;}.css-4em6pe .highlight .vi{color:#056DE8;}.css-4em6pe .highlight .il{color:#056DE8;}.css-4em6pe .highlight::-webkit-scrollbar{width:6px;height:6px;}.css-4em6pe .highlight::-webkit-scrollbar-thumb:horizontal{background-color:rgba(18,18,18,0.5);border-radius:6px;}.css-4em6pe .highlight::-webkit-scrollbar-thumb:horizontal:hover{background-color:rgba(18,18,18,0.6);}.css-4em6pe .LinkCard.old{position:relative;display:block;margin:1em auto;width:390px;box-sizing:border-box;border-radius:12px;max-width:100%;overflow:hidden;}.css-4em6pe .LinkCard.old,.css-4em6pe .LinkCard.old:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-4em6pe .LinkCard-ecommerceLoadingCard{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;padding:12px;border-radius:inherit;height:80px;box-sizing:border-box;background:rgba(246,246,246,0.88);color:#D3D3D3;}.css-4em6pe .LinkCard-ecommerceLoadingCardAvatarWrapper{width:60px;height:60px;background:#EBEBEB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;border-radius:6px;margin-right:10px;}.css-4em6pe .LinkCard-ecommerceLoadingCardNetwork{width:20px;height:20px;}.css-4em6pe .LinkCard-ecommerceLoadingCardLoadingbar{height:60px;-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.css-4em6pe .LinkCard-ecommerceLoadingCardLoadingbar span{height:16px;display:inline-block;background:#EBEBEB;}.css-4em6pe .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(1){width:60px;margin-bottom:4px;}.css-4em6pe .LinkCard-ecommerceLoadingCardLoadingbar span:nth-of-type(2){width:127px;}.css-4em6pe .LinkCard.new{position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;box-sizing:border-box;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:390px;min-height:84px;border-radius:8px;max-width:100%;overflow:hidden;margin:16px auto;padding:12px 12px 9px 12px;background-color:#F6F6F6;}.css-4em6pe .LinkCard.new,.css-4em6pe .LinkCard.new:hover{-webkit-text-decoration:none;text-decoration:none;border:none !important;color:inherit !important;}.css-4em6pe .LinkCard.new .LinkCard-contents{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;}.css-4em6pe .LinkCard.new .LinkCard-contents .loading{height:14px;background:#EBEBEB;border-radius:7px;}.css-4em6pe .LinkCard.new .LinkCard-contents.withTitle{margin-bottom:3px;}.css-4em6pe .LinkCard.new .LinkCard-title{display:-webkit-box;font-size:15px;font-weight:500;line-height:1.4;margin-bottom:2px;color:#121212;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-4em6pe .LinkCard.new .LinkCard-title.two-line{line-height:20px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-4em6pe .LinkCard.new .LinkCard-title.loading{margin-bottom:8px;width:80%;}.css-4em6pe .LinkCard.new .LinkCard-title.loading.withTitle{margin-bottom:6px;}.css-4em6pe .LinkCard.new .LinkCard-title.loadingTitle{margin-bottom:5px;}.css-4em6pe .LinkCard.new .LinkCard-excerpt{display:-webkit-box;text-overflow:ellipsis;font-size:13px;line-height:18px;color:#999999;margin-bottom:4px;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-4em6pe .LinkCard.new .LinkCard-excerpt .LinkCard-author{color:#444444;}.css-4em6pe .LinkCard.new .LinkCard-desc{display:-webkit-box;font-size:13px;height:18px;line-height:18px;color:#999999;word-break:break-all;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:1;}.css-4em6pe .LinkCard.new .LinkCard-desc .LinkCard-tag,.css-4em6pe .LinkCard.new .LinkCard-desc .tag{display:inline-block;font-size:11px;margin-left:8px;padding:0 4px;border-radius:3px;background:rgba(211,211,211,0.3);}.css-4em6pe .LinkCard.new .LinkCard-desc.loading{width:40%;}.css-4em6pe .LinkCard.new .LinkCard-desc svg{margin-right:2px;}.css-4em6pe .LinkCard.new .LinkCard-image{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;background-color:#EBEBEB;background-size:cover;background-position:center;position:relative;display:block;width:60px;height:60px;margin-left:20px;object-fit:cover;border-radius:inherit;overflow:hidden;}.css-4em6pe .LinkCard.new .LinkCard-image.LinkCard-image--default{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;background-color:#EBEBEB;color:#D3D3D3;}.css-4em6pe .LinkCard.new .LinkCard-image.LinkCard-image--default svg{color:#999999;}.css-4em6pe .LinkCard.new .LinkCard-image img{width:100%;height:100%;object-fit:cover;}.css-4em6pe .LinkCard.new .LinkCard-image .LinkCard-image--video{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;position:absolute;top:50%;left:50%;-webkit-transform:translateX(-50%) translateY(-50%);-ms-transform:translateX(-50%) translateY(-50%);transform:translateX(-50%) translateY(-50%);width:24px;height:24px;border-radius:12px;background:rgba(255,255,255,0.9);pointer-events:none;}.css-4em6pe .LinkCard.new .LinkCard-image .LinkCard-image--video svg{color:#444444;}.css-4em6pe .LinkCard.new .LinkCard-richText .text{color:#444444;}.css-4em6pe .LinkCard.new .LinkCard-richText .bold{font-weight:600;}.css-4em6pe .LinkCard.new .LinkCard-richText .tag{margin-left:4px;}.css-4em6pe .FileLinkCard{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(246,246,246,0.88);border-radius:12px;box-sizing:border-box;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;margin:1em auto;max-width:100%;overflow:hidden;padding:12px;position:relative;width:390px;}.css-4em6pe .FileLinkCard-icon{-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;height:30px;width:30px;}.css-4em6pe .FileLinkCard-info{margin-left:12px;}.css-4em6pe .FileLinkCard-name{color:#121212;font-size:15px;font-weight:500;line-height:21px;display:-webkit-box;text-overflow:ellipsis;overflow:hidden;-webkit-box-orient:vertical;-webkit-line-clamp:2;}.css-4em6pe .FileLinkCard-meta{color:#999999;font-size:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;line-height:14px;margin-top:5px;}.css-4em6pe .FileLinkCard-source{white-space:pre;}@-webkit-keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}@keyframes animation-1yvu044{from{opacity:0;}to{opacity:1;}}</style><style data-emotion-css="1s3a4zw">.css-1s3a4zw{position:relative;display:inline-block;height:30px;padding:0 12px;font-size:14px;line-height:30px;color:#056DE8;vertical-align:top;border-radius:100px;background:rgba(5,109,232,0.1);}.css-1s3a4zw:hover{background-color:rgba(5,109,232,0.15);}</style><style data-emotion-css="1xlfegr">.css-1xlfegr{background:transparent;box-shadow:none;}</style><style data-emotion-css="1gomreu">.css-1gomreu{position:relative;display:inline-block;}</style><style data-emotion-css="1any501">.css-1any501{box-sizing:border-box;margin:0;min-width:0;max-width:100%;height:auto;background-color:#FFFFFF;width:40px;height:40px;border-radius:50%;}</style><style data-emotion-css="u1frr4">.css-u1frr4{width:724px;}.css-u1frr4 .Modal-content{margin:22px 0 5px;}.css-u1frr4 .Creator-QuestionShared-title{padding-right:65px;}</style><style>.kwai-player{position:relative;z-index:1;display:inline-block}.kwai-player video{display:block;width:100%;height:100%}.kwai-player canvas{display:block;margin:0 auto;max-width:100%;max-height:100%}.kwai-player-poster{width:100%;height:100%;position:absolute;left:0;top:0;background-color:#333;background-size:cover}.kwai-player-hotzone{display:block;width:100%;height:100%;position:absolute;left:0;top:0}.kwai-player-controls{position:absolute;left:0;bottom:0;width:100%}</style><style></style><style></style><style>.controls{position:absolute;top:0;left:0;width:100%;height:100%;font-size:16px;font-family:Roboto,Noto,sans-serif;overflow:hidden}.controls .large{font-size:18px}.panel{position:relative;display:flex;flex-direction:column;justify-content:flex-end;height:100%;width:100%;background:0 bottom repeat-x linear-gradient(rgba(0,0,0,0),rgb(0,0,0));background-size:auto 5.75em;transition:opacity .3s ease-in}.button-panel{display:flex;position:relative;z-index:1;width:100%;justify-content:space-between}.button-panel-left,.button-panel-right{display:flex;flex-direction:row;align-items:center}.panel-button{position:relative;width:3em;height:3em;background:center no-repeat;background-size:1.25em 1.25em;cursor:pointer;-webkit-tap-highlight-color:transparent}.panel-button:hover>.panel-button-bg{background-color:#202124b5}.panel-button.disabled,.panel-button.disabled:hover>.panel-button-bg{background-color:initial;opacity:.3;cursor:default}.panel-button-wrapper{position:relative}.panel-button-wrapper.disabled:hover .panel-button-bg{left:0;background-color:transparent}.panel-button-wrapper:hover .panel-button-bg{left:0;background-color:#202124b5}.panel-button-wrapper .panel-button-bg{width:100%;left:0}.panel-button-bg{background-size:24px;position:absolute;width:36px;height:36px;left:6px;top:6px;pointer-events:none;z-index:-1;background-repeat:no-repeat;background-position:center center;border-radius:18px;transition:background-color .25s ease 0s}.panel-button-bg:hover{background-color:#202124b5}.current-time-display{font-size:.875em;color:#fff}.time-remaining-display{margin-left:.25em;font-size:.875em;color:#fff;white-space:nowrap}.timeline{display:block;margin:0 1em;padding-bottom:1.25em;cursor:pointer}.timeline:hover .thumb{opacity:1}.volume-button{display:flex;align-items:center;padding-left:1em}.volume-button .segmented-track{width:0em;transition:width .3s ease 0s,opacity .28s step-end 0s}.volume-button.disabled:hover .segmented-track{width:0em}.volume-button:hover .segmented-track{width:3.25em;transition:width .3s ease 0s,opacity .28s step-end 0s}.volume-progress{padding:1em 0;cursor:pointer}.segmented-track{position:relative;height:.25em;user-select:none}.thumb{position:absolute;top:-4px;width:12px;height:12px;background:rgb(255,255,255);border-radius:50%;opacity:0;transition:opacity .3s ease 0s}.segment-background{height:.25em;background-color:#ffffff4d}.segment-highlight-before{position:absolute;height:4px;border-radius:2px;background:rgb(255,255,255)}.segment-highlight-after{position:absolute;height:4px;border-radius:2px;background:rgba(255,255,255,.54)}.overlay{position:absolute;top:0;left:0;width:100%;height:100%;background:center no-repeat;background-size:contain}.overflow{position:absolute;right:16px;bottom:4px;z-index:20;max-height:250px;width:200px;overflow-x:hidden;white-space:nowrap;font-size:12px;box-shadow:#0006 0 1px 9px;background:#ffffff;border-radius:2px;transition:transform .3s ease-out,opacity .2s linear;transform-origin:right bottom;opacity:1;transform:scale(1)}.overflow.closed{transform:scale(0);transform-origin:right bottom;opacity:0}.overflow-item{height:3.75em;padding:0 1.375em;display:flex;align-items:center;cursor:pointer;user-select:none}.overflow-item:active,.overflow-item:focus,.overflow-item:hover{background-color:#e0e0e0}@media screen and (max-width: 600px){.overflow-item.overflow-item:hover{background-color:#fff}}.overflow-icon{width:2em;height:2em;margin-right:1em}.overflow-checkbox{-webkit-appearance:none;appearance:none;width:1.125em;height:1.125em;margin:.9375em;border:none}.overflow-checkbox:checked{background-image:url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjIuMSwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPgo8c3ZnIHZlcnNpb249IjEuMSIgaWQ9IkxheWVyXzEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIHg9IjBweCIgeT0iMHB4IgoJIHZpZXdCb3g9IjAgMCAyNCAyNCIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgMjQgMjQ7IiB4bWw6c3BhY2U9InByZXNlcnZlIj4KPHN0eWxlIHR5cGU9InRleHQvY3NzIj4KCS5zdDB7ZmlsbDpub25lO30KCS5zdDF7ZmlsbDojNDI4NUY0O30KPC9zdHlsZT4KPHBhdGggY2xhc3M9InN0MCIgZD0iTTAsMGgyNHYyNEgwVjB6Ii8+CjxwYXRoIGNsYXNzPSJzdDEiIGQ9Ik05LDE2LjJMNC44LDEybC0xLjQsMS40TDksMTlMMjEsN2wtMS40LTEuNEw5LDE2LjJ6Ii8+Cjwvc3ZnPgo=)}.overflow-content{flex:1;display:flex;align-items:center;justify-content:space-between}.loading{position:absolute;left:0px;top:0px;right:0px;bottom:0px;z-index:1;pointer-events:none;overflow:hidden}.spinner{animation:container-rotate 1568ms linear infinite;height:100%;width:100%}.spinner:after{position:absolute;content:" ";top:-25%;left:-25%;width:150%;height:150%}.spinner:before{position:absolute;content:" ";top:2px;left:2px;right:2px;bottom:2px;border-radius:100%}.kwai-layer{width:100%;height:100%;white-space:nowrap;animation-name:fill-unfill-rotate;animation-duration:5332ms;animation-timing-function:cubic-bezier(.4,0,.2,1);animation-iteration-count:infinite}.spinner-mask-1{left:-50%}.spinner-mask-2{right:-50%}.spinner-mask-1,.spinner-mask-2{position:absolute;width:100%;height:200%;top:-50%;overflow:hidden}.loading-spinner-frame,.loading-panel-spinner-frame{position:absolute;top:50%;left:50%;height:72px;width:72px;margin-left:-36px;margin-top:-48px;overflow:hidden}.loading-spinner-mask-2-bg{animation-name:mask-2-spin}.loading .loading-spinner-mask-2-bg{left:-100%;background-image:url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjIuMSwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPgo8c3ZnIHZlcnNpb249IjEuMSIgaWQ9IkxheWVyXzEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIHg9IjBweCIgeT0iMHB4IgoJIHZpZXdCb3g9IjAgMCAxOTYgMTk2IiBzdHlsZT0iZW5hYmxlLWJhY2tncm91bmQ6bmV3IDAgMCAxOTYgMTk2OyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+CjxwYXRoIGNsYXNzPSJzdDAiIGQ9Ik0xNDcsOThjMCwxOS42LTExLjUsMzYuNS0yOC4yLDQ0LjRsMCwwYy0wLjEsMC0wLjIsMC4xLTAuMywwLjEKCWMwLjEsMCwwLjItMC4xLDAuMy0wLjFsLTEuNy0zLjZDMTMyLjQsMTMxLjYsMTQzLDExNiwxNDMsOThjMC0yNC45LTIwLjEtNDUtNDUtNDV2LTRDMTI1LjEsNDksMTQ3LDcwLjksMTQ3LDk4eiIvPgo8L3N2Zz4K);background-position:right center}.loading-spinner-mask-1-bg{animation-name:mask-1-spin}.loading-spinner-mask-1-bg,.loading-spinner-mask-2-bg{position:absolute;background-repeat:no-repeat;width:200%;height:100%;top:0;animation-duration:1333ms;animation-timing-function:cubic-bezier(.4,0,.2,1);animation-iteration-count:infinite;animation-fill-mode:forwards}.loading .loading-spinner-mask-1-bg{left:0px;background-image:url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjIuMSwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPgo8c3ZnIHZlcnNpb249IjEuMSIgaWQ9IkxheWVyXzEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgeG1sbnM6eGxpbms9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkveGxpbmsiIHg9IjBweCIgeT0iMHB4IgoJIHZpZXdCb3g9IjAgMCAxOTYgMTk2IiBzdHlsZT0iZW5hYmxlLWJhY2tncm91bmQ6bmV3IDAgMCAxOTYgMTk2OyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+CjxwYXRoIGNsYXNzPSJzdDAiIGQ9Ik05OCw0OXY0Yy0yNC45LDAtNDUsMjAuMS00NSw0NQoJYzAsMTgsMTAuNiwzMy42LDI1LjksNDAuOGwtMS43LDMuNmMwLjEsMCwwLjIsMC4xLDAuMywwLjFjLTAuMSwwLTAuMi0wLjEtMC4zLTAuMWwwLDBDNjAuNSwxMzQuNSw0OSwxMTcuNiw0OSw5OAoJQzQ5LDcwLjksNzAuOSw0OSw5OCw0OXoiLz4KPC9zdmc+Cg==);background-position:left center}.switchPanel{position:absolute;left:0;top:0;width:251px;background:rgba(19,18,18,.8);border:1px solid rgb(50,49,49);color:#fff;z-index:20;border-radius:2px}.switchMenu{padding:5px 0}.line{cursor:pointer;display:table-row;height:40px}.line:hover{background-color:#5559;color:#eee}.icon{display:table-cell;vertical-align:middle;width:47px;height:20px;padding:0 10px}.iconImg{width:1.5em;height:1.5em;background-image:url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNjY2MjQ5Mjk5MTA5IiBjbGFzcz0iaWNvbiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjQxNTAiIHdpZHRoPSIyNTAiIGhlaWdodD0iMjUwIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PHBhdGggZD0iTTUxMiAxMDI0QTUxMiA1MTIgMCAxIDEgNTEyIDBhNTEyIDUxMiAwIDAgMSAwIDEwMjR6IG0zLjAwOC05Mi45OTJhNDE2IDQxNiAwIDEgMCAwLTgzMiA0MTYgNDE2IDAgMCAwIDAgODMyek00NDggNDQ4aDEyOHYzODRINDQ4VjQ0OHogbTAtMjU2aDEyOHYxMjhINDQ4VjE5MnoiIGZpbGw9IiNmZmZmZmYiIHAtaWQ9IjQxNTEiPjwvcGF0aD48L3N2Zz4=);background-size:contain}.label{display:table-cell;vertical-align:middle;padding:0 15px 0 0;width:191px;font-weight:400;font-size:13.3px}.switchPanel-checkbox{-webkit-appearance:none;appearance:none;width:1.125em;height:1.125em;margin:.9375em}.switchPanel-checkbox:checked{background-image:url(data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBzdGFuZGFsb25lPSJubyI/PjwhRE9DVFlQRSBzdmcgUFVCTElDICItLy9XM0MvL0RURCBTVkcgMS4xLy9FTiIgImh0dHA6Ly93d3cudzMub3JnL0dyYXBoaWNzL1NWRy8xLjEvRFREL3N2ZzExLmR0ZCI+PHN2ZyB0PSIxNjY2MTczNTA3OTc2IiBjbGFzcz0iaWNvbiIgdmlld0JveD0iMCAwIDEwMjQgMTAyNCIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHAtaWQ9IjQ0MjUiIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB3aWR0aD0iMjUwIiBoZWlnaHQ9IjI1MCI+PHBhdGggZD0iTTgxNy43MjggMTk4LjcybDExMS43NDQgMTE0LjU2LTU0NS4yMTYgNTMyLjEyOC0yODUuOTItMjczLjAyNCAxMTAuNTI4LTExNS43MTIgMTc0LjE3NiAxNjYuMzM2eiIgcC1pZD0iNDQyNiIgZmlsbD0iI2ZmZmZmZiI+PC9wYXRoPjwvc3ZnPg==);background-size:contain}@keyframes container-rotate{to{transform:rotate(360deg)}}@keyframes fill-unfill-rotate{12.5%{transform:rotate(135deg)}25%{transform:rotate(270deg)}37.5%{transform:rotate(405deg)}50%{transform:rotate(540deg)}62.5%{transform:rotate(675deg)}75%{transform:rotate(810deg)}87.5%{transform:rotate(945deg)}to{transform:rotate(1080deg)}}@keyframes mask-1-spin{0%{transform:rotate(157deg)}50%{transform:rotate(30deg)}to{transform:rotate(157deg)}}@keyframes mask-2-spin{0%{transform:rotate(-155deg)}50%{transform:rotate(-30deg)}to{transform:rotate(-155deg)}}</style><style>._container_1m0zi_1{position:absolute;left:0;top:0;padding:5px;width:90%;max-width:300px;max-height:100%;overflow:scroll;background:rgba(0,0,0,.8);word-break:break-all;white-space:normal}._container-h5_1m0zi_15{position:absolute;left:0;top:0;padding:5px;width:100%;height:100%;overflow:scroll;background:rgba(0,0,0,.8);word-break:break-all;white-space:normal}._closeEventLog-btn_1m0zi_28{position:absolute;right:10px;top:2px;cursor:pointer}._eventLogItem_1m0zi_35{display:flex}._eventLogItem-name_1m0zi_39{min-width:30%;text-align:right}._eventLogItem-colon_1m0zi_44{text-align:center;flex:5% 0 0}._eventLogItem-value_1m0zi_49{flex:1;cursor:pointer}</style><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.zswsdid.5bb695fcd05d84fd15ca.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.shared_da7c8ae9ba4d3befc7c2f1c0b3e151cc8ee375ec.46bf172c7aeac9f5ae11.js" crossorigin="anonymous"></script><link rel="stylesheet" type="text/css" href="https://static.zhihu.com/heifetz/column.user-hover-card.216a26f4.eabb3dfa660799d5f829.css" crossorigin="anonymous"><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.user-hover-card.513059ec149ce9c62576.js" crossorigin="anonymous"></script><link rel="stylesheet" type="text/css" href="https://static.zhihu.com/heifetz/column.Labels.216a26f4.7d19d2afdc588e36471f.css" crossorigin="anonymous"><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.Labels.99df6fe93d8447bc27f9.js" crossorigin="anonymous"></script><link rel="stylesheet" type="text/css" href="https://static.zhihu.com/heifetz/column.GoodsRecommendGoodsCardList.216a26f4.fa4bea774ed719d42a42.css" crossorigin="anonymous"><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.GoodsRecommendGoodsCardList.8af7137eaa8961dc01f5.js" crossorigin="anonymous"></script><link rel="stylesheet" type="text/css" href="https://static.zhihu.com/heifetz/column.modals.216a26f4.28903b70481acfdc9ee2.css" crossorigin="anonymous"><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.modals.722d9c30e43170814f90.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.report_modals.8f5020365d67291c787c.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.lib_cf230269.a532add9245cdb169cfd.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.comments-v3.5e88d6f29b731c6589cb.js" crossorigin="anonymous"></script><style data-emotion="css"></style><style>.MathJax_Preview ~ .math-holder {display: none}</style><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.85.0812fcf1ea2dfbfd754b.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.lib_9974496f.37f7e4d7be895c5a9034.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.shared_bf8e55f747b1310da866a56e0f5656c66d9272a4.c2860ebe27292a5c17b9.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.shared_03103dc5b8182a3433c5ef33bacb88093fd08a38.6ee14818e0f663c75523.js" crossorigin="anonymous"></script><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.shared_889419e066c38251e2dcbb9a7c42dbfc379cadeb.16d1996aee0be528166b.js" crossorigin="anonymous"></script><link rel="stylesheet" type="text/css" href="https://static.zhihu.com/heifetz/column.EditableV2.216a26f4.55b44f79780ebb4322e0.css" crossorigin="anonymous"><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.EditableV2.205fb6363c10d7013b54.js" crossorigin="anonymous"></script><link rel="stylesheet" type="text/css" href="https://static.zhihu.com/heifetz/column.signflow.216a26f4.9f1336000b7d439a30d2.css" crossorigin="anonymous"><script charset="utf-8" src="https://static.zhihu.com/heifetz/column.signflow.e1fac27a855eec92a4dc.js" crossorigin="anonymous"></script></head><body class="WhiteBg-body PostIndex-body" data-rh="class"><div id="root"><div class="App"><div class="LoadingBar  css-nvw5ip"></div><div><span style="position:absolute;top:-10000px;left:-10000px" role="log" aria-live="assertive"></span></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;yangquanhai&quot;}" data-zop="{&quot;authorName&quot;:&quot;鱼遇雨欲语与余&quot;,&quot;itemId&quot;:32877396,&quot;title&quot;:&quot;机器学习面试干货精讲&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;32877396&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader css-1l12z7y"><div class="ColumnPageHeader-content"><a href="//www.zhihu.com" aria-label="知乎"><svg viewBox="0 0 64 30" fill="#056DE8" width="64" height="30" class="css-1hlrcxk"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="//www.zhihu.com/column/DataAI">机器学习理论与数据竞赛实战</a></div></div><div class="ColumnPageHeader-Button"><div class="Popover"><button title="更多" id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content" type="button" class="Button ColumnPageHeader-MenuToggler Button--plain"><svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--Dots" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></button></div><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg width="24" height="24" viewBox="0 0 24 24" class="Zi Zi--EditSurround" fill="currentColor"><path fill-rule="evenodd" d="M3.55 5.97a2.415 2.415 0 0 1 2.415-2.416h7.56a.75.75 0 0 1 0 1.5h-7.56a.915.915 0 0 0-.915.915v12.072c0 .505.41.915.915.915h12.074c.506 0 .915-.41.915-.915v-7.557a.75.75 0 0 1 1.5 0v7.557a2.415 2.415 0 0 1-2.415 2.415H5.965A2.415 2.415 0 0 1 3.55 18.04V5.969Z" clip-rule="evenodd"></path><path fill-rule="evenodd" d="M20.239 3.77a.75.75 0 0 1 0 1.06l-8.206 8.206a.75.75 0 0 1-1.06-1.06l8.205-8.206a.75.75 0 0 1 1.06 0Z" clip-rule="evenodd"></path></svg>写文章</button></div></div><div class="ColumnPageHeader-profile"><div class="Popover AppHeader-menu"><button id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content" type="button" class="Button AppHeader-profileEntry Button--plain"><img class="Avatar AppHeader-profileAvatar css-1tzrnvf" src="https://picd.zhimg.com/f1fbab298af0760f5e2c63554e086a15_l.jpg?source=32738c0c" srcset="https://picd.zhimg.com/f1fbab298af0760f5e2c63554e086a15_l.jpg?source=32738c0c 2x" alt="点击打开杨全海的主页"></button></div><div class="Popover AppHeaderProfileMenu-creatorHintPopover"><div class="AppHeaderProfileMenu-creatorHintToggler" id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content"></div></div></div></div></div></div><div class="css-78p1r9"><div class="css-vn9cxj"><div class="css-1ld0bim"><img src="https://picd.zhimg.com/v2-379d54a5d9d4f5065efec52438a8a901_720w.jpg?source=172ae18b" alt="机器学习面试干货精讲" width="100%" height="100%" class="css-1ujtx97" srcset="https://picd.zhimg.com/v2-379d54a5d9d4f5065efec52438a8a901_200x0.jpg?source=172ae18b 200w,https://picd.zhimg.com/v2-379d54a5d9d4f5065efec52438a8a901_qhd.jpg?source=172ae18b 480w,https://picd.zhimg.com/v2-379d54a5d9d4f5065efec52438a8a901_720w.jpg?source=172ae18b 720w,https://picd.zhimg.com/v2-379d54a5d9d4f5065efec52438a8a901_1440w.jpg?source=172ae18b 1440w" loading="lazy"></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">机器学习面试干货精讲</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><div class="AuthorInfo"><meta itemprop="name" content="鱼遇雨欲语与余"><meta itemprop="image" content="https://picd.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b"><meta itemprop="url" content="https://www.zhihu.com/people/wang-he-13-93"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="css-1gomreu"><a href="//www.zhihu.com/people/wang-he-13-93" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User"><img class="Avatar AuthorInfo-avatar css-1y9jkzv" src="https://picd.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b" srcset="https://picd.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b 2x" alt="鱼遇雨欲语与余"></a></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="css-1gomreu"><a href="//www.zhihu.com/people/wang-he-13-93" target="_blank" class="UserLink-link" data-za-detail-view-element_name="User">鱼遇雨欲语与余</a></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"></div></div></div></div></div><button type="button" class="Button FollowButton Button--primary Button--blue"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Plus FollowButton-icon" fill="currentColor"><path fill-rule="evenodd" d="M13.25 3.25a1.25 1.25 0 1 0-2.5 0v7.5h-7.5a1.25 1.25 0 1 0 0 2.5h7.5v7.5a1.25 1.25 0 1 0 2.5 0v-7.5h7.5a1.25 1.25 0 0 0 0-2.5h-7.5v-7.5Z" clip-rule="evenodd"></path></svg></span>关注他</button></div><div class="LabelContainer-wrapper"></div><div role="button" tabindex="0"><span class="Voters"><button type="button" class="Button Button--plain">925 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="css-1yuhvjn"><div class="css-79elbk"><div class="RichText ztext Post-RichText css-4em6pe" options="[object Object]"><p data-first-child="" data-pid="6Q5oGChn">写于二零一八年二月十五日，农历大年三十</p><blockquote data-pid="TafwsDRU"><b>整体内容划分</b>：</blockquote><ul><li data-pid="i1ym-Y7B">推荐书单、公开课</li><li data-pid="a2XrswCs">面试核心内容提纲</li></ul><p data-pid="VfNJcPq1">本内容涉及模型核心数学公式，把本人面试中常被问到问题以及模型知识点的总结，起到提纲挈领作用，在准备的过程中抓住每个模型的重点。</p><blockquote data-pid="aCcfWcJo"><b>面试核心内容目录：</b></blockquote><ul><li data-pid="eXWdb57M">决策树</li><li data-pid="fQKmPpX9">随机森林</li><li data-pid="4oDG2rUJ">Adaboost</li><li data-pid="iAh3Ccmo">GBDT</li><li data-pid="xIcZgzFD">logistic 回归</li><li data-pid="CbrDKRwK">SVM支持向量机</li><li data-pid="-Rt-yPHO">朴素贝叶斯</li><li data-pid="_HCBG1IJ">xgboost</li><li data-pid="Pvn-FAGp">lightgbm</li></ul><hr><h2>        推荐书单、公开课</h2><p data-pid="6Dfee-tf"><b>网络公开课：</b></p><ul><li data-pid="4CBel4mV"><a href="https://link.zhihu.com/?target=http%3A//open.163.com/special/opencourse/daishu.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">麻省理工公开课 线性代数</a>——学习矩阵理论及线性代数的基本知识，推荐笔记<a href="https://zhuanlan.zhihu.com/p/28277072" class="internal">MIT线性代数课程精细笔记by忆瑧</a>。</li><li data-pid="7mX8dnlL"><a href="https://link.zhihu.com/?target=https%3A//www.csie.ntu.edu.tw/~htlin/mooc/" class=" wrap external" target="_blank" rel="nofollow noreferrer">台大机器学习公开课</a>——授课人林轩田，课程分为机器学习基石和机器学习技法两部分。</li><li data-pid="qLW2n-aE"><a href="https://link.zhihu.com/?target=https%3A//www.coursera.org/specializations/machine-learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">华盛顿大学机器学习公开课</a>——华盛顿大学在Coursera开的机器学习专项课，共有四个部分，这个课直接从应用案例开始讲起，对于回归，分类，协同过滤和情感分析等都会具体去讲怎么实现应用，并且会告诉你如何在Python中利用网上一些现有的库来实现特定的功能，也就是说基本上在课程的第一部分你就可以全面的知道机器学习能够在现实生活中的应用，以及简单方式去实现一些功能。</li><li data-pid="-fIRaoAS"><a href="https://link.zhihu.com/?target=http%3A//open.163.com/special/opencourse/machinelearning.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">斯坦福大学公开课 机器学习</a>——Andrew Ng（吴恩达）在斯坦福开设的CS229，难度远高于Coursera上面的课程。</li></ul><p data-pid="3zaXQB-j"><b>书单：</b></p><ul><li data-pid="AS6gV_Dk"><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/26708119/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《机器学习》</a>by 周志华，这是一本中国无数Machine Learning热爱者的启蒙教材，它非常合适没有任何背景的初学者看，每一个概念的来龙去脉讲的都很细致，是一本大而全的教材。</li><li data-pid="lNEx7Xuc"><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《统计学习方法》</a>by 李航，这本书主要偏优化和推倒，推倒相应算法的时候可以参考这本书。虽然只是薄薄的一本，但全是精华内容。</li><li data-pid="9DvyXjHx"><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/24703171/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《机器学习实战</a>》by Peter Harrington，可以对应《统计学习方法》进行实现代码。</li><li data-pid="sV5E-TJ4"><a href="https://link.zhihu.com/?target=http%3A//www.rmki.kfki.hu/~banmi/elte/Bishop%2520-%2520Pattern%2520Recognition%2520and%2520Machine%2520Learning.pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">《Pattern Recognition And Machine Learning》</a> by Christopher Bishop，属于机器学习进阶书籍，内容全，建议首先完成以上三本书籍，再看这本。</li><li data-pid="90yL1DMB"><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/25779298/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《利用Python进行数据分析》</a>——Python常用的库学习（numpy，pandas）</li><li data-pid="iqr1dXAT"><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/25910559/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《剑指offer》</a>——常见面试题，面试必备。</li></ul><p data-pid="Xj8hsNoF">最后推荐一个网站，收集了进阶的机器学习各种资源<a href="https://link.zhihu.com/?target=https%3A//github.com/JustFollowUs/Machine-Learning%23learning_route" class=" wrap external" target="_blank" rel="nofollow noreferrer">Github机器学习Machine-Learning</a></p><hr><h2>面试核心内容提纲</h2><h2><b>一、决策树</b></h2><h2><b>1.1 原理</b></h2><p data-pid="AjC6dee6">决策树是一种基本的分类与回归方法，其模型就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如<b>CART 只能是二叉树</b>），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。根节点包含整个样本集，每个叶节都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。</p><p data-pid="9VgLCru0">举个例子：</p><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-6611c71ca64d987c8bc698730219f489_b.jpg" data-caption="" data-size="normal" data-rawwidth="573" data-rawheight="404" class="origin_image zh-lightbox-thumb" width="573" data-original="https://pic2.zhimg.com/v2-6611c71ca64d987c8bc698730219f489_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='573' height='404'></svg>" data-caption="" data-size="normal" data-rawwidth="573" data-rawheight="404" class="origin_image zh-lightbox-thumb lazy" width="573" data-original="https://pic2.zhimg.com/v2-6611c71ca64d987c8bc698730219f489_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-6611c71ca64d987c8bc698730219f489_b.jpg"></div></figure><p data-pid="ubUBryU1">就像上面这个例子，训练集由三个特征：outlook(天气)，humidity（度），windy（是否有风）。那么我们该如何选择特征对训练集进行划分？连续型特征（比如湿度）划分的阈值又是如何确定的？<br>       决策树的生成就是不断的选择最优的特征对训练集进行划分，是一个递归的过程。递归返回的条件有三种：</p><ol><li data-pid="Mgp3MA2s">当前节点包含的样本属于同一类别，无需划分；</li><li data-pid="GqPfKaXM">当前属性集为空，或所有样本在属性集上取值相同，无法划分；</li><li data-pid="MlYro6X6">当前节点包含样本集合为空，无法划分。</li></ol><h2><b>1.2 ID3、C4.5、CART</b></h2><p data-pid="_iDr2cOe">这三个是非常著名的决策树算法。简单粗暴来说:</p><ul><li data-pid="338bffOJ">ID3 使用信息增益作为选择特征的准则；</li><li data-pid="4Fo26RZD">C4.5 使用信息增益比作为选择特征的准则；</li><li data-pid="qdgfyKGX">CART 使用 Gini 指数作为选择特征的准则。</li></ul><blockquote data-pid="Q749oO9M"><b>熵(entropy)</b></blockquote><p data-pid="lnHnfXmH">在信息论与概率论中，熵用于表示<b>随机变量不确定性的度量</b>。</p><p data-pid="tUZJl8xs">设X是一个有限状态的离散型随机变量，其概率分布为:</p><p data-pid="rdJHclFZ"><span class="ztext-math" data-eeimg="1" data-tex="P(X = x_i) = p_i,\ i=1,2,\cdots,n"><span></span><span><script type="math/tex;mode=inline">P(X = x_i) = p_i,\ i=1,2,\cdots,n</script><span class="tex2jax_ignore math-holder">P(X = x_i) = p_i,\ i=1,2,\cdots,n</span></span></span></p><p data-pid="7_7mikZb">则随机变量<span class="ztext-math" data-eeimg="1" data-tex="X"><span></span><span><script type="math/tex;mode=inline">X</script><span class="tex2jax_ignore math-holder">X</span></span></span>的熵定义为</p><p data-pid="Oz-evZ_g"><span class="ztext-math" data-eeimg="1" data-tex="H(X)= - \sum_{i=1}^{n} p_{i}log(p_i)"><span></span><span><script type="math/tex;mode=inline">H(X)= - \sum_{i=1}^{n} p_{i}log(p_i)</script><span class="tex2jax_ignore math-holder">H(X)= - \sum_{i=1}^{n} p_{i}log(p_i)</span></span></span></p><p data-pid="p6qBJZPL">熵越大，则随机变量的不确定性越大。</p><blockquote data-pid="7tkmd4Jo"><b>条件熵(conditional entropy)</b> </blockquote><p data-pid="-t9u42Cp">随机变量<span class="ztext-math" data-eeimg="1" data-tex="X"><span></span><span><script type="math/tex;mode=inline">X</script><span class="tex2jax_ignore math-holder">X</span></span></span>给定的条件下，随机变量<span class="ztext-math" data-eeimg="1" data-tex="Y"><span></span><span><script type="math/tex;mode=inline">Y</script><span class="tex2jax_ignore math-holder">Y</span></span></span>的条件熵<span class="ztext-math" data-eeimg="1" data-tex="H(Y|X)"><span></span><span><script type="math/tex;mode=inline">H(Y|X)</script><span class="tex2jax_ignore math-holder">H(Y|X)</span></span></span>定义为：</p><p data-pid="Zv7baPnU"><span class="ztext-math" data-eeimg="1" data-tex="H(Y|X) = \sum_{i=1}^{n}p_i H(Y|X=x_i)"><span></span><span><script type="math/tex;mode=inline">H(Y|X) = \sum_{i=1}^{n}p_i H(Y|X=x_i)</script><span class="tex2jax_ignore math-holder">H(Y|X) = \sum_{i=1}^{n}p_i H(Y|X=x_i)</span></span></span></p><p data-pid="J-pa3Exy">其中，<span class="ztext-math" data-eeimg="1" data-tex="p_i = P(X = x_i)"><span></span><span><script type="math/tex;mode=inline">p_i = P(X = x_i)</script><span class="tex2jax_ignore math-holder">p_i = P(X = x_i)</span></span></span>。</p><blockquote data-pid="yFDAZJlc"><b>信息增益(information gain)</b> </blockquote><p data-pid="wwnCl_aq">信息增益表示的是：<b><u>得知特征X的信息而使得类Y的信息的不确定性减少的程度</u></b>。</p><p data-pid="GI1Pk5KB">具体定义如下。</p><p data-pid="JegxCKQc">特征A对训练数据集D的信息增益<span class="ztext-math" data-eeimg="1" data-tex="g(D,A)"><span></span><span><script type="math/tex;mode=inline">g(D,A)</script><span class="tex2jax_ignore math-holder">g(D,A)</span></span></span>定义为集合D的经验熵<span class="ztext-math" data-eeimg="1" data-tex="H(D)"><span></span><span><script type="math/tex;mode=inline">H(D)</script><span class="tex2jax_ignore math-holder">H(D)</span></span></span>与特征A给定条件下D的经验条件熵<span class="ztext-math" data-eeimg="1" data-tex="H(D|A)"><span></span><span><script type="math/tex;mode=inline">H(D|A)</script><span class="tex2jax_ignore math-holder">H(D|A)</span></span></span>之差，即</p><p data-pid="VxJ3RNZV"><span class="ztext-math" data-eeimg="1" data-tex="g(D,A)=H(D)-H(D|A)"><span></span><span><script type="math/tex;mode=inline">g(D,A)=H(D)-H(D|A)</script><span class="tex2jax_ignore math-holder">g(D,A)=H(D)-H(D|A)</span></span></span></p><p data-pid="VoFUfJEM">一般地，熵<span class="ztext-math" data-eeimg="1" data-tex="H(Y)"><span></span><span><script type="math/tex;mode=inline">H(Y)</script><span class="tex2jax_ignore math-holder">H(Y)</span></span></span>与条件熵<span class="ztext-math" data-eeimg="1" data-tex="H(Y|X)"><span></span><span><script type="math/tex;mode=inline">H(Y|X)</script><span class="tex2jax_ignore math-holder">H(Y|X)</span></span></span>之差称为互信息(mutual information).</p><blockquote data-pid="vMP2G280"><b>ID3</b></blockquote><p data-pid="I0vZVjRE">熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。</p><p data-pid="tAgYRsOi"><b>信息增益 = 划分前熵 - 划分后熵</b>。信息增益越大，则意味着使用属性a来进行划分所获得的 “纯度提升” 越大 。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。</p><blockquote data-pid="TSs5gyDr">ID3优点是理论清晰、方法简单、学习能力较强，但也存在一些缺点：</blockquote><ul><li data-pid="NIFuBxgQ">只能处理分类属性的数据，不能处理连续的数据；</li><li data-pid="xvBX1KZD">划分过程会由于子集规模过小而造成统计特征不充分而停止；</li><li data-pid="8LeTX_nX">ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。</li></ul><blockquote data-pid="QF__nDtv"><b>C4.5</b></blockquote><p data-pid="ibZKtU8C">C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。<b>信息增益比 = 信息增益 / 划分前熵</b>  选择信息增益比最大的作为最优特征。公式：</p><p data-pid="BevVAm9n"><span class="ztext-math" data-eeimg="1" data-tex="g_R(D,A)=\frac{g(D,A)}{H_A(D)}"><span></span><span><script type="math/tex;mode=inline">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</script><span class="tex2jax_ignore math-holder">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</span></span></span> </p><p data-pid="K43-S490">其中 <span class="ztext-math" data-eeimg="1" data-tex="H_A(D)=-\sum_{i=1}^{n}{\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}}"><span></span><span><script type="math/tex;mode=inline">H_A(D)=-\sum_{i=1}^{n}{\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}}</script><span class="tex2jax_ignore math-holder">H_A(D)=-\sum_{i=1}^{n}{\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}}</span></span></span> ， <span class="ztext-math" data-eeimg="1" data-tex="n"><span></span><span><script type="math/tex;mode=inline">n</script><span class="tex2jax_ignore math-holder">n</span></span></span> 是特征 <span class="ztext-math" data-eeimg="1" data-tex="A"><span></span><span><script type="math/tex;mode=inline">A</script><span class="tex2jax_ignore math-holder">A</span></span></span> 取值的个数</p><p data-pid="2P_hSX9_">C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。</p><blockquote data-pid="g7Rfk7iW"><b>CART</b></blockquote><p data-pid="0eNJwrF1">CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。</p><p data-pid="jvSRE0HS">CART 的全称是分类与回归树（classification and regression tree）。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。</p><ul><li data-pid="BtJ7X2e7">回归树</li></ul><p data-pid="3ftgdjCO">使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。</p><p data-pid="xDR2eHNX">要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。</p><p data-pid="w15DlBCS">具体为，假设已将输入空间划分为 <span class="ztext-math" data-eeimg="1" data-tex="M"><span></span><span><script type="math/tex;mode=inline">M</script><span class="tex2jax_ignore math-holder">M</span></span></span> 个单元<span class="ztext-math" data-eeimg="1" data-tex="R_1,R_2,...,R_M"><span></span><span><script type="math/tex;mode=inline">R_1,R_2,...,R_M</script><span class="tex2jax_ignore math-holder">R_1,R_2,...,R_M</span></span></span>，即 <span class="ztext-math" data-eeimg="1" data-tex="M"><span></span><span><script type="math/tex;mode=inline">M</script><span class="tex2jax_ignore math-holder">M</span></span></span> 个特征，并且在每个单元<span class="ztext-math" data-eeimg="1" data-tex="R_m"><span></span><span><script type="math/tex;mode=inline">R_m</script><span class="tex2jax_ignore math-holder">R_m</span></span></span>上有一个固定的输出值<span class="ztext-math" data-eeimg="1" data-tex="c_m"><span></span><span><script type="math/tex;mode=inline">c_m</script><span class="tex2jax_ignore math-holder">c_m</span></span></span>，于是回归树可以表示为</p><p data-pid="pFaCE8YC"><span class="ztext-math" data-eeimg="1" data-tex="f(x)=\sum_{m=1}^{M}c_{m}I(x\in R_m)"><span></span><span><script type="math/tex;mode=inline">f(x)=\sum_{m=1}^{M}c_{m}I(x\in R_m)</script><span class="tex2jax_ignore math-holder">f(x)=\sum_{m=1}^{M}c_{m}I(x\in R_m)</span></span></span></p><p data-pid="IGRlgI1Q">当输入空间的划分确定时，可以用平方误差<span class="ztext-math" data-eeimg="1" data-tex="\sum_{x_i \in R_m}(y_i - f(x_i))^2"><span></span><span><script type="math/tex;mode=inline">\sum_{x_i \in R_m}(y_i - f(x_i))^2</script><span class="tex2jax_ignore math-holder">\sum_{x_i \in R_m}(y_i - f(x_i))^2</span></span></span>来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优值。</p><p data-pid="myiLqFpf"><b>选择最优切分特征 </b><span class="ztext-math" data-eeimg="1" data-tex="j"><span></span><span><script type="math/tex;mode=inline">j</script><span class="tex2jax_ignore math-holder">j</span></span></span><b> 和切分点 </b><span class="ztext-math" data-eeimg="1" data-tex="s"><span></span><span><script type="math/tex;mode=inline">s</script><span class="tex2jax_ignore math-holder">s</span></span></span><b> ,具体做法为:</b></p><p data-pid="iGIr3IXJ"><span class="ztext-math" data-eeimg="1" data-tex="\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}{(y_i-c_1)^2}+\min_{c_2}\sum_{x_i\in R_2(j,s)}{(y_i-c_2)^2}]"><span></span><span><script type="math/tex;mode=inline">\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}{(y_i-c_1)^2}+\min_{c_2}\sum_{x_i\in R_2(j,s)}{(y_i-c_2)^2}]</script><span class="tex2jax_ignore math-holder">\min_{j,s}[\min_{c_1}\sum_{x_i\in R_1(j,s)}{(y_i-c_1)^2}+\min_{c_2}\sum_{x_i\in R_2(j,s)}{(y_i-c_2)^2}]</span></span></span> </p><p data-pid="VQQLyyMz"><b>首先遍历特征 </b><span class="ztext-math" data-eeimg="1" data-tex="j"><span></span><span><script type="math/tex;mode=inline">j</script><span class="tex2jax_ignore math-holder">j</span></span></span><b> ,对固定的切分特征 </b><span class="ztext-math" data-eeimg="1" data-tex="j"><span></span><span><script type="math/tex;mode=inline">j</script><span class="tex2jax_ignore math-holder">j</span></span></span><b> 扫描切分点 </b><span class="ztext-math" data-eeimg="1" data-tex="s"><span></span><span><script type="math/tex;mode=inline">s</script><span class="tex2jax_ignore math-holder">s</span></span></span><b> ,选择使上式达到最小值的对 </b><span class="ztext-math" data-eeimg="1" data-tex="(j,s)"><span></span><span><script type="math/tex;mode=inline">(j,s)</script><span class="tex2jax_ignore math-holder">(j,s)</span></span></span> </p><ul><li data-pid="6V2y2rs1">分类树</li></ul><p data-pid="9S5ylX6w">使用 Gini 指数最小化准则来选择特征并进行划分；Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。</p><p data-pid="o7DuGkgi">假设有 <span class="ztext-math" data-eeimg="1" data-tex="K"><span></span><span><script type="math/tex;mode=inline">K</script><span class="tex2jax_ignore math-holder">K</span></span></span> 个类别，样本点属于第<span class="ztext-math" data-eeimg="1" data-tex="k"><span></span><span><script type="math/tex;mode=inline">k</script><span class="tex2jax_ignore math-holder">k</span></span></span>类的概率为<span class="ztext-math" data-eeimg="1" data-tex="p_k"><span></span><span><script type="math/tex;mode=inline">p_k</script><span class="tex2jax_ignore math-holder">p_k</span></span></span>,则概率分布的基尼指数定义为：</p><p data-pid="OZpxUkx4"><span class="ztext-math" data-eeimg="1" data-tex="Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_k)=1-\sum_{k=1}^{K}p_{k}^{2}"><span></span><span><script type="math/tex;mode=inline">Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_k)=1-\sum_{k=1}^{K}p_{k}^{2}</script><span class="tex2jax_ignore math-holder">Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_k)=1-\sum_{k=1}^{K}p_{k}^{2}</span></span></span></p><h2><b>1.3 信息增益 vs 信息增益比</b></h2><p data-pid="BL_fAzIP">之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。</p><h2><b>1.4 Gini 指数 vs 熵</b></h2><p data-pid="yizDAQx_">既然这两个都可以<b>表示数据的不确定性，不纯度</b>。那么这两个有什么区别那？</p><ul><li data-pid="0WXbYbFg">Gini 指数的计算不需要对数运算，更加高效；</li><li data-pid="A-YAG_yz">Gini 指数更偏向于连续属性，熵更偏向于离散属性。</li></ul><h2><b>1.5 剪枝</b></h2><p data-pid="fou8-sdo">决策树算法很容易过拟合（overfitting），剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法，剪枝分为预剪枝与后剪枝。</p><p data-pid="vspY71cV"><b>预剪枝</b>是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。</p><p data-pid="kAkWlsZJ"><b>后剪枝</b>是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。</p><p data-pid="0bBWGutl">那么怎么来判断是否带来泛化性能的提升呢？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。<a href="https://zhuanlan.zhihu.com/p/32627500" class="internal">交叉验证</a> </p><blockquote data-pid="g7Ufqmnh"><b>预剪枝和后剪枝的比较</b></blockquote><p class="ztext-empty-paragraph"><br></p><h2><b>1.6 总结</b></h2><p data-pid="pdafh-Ck">决策树算法主要包括三个部分：特征选择、树的生成、树的剪枝。常用算法有 ID3、C4.5、CART。</p><ul><li data-pid="U1pPxePK">特征选择。特征选择的目的是选取能够对训练集分类的特征。特征选择的关键是准则：信息增益、信息增益比、Gini 指数；</li><li data-pid="EPSQd9Gm">决策树的生成。通常是利用信息增益最大、信息增益比最大、Gini 指数最小作为特征选择的准则。从根节点开始，递归的生成决策树。相当于是不断选取局部最优特征，或将训练集分割为基本能够正确分类的子集；</li><li data-pid="LpTodN7C">决策树的剪枝。决策树的剪枝是为了防止树的过拟合，增强其泛化能力。包括预剪枝和后剪枝。</li></ul><h2><b>二、随机森林（Random Forest）</b></h2><blockquote data-pid="0P9NpTBD">要说随机森林就要先说 Bagging，要说 Bagging 就要先说<a href="https://zhuanlan.zhihu.com/p/32412775" class="internal">集成学习</a>。</blockquote><p data-pid="H1lmlo9V"><b>2.1 集成学习方法</b> </p><p data-pid="CtI2dify">集成学习（ensemble learning）通过构建并组合多个学习器来完成学习任务。集成学习将多个学习器进行结合，常获得比单一学习器显著优越的泛化性能。</p><p data-pid="Ramqtkgv">个体学习器通常由一个现有的学习算法从训练数据产生（比如 C4.5,BP 等），此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”的，同质集成中的个体学习器称“基学习器”。当然也存在不同质的学习器存在于统一集成中，常称为“组件学习器”或直接称为个体学习器。<a href="https://zhuanlan.zhihu.com/p/32877396/edit" class="internal">《机器学习》周志华</a></p><p data-pid="lLz718Fk"><b>原则</b>：要获得比单一学习器更好的性能，个体学习器应该好而不同。即个体学习器应该具有一定的准确性，不能差于弱学习器，并且具有多样性，即学习器之间有差异。</p><p data-pid="Vwyy3Iap">根据个体学习器的生成方式，目前集成学习分为两大类：</p><ul><li data-pid="lxHUASwp">个体学习器之间存在强依赖关系、必须串行生成的序列化方法。代表是 Boosting；</li><li data-pid="e_hOUHVE">个体学习器之间不存在强依赖关系、可同时生成的并行化方法。代表是 Bagging 和随机森林（Random Forest）。</li></ul><h2><b>2.2 Bagging</b></h2><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-0178314df8b6fa4a2754ae07890a82db_b.jpg" data-size="normal" data-rawwidth="420" data-rawheight="559" class="content_image" width="420"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='420' height='559'></svg>" data-size="normal" data-rawwidth="420" data-rawheight="559" class="content_image lazy" width="420" data-actualsrc="https://pic4.zhimg.com/v2-0178314df8b6fa4a2754ae07890a82db_b.jpg"></div><figcaption>bagging平均法</figcaption></figure><p data-pid="eOKsi2Vg">前面提到，想要集成算法获得性能的提升，个体学习器应该具有独立性。虽然 “独立” 在现实生活中往往无法做到，但是可以设法让基学习器尽可能的有较大的差异。</p><p data-pid="FG0g1_KG">Bagging 给出的做法就是对训练集进行采样，产生出若干个不同的子集，再从每个训练子集中训练一个基学习器。由于训练数据不同，我们的基学习器可望具有较大的差异。</p><p data-pid="zPtfebCA">Bagging 是并行式集成学习方法的代表，采样方法是自助采样法（bootstrap），用的是有放回的采样。初始训练集中大约有 63.2% 的数据出现在采样集中。</p><p data-pid="U36k_C34">Bagging 在预测输出进行结合时，对于分类问题，采用简单投票法；对于回归问题，采用简单平均法。</p><p data-pid="PvtyZs31">Bagging 优点：</p><ul><li data-pid="2rwyxHYh">高效。Bagging 集成与直接训练基学习器的复杂度同阶；</li><li data-pid="t9AnFyty">Bagging 能不经修改的适用于多分类、回归任务；</li><li data-pid="07rFRKTA">包外估计。使用剩下的样本作为验证集进行包外估计（out-of-bag estimate）。</li></ul><blockquote data-pid="_617iz3h">Bagging 主要关注降低方差。（low variance）</blockquote><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-b9355fcccc2be171253913eda7c6fd0c_b.jpg" data-caption="" data-size="normal" data-rawwidth="600" data-rawheight="567" class="origin_image zh-lightbox-thumb" width="600" data-original="https://pic1.zhimg.com/v2-b9355fcccc2be171253913eda7c6fd0c_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='600' height='567'></svg>" data-caption="" data-size="normal" data-rawwidth="600" data-rawheight="567" class="origin_image zh-lightbox-thumb lazy" width="600" data-original="https://pic1.zhimg.com/v2-b9355fcccc2be171253913eda7c6fd0c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-b9355fcccc2be171253913eda7c6fd0c_b.jpg"></div></figure><blockquote data-pid="K3JzYg5O"><b>偏差（bias）：</b>描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如上图第二行所示。<br><b>方差（variance）：</b>描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如上图右列所示。</blockquote><h2><b>2.3 随机森林（Random Forest）</b></h2><p data-pid="LPD9tYbJ"><b>2.3.1 原理</b></p><p data-pid="s-4dWSZE">随机森林（Random Forest）是 Bagging 的一个变体。Ramdon Forest 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入随机属性选择。</p><p data-pid="rqZ03VE1">原来决策树从所有特征中，选择最优特征。Ramdom Forest 的每一颗决策树中的每一个节点，先从该节点的特征集中通过bootstrap的方法随机选择 K 个特征的子集，然后从这个特征子集中通过决策树算法选择最优特征进行划分。（<b>注意</b>：除了随机获取特征以外，还能随机获取样本集，使用这些随机抽取的样本得到不同的决策树）</p><p data-pid="nqkn3M3R"><span class="ztext-math" data-eeimg="1" data-tex="K"><span></span><span><script type="math/tex;mode=inline">K</script><span class="tex2jax_ignore math-holder">K</span></span></span> 控制了随机性的引入程度，是一个重要的超参数。</p><p data-pid="AT9_8Fmb"><b>预测 </b>：</p><ul><li data-pid="ShKRmtCJ">分类：简单投票法；</li><li data-pid="65a4MtR0">回归：简单平均法。</li></ul><p data-pid="39JmLo7F"><b>2.3.2 具体构造过程</b></p><ol><li data-pid="P5SfT1di">从原始训练集中使用bootstrap方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集</li><li data-pid="NjH3vZlX">对于n_tree个训练集，我们分别训练n_tree个决策树模型</li><li data-pid="G8C3bP4m">对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数选择最好的特征进行分裂</li><li data-pid="tB0YMUey">每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝</li><li data-pid="AQsXke1G">将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果</li></ol><p data-pid="2xm592hE"><b>2.3.3 优缺点</b> </p><p data-pid="W9kcqMcg"><b>优点：</b></p><ul><li data-pid="-RcKWldE">由于每次不再考虑全部的属性，而是一个属性子集，所以相比于 Bagging 计算开销更小，训练效率更高；</li><li data-pid="aC3-tMEq">由于增加了属性的扰动，随机森林中基学习器的性能降低，使得在随机森林在起始时候性能较差，但是随着基学习器的增多，随机森林通常会收敛于更低的泛化误差，相比于 Bagging；</li><li data-pid="0P21IrtN">两个随机性的引入，使得随机森林不容易陷入过拟合，具有很好的抗噪声能力；</li><li data-pid="yorXO4W0">对数据的适应能力强，可以处理离散和连续的，无需要规范化；</li><li data-pid="Lswx77Xa">可以得到变量的重要性， 基于oob错误率（袋外错误率out-of-bag error）和基于 Gini 系数的变化。</li><li data-pid="hWmitJPr">不同决策树可以由不同主机并行训练生成，效率很高</li></ul><p data-pid="fdHoIc26"><b>缺点：</b></p><ul><li data-pid="mPj1d1h6">在噪声较大的时候容易过拟合。</li></ul><h2><b>三、AdaBoost</b></h2><blockquote data-pid="N1sFfulz">AdaBoost 是 Boosting 的代表，前面我们提到 Boosting 是集成学习中非常重要的一类串行化学习方法。</blockquote><h2><b>3.1 Boosting</b></h2><p data-pid="xTI8FOir">Boosting 是指个体学习器之间存在强依赖关系，必须串行序列化生成的集成学习方法。他的思想来源是三个臭皮匠顶个诸葛亮。Boosting 意为提升，意思是希望将每个弱学习器提升为强学习器。</p><p data-pid="V8KX4oDH"><b>工作机制如下：</b></p><ul><li data-pid="s2IXkf8H">先从初始训练集中学习一个基学习器；</li><li data-pid="EnIpAeXT">根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注；</li><li data-pid="3z3e_fW2">基于调整后的样本分布来训练下一个基学习器；</li><li data-pid="G6USMBEd">如此反复，直到基学习器数目达到 T，最终将这 T 个基学习器进行加权结合。</li></ul><p data-pid="xTa-DY8J">对训练样本分布调整，主要是通过增加误分类样本的权重，降低正确分类样本的权重。</p><blockquote data-pid="2RImJpb-">Boosting 关注的主要是降低偏差。（low bias）</blockquote><h2><b>3.2 AdaBoost 原理</b></h2><p data-pid="adrhY_tA"><b>面临两个问题：</b></p><ol><li data-pid="THaDEC_Y">在每一轮，如何改变训练数据的概率分布或者权值分布。（也可以重采样，但是 AdaBoost 没这么做）；</li><li data-pid="mQ6yKHzj">如何将弱分类器组合成强分类器。</li></ol><p data-pid="yviKjghn"><b>AdaBoost 的做法：</b></p><ol><li data-pid="YwX_2t9j">提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮弱分类器的关注；</li><li data-pid="rfczEgqy">采用加权多数表决。具体的，加大分类错误率低的分类器的权值，使其在表决中起较大作用，减少分类误差率大的弱分类器的权值，使其在表决中起较小作用。</li></ol><blockquote data-pid="djIIkkjl">弱分类器被线性组合成为一个强分类器。</blockquote><p data-pid="Nv7if9BV"><b>训练目标：</b></p><ul><li data-pid="h9E183UY">最小化指数损失函数。</li></ul><p data-pid="Qq83I1bL"><b>三部分组成：</b></p><ol><li data-pid="UZvMoVVw">分类器权重更新公式；</li><li data-pid="MKB0VGto">样本分布（也就是样本权重）更新公式；</li><li data-pid="Pu_QstaV">加性模型。 最小化指数损失函数。</li></ol><h2><b>3.3 AdaBoost 优缺点</b></h2><p data-pid="4hc-3nrl"><b>优点：</b></p><ul><li data-pid="xWZ5QS7b">Adaboost作为分类器时，分类精度很高</li><li data-pid="eqA1BHJS">在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。</li><li data-pid="szadCbp7">作为简单的二元分类器时，构造简单，结果可理解。</li><li data-pid="56haZ8sl">不容易发生过拟合</li></ul><p data-pid="_PQo8r1I"><b>缺点：</b></p><ul><li data-pid="QWpJui5m">对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li></ul><h2><b>四、GBDT</b></h2><blockquote data-pid="FL5VuGuj">GBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法。本小节从以下几个方面进行阐述：</blockquote><ol><li data-pid="JioRa3LU">Regression Decision Tree(DT)；</li><li data-pid="bc5tTI4z">Gradient Boosting(GB)；</li><li data-pid="pBqODnlA">Shrinkage(算法的一个重要演进分支，目前大部分源码都是基于该版本实现)；</li><li data-pid="zl4LQyjw">GBDT 适用范围；</li><li data-pid="Rsg2JYSm">与随机森林的对比。</li></ol><h2><b>4.1 DT：回归树 Regression Decision Tree</b></h2><p data-pid="ffSH9WJa">GBDT 中的树全部都是回归树，核心就是累加所有树的结果作为最终结果。只有回归树的结果累加起来才是有意义的，分类的结果累加是没有意义的。</p><p data-pid="49OrI3HR">GBDT 调整之后可以用于分类问题，但是内部还是回归树。</p><p data-pid="HxNrmCDE">这部分和决策树中的是一样的，无非就是特征选择。回归树用的是最小化均方误差，分类树是用的是最小化基尼指数（CART）</p><h2><b>4.2 GB：梯度迭代 Gradient Boosting</b></h2><p data-pid="UIBmiKYq">首先 Boosting 是一种集成方法。通过对弱分类器的组合得到强分类器，他是串行的，几个弱分类器之间是依次训练的。GBDT 的核心就在于，每一颗树学习的是之前所有树结论和的残差。</p><p data-pid="XfMnkS0t">Gradient 体现在：无论前面一颗树的 cost function 是什么，是均方差还是均差，只要它以误差作为衡量标准，那么残差向量都是它的全局最优方向，这就是 Gradient。</p><h2><b>4.3 Shrinkage</b></h2><blockquote data-pid="-DTKh_bF">Shrinkage（缩减）是 GBDT 算法的一个重要演进分支，目前大部分的源码都是基于这个版本的。</blockquote><p data-pid="3shiqvBc">核心思想在于：Shrinkage 认为每次走一小步来逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易防止过拟合。</p><p data-pid="rIU7B-dM">也就是说，它不信任每次学习到的残差，它认为每棵树只学习到了真理的一小部分，累加的时候只累加一小部分，通过多学习几棵树来弥补不足。</p><p data-pid="1MkuDo_U">具体的做法就是：仍然以残差作为学习目标，但是对于残差学习出来的结果，只累加一小部分（step* 残差）逐步逼近目标，step 一般都比较小 0.01-0.001, 导致各个树的残差是渐变而不是陡变的。</p><p data-pid="LjXTj5-Y">本质上，Shrinkage 为每一颗树设置了一个 weight，累加时要乘以这个 weight，但和 Gradient 没有关系。</p><p data-pid="2bmrin5M">这个 weight 就是 step。跟 AdaBoost 一样，Shrinkage 能减少过拟合也是经验证明的，目前还没有理论证明。</p><h2><b>4.4 GBDT 适用范围</b></h2><ul><li data-pid="UdIJv9Gr">GBDT 可以适用于回归问题（线性和非线性）；</li><li data-pid="ZMFPtrak">GBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。</li></ul><h2><b>4.5 GBDT 和随机森林区别（重点）</b></h2><ul><li data-pid="-GMc8lxJ">GBDT 和随机森林的相同点：</li><ul><li data-pid="IDLyeqE7">都是由多棵树组成；</li><li data-pid="iL7gxZRQ">最终的结果都由多棵树共同决定。</li></ul><li data-pid="hASC177I">GBDT 和随机森林的不同点：</li><ul><li data-pid="XPUIQVtl">组成随机森林的可以是分类树、回归树；组成 GBDT 只能是回归树；</li><li data-pid="Xujb2a6l">组成随机森林的树可以并行生成（Bagging）；GBDT 只能串行生成（Boosting）；这两种模型都用到了Bootstrap的思想。</li><li data-pid="-XrOefUf">对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而 GBDT 则是将所有结果累加起来，或者加权累加起来；</li><li data-pid="skx5_IZX">随机森林对异常值不敏感，GBDT 对异常值非常敏感；</li><li data-pid="rUQUUzI0">随机森林对训练集一视同仁权值一样，GBDT 是基于权值的弱分类器的集成；</li><li data-pid="IduaYiS_">随机森林通过减小模型的方差提高性能，GBDT 通过减少模型偏差提高性能。</li></ul></ul><h2><b>TIP</b></h2><p data-pid="hbJfbbIi"><b>1. GBDT 相比于决策树有什么优点</b></p><p data-pid="wipbgV-3">泛化性能更好！GBDT 的最大好处在于，每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于 0。这样后面就更加专注于那些分错的样本。</p><p data-pid="X3jVPllV"><b>2. Gradient 体现在哪里？</b></p><p data-pid="NyrwHLtO">可以理解为残差是全局最优的绝对方向，类似于求梯度。</p><p data-pid="u2USmTcB"><b>3. re-sample</b></p><p data-pid="B-zFUjFV">GBDT 也可以在使用残差的同时引入 Bootstrap re-sampling，GBDT 多数实现版本中引入了这个选项，但是是否一定使用有不同的看法。</p><p data-pid="FNYa8yBe">原因在于 re-sample 导致的随机性，使得模型不可复现，对于评估提出一定的挑战，比如很难确定性能的提升是由于 feature 的原因还是 sample 的随机因素。</p><h2><b>五、Logistic 回归（熟悉推导过程）</b></h2><ol><li data-pid="AyGmqzKg">LR 原理；</li><li data-pid="0mvsiOoi">参数估计；</li><li data-pid="W4PaEy9x">LR 的正则化；</li><li data-pid="yMnW22wc">为什么 LR 能比线性回归好？</li><li data-pid="t3GUxooA">LR 与 MaxEnt 的关系。</li></ol><h2><b>5.1 LR 模型原理</b></h2><p data-pid="Qyw86nyx">首先必须给出Logistic分布：</p><p data-pid="sr7j-Cux"><span class="ztext-math" data-eeimg="1" data-tex="\mu"><span></span><span><script type="math/tex;mode=inline">\mu</script><span class="tex2jax_ignore math-holder">\mu</span></span></span> 是位置参数，<span class="ztext-math" data-eeimg="1" data-tex="\gamma"><span></span><span><script type="math/tex;mode=inline">\gamma</script><span class="tex2jax_ignore math-holder">\gamma</span></span></span> 是形状参数。对称点是 <span class="ztext-math" data-eeimg="1" data-tex="(\mu，\frac{1}{2})"><span></span><span><script type="math/tex;mode=inline">(\mu，\frac{1}{2})</script><span class="tex2jax_ignore math-holder">(\mu，\frac{1}{2})</span></span></span> ， <span class="ztext-math" data-eeimg="1" data-tex="\gamma"><span></span><span><script type="math/tex;mode=inline">\gamma</script><span class="tex2jax_ignore math-holder">\gamma</span></span></span> 越小，函数在 <span class="ztext-math" data-eeimg="1" data-tex="\mu"><span></span><span><script type="math/tex;mode=inline">\mu</script><span class="tex2jax_ignore math-holder">\mu</span></span></span> 附近越陡峭。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-0f5b7494c719d064bc2dd908734c5687_b.jpg" data-caption="" data-size="normal" data-rawwidth="618" data-rawheight="128" class="origin_image zh-lightbox-thumb" width="618" data-original="https://pic4.zhimg.com/v2-0f5b7494c719d064bc2dd908734c5687_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='618' height='128'></svg>" data-caption="" data-size="normal" data-rawwidth="618" data-rawheight="128" class="origin_image zh-lightbox-thumb lazy" width="618" data-original="https://pic4.zhimg.com/v2-0f5b7494c719d064bc2dd908734c5687_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-0f5b7494c719d064bc2dd908734c5687_b.jpg"></div></figure><figure data-size="normal"><noscript><img src="https://pic2.zhimg.com/v2-8c5035b34e00929a3940feb6cbbbd5b5_b.jpg" data-caption="" data-size="normal" data-rawwidth="1414" data-rawheight="536" class="origin_image zh-lightbox-thumb" width="1414" data-original="https://pic2.zhimg.com/v2-8c5035b34e00929a3940feb6cbbbd5b5_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1414' height='536'></svg>" data-caption="" data-size="normal" data-rawwidth="1414" data-rawheight="536" class="origin_image zh-lightbox-thumb lazy" width="1414" data-original="https://pic2.zhimg.com/v2-8c5035b34e00929a3940feb6cbbbd5b5_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-8c5035b34e00929a3940feb6cbbbd5b5_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="Y8thVq84">然后，二分类 LR 模型，是参数化的 logistic 分布，使用条件概率来表示：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-587fdf891b6be074712676c8e8f2a922_b.jpg" data-caption="" data-size="normal" data-rawwidth="532" data-rawheight="236" class="origin_image zh-lightbox-thumb" width="532" data-original="https://pic3.zhimg.com/v2-587fdf891b6be074712676c8e8f2a922_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='532' height='236'></svg>" data-caption="" data-size="normal" data-rawwidth="532" data-rawheight="236" class="origin_image zh-lightbox-thumb lazy" width="532" data-original="https://pic3.zhimg.com/v2-587fdf891b6be074712676c8e8f2a922_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-587fdf891b6be074712676c8e8f2a922_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="t8_ddZwW">然后，一个事件的几率（odds）：指该事件发生与不发生的概率比值：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-7aeed809a148ccdb68b3598a431c3310_b.jpg" data-caption="" data-size="normal" data-rawwidth="427" data-rawheight="100" class="origin_image zh-lightbox-thumb" width="427" data-original="https://pic1.zhimg.com/v2-7aeed809a148ccdb68b3598a431c3310_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='427' height='100'></svg>" data-caption="" data-size="normal" data-rawwidth="427" data-rawheight="100" class="origin_image zh-lightbox-thumb lazy" width="427" data-original="https://pic1.zhimg.com/v2-7aeed809a148ccdb68b3598a431c3310_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-7aeed809a148ccdb68b3598a431c3310_b.jpg"></div></figure><p data-pid="RiuWljF6">对数几率：</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-65e6847355e4a5d5f23f9d5da458009f_b.jpg" data-caption="" data-size="normal" data-rawwidth="346" data-rawheight="99" class="content_image" width="346"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='346' height='99'></svg>" data-caption="" data-size="normal" data-rawwidth="346" data-rawheight="99" class="content_image lazy" width="346" data-actualsrc="https://pic4.zhimg.com/v2-65e6847355e4a5d5f23f9d5da458009f_b.jpg"></div></figure><p data-pid="hNU3hw0-">那么对于逻辑回归而言， <span class="ztext-math" data-eeimg="1" data-tex="Y=1"><span></span><span><script type="math/tex;mode=inline">Y=1</script><span class="tex2jax_ignore math-holder">Y=1</span></span></span> 的对数几率就是：</p><p data-pid="zUMccMbP">最终，输出 <span class="ztext-math" data-eeimg="1" data-tex="Y=1"><span></span><span><script type="math/tex;mode=inline">Y=1</script><span class="tex2jax_ignore math-holder">Y=1</span></span></span> 的对数几率是输入 <span class="ztext-math" data-eeimg="1" data-tex="x"><span></span><span><script type="math/tex;mode=inline">x</script><span class="tex2jax_ignore math-holder">x</span></span></span> 的线性函数表示的模型，这就是逻辑回归模型。</p><h2><b>5.2 参数估计</b></h2><p data-pid="dnzg4auF">在统计学中，常常使用极大似然估计法来估计参数。即找到一组参数，使得在这组参数下，我们数据的似然度（概率）最大。<b>(</b><a href="https://zhuanlan.zhihu.com/p/26614750" class="internal">极大似然估计</a><b>：就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，即‘模型已定，参数未知’</b>)</p><p data-pid="B1jAwX7a">似然函数：</p><p data-pid="EDBaBGKr"><span class="ztext-math" data-eeimg="1" data-tex="\begin{eqnarray*} L(w)&amp;=&amp; \prod_{}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\end{eqnarray*}"><span></span><span><script type="math/tex;mode=inline">\begin{eqnarray*} L(w)&=& \prod_{}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\end{eqnarray*}</script><span class="tex2jax_ignore math-holder">\begin{eqnarray*} L(w)&amp;=&amp; \prod_{}[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}\end{eqnarray*}</span></span></span> </p><p data-pid="8GRdsTtZ">对数似然函数：</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-df5d56c026b6c33f9e7f64f23883ceb0_b.jpg" data-caption="" data-size="normal" data-rawwidth="740" data-rawheight="294" class="origin_image zh-lightbox-thumb" width="740" data-original="https://pic1.zhimg.com/v2-df5d56c026b6c33f9e7f64f23883ceb0_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='740' height='294'></svg>" data-caption="" data-size="normal" data-rawwidth="740" data-rawheight="294" class="origin_image zh-lightbox-thumb lazy" width="740" data-original="https://pic1.zhimg.com/v2-df5d56c026b6c33f9e7f64f23883ceb0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-df5d56c026b6c33f9e7f64f23883ceb0_b.jpg"></div></figure><blockquote data-pid="1uhq1cbO"><b>如何得到逻辑回归的损失函数?</b></blockquote><p data-pid="PGXfdCPl">我们希望上式越大越好，换句话说，对于给定样本数量 <span class="ztext-math" data-eeimg="1" data-tex="n"><span></span><span><script type="math/tex;mode=inline">n</script><span class="tex2jax_ignore math-holder">n</span></span></span> ，我们希望 <span class="ztext-math" data-eeimg="1" data-tex="-\frac{1}{n}lnL(w)"><span></span><span><script type="math/tex;mode=inline">-\frac{1}{n}lnL(w)</script><span class="tex2jax_ignore math-holder">-\frac{1}{n}lnL(w)</span></span></span> 越小越好，这个也就是LogLoss。</p><p data-pid="s6V4QnvH">对应的损失函数：</p><p data-pid="hhgWOTj4"><span class="ztext-math" data-eeimg="1" data-tex="J(W) = -\frac{1}{n}lnL(w)"><span></span><span><script type="math/tex;mode=inline">J(W) = -\frac{1}{n}lnL(w)</script><span class="tex2jax_ignore math-holder">J(W) = -\frac{1}{n}lnL(w)</span></span></span> </p><blockquote data-pid="lbcPEk84"><b>为什么LR的输入特征一般是离散的而不是连续的？</b></blockquote><p data-pid="XjVkm11I">     在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p><ol><li data-pid="0LHJ1zD2">离散特征的增加和减少都很容易，易于模型的快速迭代；</li><li data-pid="iwvxBIIH">稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li><li data-pid="qz_X7aEh">离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li><li data-pid="eSHpyMq0">逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；</li><li data-pid="nAK6HuHL">离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li><li data-pid="NHXDWQ7w">特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li><li data-pid="4LgbcQhq">特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li></ol><blockquote data-pid="ooZUo-1U">李沐少帅指出，模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</blockquote><p data-pid="Zhlw3ZPn"><b>5.3 最优化方法</b></p><blockquote data-pid="Il1iFnxx">逻辑回归模型的参数估计中，最后就是对 <span class="ztext-math" data-eeimg="1" data-tex="J(W)"><span></span><span><script type="math/tex;mode=inline">J(W)</script><span class="tex2jax_ignore math-holder">J(W)</span></span></span> 求最小值。这种不带约束条件的最优化问题，常用梯度下降，牛顿法和拟牛顿法，共轭梯度法来解决。</blockquote><p data-pid="fCtGHJRK"><b>使用梯度下降法求解逻辑回归参数估计</b></p><p data-pid="CfUe84Dh">求 <span class="ztext-math" data-eeimg="1" data-tex="J(W)"><span></span><span><script type="math/tex;mode=inline">J(W)</script><span class="tex2jax_ignore math-holder">J(W)</span></span></span> 梯度： <span class="ztext-math" data-eeimg="1" data-tex="g(w)"><span></span><span><script type="math/tex;mode=inline">g(w)</script><span class="tex2jax_ignore math-holder">g(w)</span></span></span> :</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-b21ae715c95063854a74311be9a8ef3f_b.jpg" data-caption="" data-size="normal" data-rawwidth="616" data-rawheight="456" class="origin_image zh-lightbox-thumb" width="616" data-original="https://pic4.zhimg.com/v2-b21ae715c95063854a74311be9a8ef3f_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='616' height='456'></svg>" data-caption="" data-size="normal" data-rawwidth="616" data-rawheight="456" class="origin_image zh-lightbox-thumb lazy" width="616" data-original="https://pic4.zhimg.com/v2-b21ae715c95063854a74311be9a8ef3f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-b21ae715c95063854a74311be9a8ef3f_b.jpg"></div></figure><p data-pid="0UZfS0BW">更新 <span class="ztext-math" data-eeimg="1" data-tex="W_k"><span></span><span><script type="math/tex;mode=inline">W_k</script><span class="tex2jax_ignore math-holder">W_k</span></span></span> ：</p><p data-pid="PYtK71R1"><span class="ztext-math" data-eeimg="1" data-tex="W_{k+1} = W_k - \lambda g(W_k)"><span></span><span><script type="math/tex;mode=inline">W_{k+1} = W_k - \lambda g(W_k)</script><span class="tex2jax_ignore math-holder">W_{k+1} = W_k - \lambda g(W_k)</span></span></span> </p><h2><b>5.4 正则化</b></h2><blockquote data-pid="X4_uTjkQ">正则化为了解决过拟合问题。分为 L1 和 L2 正则化。主要通过修正损失函数，加入模型复杂性评估；<br>正则化是符合<b>奥卡姆剃刀原理</b>：在所有可能的模型中，能够很好的解释已知数据并且十分简单的才是最好的模型。</blockquote><p data-pid="8qK4qdi3"><span class="ztext-math" data-eeimg="1" data-tex="J(w)\Rightarrow J(w) + \lambda{||w||}_p"><span></span><span><script type="math/tex;mode=inline">J(w)\Rightarrow J(w) + \lambda{||w||}_p</script><span class="tex2jax_ignore math-holder">J(w)\Rightarrow J(w) + \lambda{||w||}_p</span></span></span> </p><p data-pid="mzxJZEk5"><span class="ztext-math" data-eeimg="1" data-tex="p"><span></span><span><script type="math/tex;mode=inline">p</script><span class="tex2jax_ignore math-holder">p</span></span></span> 表示范数， <span class="ztext-math" data-eeimg="1" data-tex="p=1"><span></span><span><script type="math/tex;mode=inline">p=1</script><span class="tex2jax_ignore math-holder">p=1</span></span></span> 和 <span class="ztext-math" data-eeimg="1" data-tex="p=2"><span></span><span><script type="math/tex;mode=inline">p=2</script><span class="tex2jax_ignore math-holder">p=2</span></span></span> 分别应用 L1 和 L2 正则。</p><ul><li data-pid="XdZOgqzb">L1 正则化。向量中各元素绝对值之和。又叫做稀疏规则算子（Lasso regularization）。关键在于能够实现特征的自动选择，参数稀疏可以避免非必要的特征引入的噪声；</li><li data-pid="btW1JTRZ">L2 正则化。使得每个元素都尽可能的小，但是都不为零。在回归里面，有人把他的回归叫做岭回归（Ridge Regression），也有人叫他 “权值衰减”（weight decay）。</li></ul><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-22ee181fbe6046ccec1dc443228ae176_b.jpg" data-caption="" data-size="normal" data-rawwidth="1160" data-rawheight="564" class="origin_image zh-lightbox-thumb" width="1160" data-original="https://pic3.zhimg.com/v2-22ee181fbe6046ccec1dc443228ae176_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1160' height='564'></svg>" data-caption="" data-size="normal" data-rawwidth="1160" data-rawheight="564" class="origin_image zh-lightbox-thumb lazy" width="1160" data-original="https://pic3.zhimg.com/v2-22ee181fbe6046ccec1dc443228ae176_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-22ee181fbe6046ccec1dc443228ae176_b.jpg"></div></figure><p data-pid="u-4MN9dk">一句话总结就是：L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，这些特征都会接近于 0.</p><h2><b>5.5 逻辑回归 vs 线性回归</b></h2><blockquote data-pid="48ihljJ3">首先，逻辑回归比线性回归要好。</blockquote><p data-pid="on1DHhoD">两者都属于广义线性模型。</p><p data-pid="hgktOeVV">线性回归优化目标函数用的最小二乘法，而逻辑回归用的是最大似然估计。逻辑回归只是在线性回归的基础上，将加权和通过 <span class="ztext-math" data-eeimg="1" data-tex="sigmoid"><span></span><span><script type="math/tex;mode=inline">sigmoid</script><span class="tex2jax_ignore math-holder">sigmoid</span></span></span> 函数，映射到 <span class="ztext-math" data-eeimg="1" data-tex="0-1"><span></span><span><script type="math/tex;mode=inline">0-1</script><span class="tex2jax_ignore math-holder">0-1</span></span></span> 范围内空间。</p><p data-pid="3SvKL8ga">线性回归在整个实数范围内进行预测，敏感度一致，而分类范围，需要在 <span class="ztext-math" data-eeimg="1" data-tex="[0,1]"><span></span><span><script type="math/tex;mode=inline">[0,1]</script><span class="tex2jax_ignore math-holder">[0,1]</span></span></span> 。而逻辑回归就是一种减小预测范围，将预测值限定为 [0,1] 间的一种回归模型。</p><p data-pid="Qew5UHtR">逻辑曲线在 <span class="ztext-math" data-eeimg="1" data-tex="z=0"><span></span><span><script type="math/tex;mode=inline">z=0</script><span class="tex2jax_ignore math-holder">z=0</span></span></span> 时，十分敏感，在 <span class="ztext-math" data-eeimg="1" data-tex="z>>0"><span></span><span><script type="math/tex;mode=inline">z>>0</script><span class="tex2jax_ignore math-holder">z&gt;&gt;0</span></span></span> 或 <span class="ztext-math" data-eeimg="1" data-tex="z<<0"><span></span><span><script type="math/tex;mode=inline">z<<0</script><span class="tex2jax_ignore math-holder">z&lt;&lt;0</span></span></span> 处，都不敏感，将预测值限定为 <span class="ztext-math" data-eeimg="1" data-tex="(0,1)"><span></span><span><script type="math/tex;mode=inline">(0,1)</script><span class="tex2jax_ignore math-holder">(0,1)</span></span></span> 。逻辑回归的鲁棒性比线性回归要好。</p><h2><b>5.6 逻辑回归模型 vs 最大熵模型 MaxEnt</b></h2><blockquote data-pid="qBlAKkQY">简单粗暴的说：逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应为二类时的特殊情况，也就是说，当逻辑回归扩展为多类别的时候，就是最大熵模型。</blockquote><p data-pid="5IC__BoG">最大熵原理：学习概率模型的时候，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。</p><h2><b>5.7 最大熵模型</b></h2><p data-pid="O2h_poCF"><a href="https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6093948.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">最大熵模型推荐学习博客</a></p><h2><b>六、SVM 支持向量机</b></h2><blockquote data-pid="1oTY5J1-">虽然咱们的目标是尽可能的不涉及到公式，但是提到 SVM 就没有办法不涉及到公式推导，因为面试中只要问到 SVM，最基本也是最难的问题就是：SVM 的对偶问题数学公式推导。<br>所以想学好机器学习，还是要踏下心来，不仅仅要把原理的核心思想掌握了，公式推导也要好好学习才行。</blockquote><h2><b>6.1 SVM 原理</b></h2><blockquote data-pid="q9l3ayJJ">简单粗暴的说：SVM 的思路就是找到一个超平面将数据集进行正确的分类。对于在现有维度不可分的数据，利用核函数映射到高纬空间使其线性可分。</blockquote><p data-pid="xfLeWIyy">支持向量机 SVM 是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。SVM 的学习策略是间隔最大化，可形式化为求解凸二次规划问题。</p><p data-pid="PSK7X9a1">SVM 分为：</p><ul><li data-pid="qi6__Utm">线性可分支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习到的一个线性分类器；</li><li data-pid="rDrVbqCP">线性支持向量机。当训练数据近似线性可分时，通过软间隔最大化，学习到的一个线性分类器；</li><li data-pid="z98PWFBx">非线性支持向量机。当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</li></ul><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-968c6fdab38d2c75be3cd81355993a3f_b.jpg" data-caption="" data-size="normal" data-rawwidth="470" data-rawheight="407" class="origin_image zh-lightbox-thumb" width="470" data-original="https://pic4.zhimg.com/v2-968c6fdab38d2c75be3cd81355993a3f_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='470' height='407'></svg>" data-caption="" data-size="normal" data-rawwidth="470" data-rawheight="407" class="origin_image zh-lightbox-thumb lazy" width="470" data-original="https://pic4.zhimg.com/v2-968c6fdab38d2c75be3cd81355993a3f_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-968c6fdab38d2c75be3cd81355993a3f_b.jpg"></div></figure><p data-pid="V5ggx1G9">上图中，X 表示负例，O 表示正例。此时的训练数据可分，线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线。</p><p data-pid="P0etBsMO"><b>6.1.1 支持向量与间隔</b></p><p data-pid="NWvnBtDR">支持向量：在线性可分的情况下，训练数据样本集中的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。</p><p data-pid="m4Fj2P1R">函数间隔定义如下：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-0e8219bc42d97aef5bb70a8e93427928_b.jpg" data-caption="" data-size="normal" data-rawwidth="1207" data-rawheight="378" class="origin_image zh-lightbox-thumb" width="1207" data-original="https://pic1.zhimg.com/v2-0e8219bc42d97aef5bb70a8e93427928_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1207' height='378'></svg>" data-caption="" data-size="normal" data-rawwidth="1207" data-rawheight="378" class="origin_image zh-lightbox-thumb lazy" width="1207" data-original="https://pic1.zhimg.com/v2-0e8219bc42d97aef5bb70a8e93427928_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-0e8219bc42d97aef5bb70a8e93427928_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><p data-pid="kVVrv5CP"><span class="ztext-math" data-eeimg="1" data-tex="y_i"><span></span><span><script type="math/tex;mode=inline">y_i</script><span class="tex2jax_ignore math-holder">y_i</span></span></span> 表示目标值，取值为 <span class="ztext-math" data-eeimg="1" data-tex="+1"><span></span><span><script type="math/tex;mode=inline">+1</script><span class="tex2jax_ignore math-holder">+1</span></span></span> 、 <span class="ztext-math" data-eeimg="1" data-tex="-1"><span></span><span><script type="math/tex;mode=inline">-1</script><span class="tex2jax_ignore math-holder">-1</span></span></span> . 函数间隔虽然可以表示分类预测的准确性以及确信度。但是有个不好的性质：只要成倍的改变 <span class="ztext-math" data-eeimg="1" data-tex="w"><span></span><span><script type="math/tex;mode=inline">w</script><span class="tex2jax_ignore math-holder">w</span></span></span> 和 <span class="ztext-math" data-eeimg="1" data-tex="b"><span></span><span><script type="math/tex;mode=inline">b</script><span class="tex2jax_ignore math-holder">b</span></span></span> ，虽然此时的超平面并没有改变，但是函数间隔会变大。</p><p data-pid="sZutUtOq">所以我们想到了对超平面的法向量 <span class="ztext-math" data-eeimg="1" data-tex="w"><span></span><span><script type="math/tex;mode=inline">w</script><span class="tex2jax_ignore math-holder">w</span></span></span> 施加一些约束，比如规范化，使得间隔确定，这就引出了几何间隔：</p><p data-pid="tweXFAkn">支持向量学习的基本思想就是求解能够正确划分训练数据集并且几何间隔最大的分类超平面。</p><p data-pid="yfnzMhty"><b>6.1.2 对偶问题</b></p><p data-pid="gJvVAMS2">为了求解线性可分支持向量机的最优化问题：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-4d75bb1dffffd99371c903fadf8c7a16_b.jpg" data-caption="" data-size="normal" data-rawwidth="589" data-rawheight="157" class="origin_image zh-lightbox-thumb" width="589" data-original="https://pic3.zhimg.com/v2-4d75bb1dffffd99371c903fadf8c7a16_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='589' height='157'></svg>" data-caption="" data-size="normal" data-rawwidth="589" data-rawheight="157" class="origin_image zh-lightbox-thumb lazy" width="589" data-original="https://pic3.zhimg.com/v2-4d75bb1dffffd99371c903fadf8c7a16_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-4d75bb1dffffd99371c903fadf8c7a16_b.jpg"></div></figure><p data-pid="pdNjkQdR">将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。</p><p data-pid="hqBYu6lm">本来的算法也可以求解 SVM，但是之所以要用对偶问题来求解，优点是：</p><ul><li data-pid="Pb0VSPzU">一是对偶问题往往更容易求解；</li><li data-pid="vz4RB_L2">二是自然引入核函数，进而推广到非线性分类问题。</li></ul><p data-pid="W0DBPKiq"><b>6.1.3 核函数</b></p><p data-pid="L8emVsTx">对于在原始空间中不可分的问题，在高维空间中是线性可分的。</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-84cf4a913ef8a47d9c0cc5574ca134be_b.jpg" data-caption="" data-size="normal" data-rawwidth="1225" data-rawheight="487" class="origin_image zh-lightbox-thumb" width="1225" data-original="https://pic3.zhimg.com/v2-84cf4a913ef8a47d9c0cc5574ca134be_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1225' height='487'></svg>" data-caption="" data-size="normal" data-rawwidth="1225" data-rawheight="487" class="origin_image zh-lightbox-thumb lazy" width="1225" data-original="https://pic3.zhimg.com/v2-84cf4a913ef8a47d9c0cc5574ca134be_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-84cf4a913ef8a47d9c0cc5574ca134be_b.jpg"></div></figure><p data-pid="XLgQGRix">对于线性不可分的问题，使用核函数可以从原始空间映射到高纬空间，使得问题变得线性可分。核函数还可以使得在高维空间计算的内积在低维空间中通过一个函数来完成。</p><p data-pid="Eb524iwX">常用的核函数有：高斯核、线性核、多项式核。</p><p data-pid="E6gkSCOr"><b>6.1.4 软间隔</b></p><p data-pid="dG6WO75n">线性可分问题的支持向量机学习方法，对现行不可分训练数据是不适用的。所以讲间隔函数修改为软间隔，对于函数间隔，在其上加上一个松弛变量，使其满足大于等于 1。约束条件变为：</p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-6844f79f791fa9a5f890ec3f60b23ec2_b.jpg" data-caption="" data-size="normal" data-rawwidth="314" data-rawheight="73" class="content_image" width="314"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='314' height='73'></svg>" data-caption="" data-size="normal" data-rawwidth="314" data-rawheight="73" class="content_image lazy" width="314" data-actualsrc="https://pic3.zhimg.com/v2-6844f79f791fa9a5f890ec3f60b23ec2_b.jpg"></div></figure><h2><b>6.2 优缺点</b></h2><p data-pid="vvsqI4ny">缺点：</p><ul><li data-pid="2ETeQ_99">时空开销比较大，训练时间长；</li><li data-pid="Q7gRsSuK">核函数的选取比较难，主要靠经验。</li></ul><p data-pid="Wqfvngf7">优点：</p><ul><li data-pid="_ch23kB5">在小训练集上往往得到比较好的结果；</li><li data-pid="yPx3HMiy">使用核函数避开了高纬空间的复杂性；</li><li data-pid="Iy0VHmdy">泛化能力强。</li></ul><h2><b>6.3 LR和SVM的区别</b></h2><ul><li data-pid="Y6oQekGV">Linear SVM和LR都是线性分类器</li><li data-pid="o45MYUfb">Linear SVM和LR都是<a href="https://zhuanlan.zhihu.com/p/32655097" class="internal">判别模型</a> </li><li data-pid="VxtZbAOe">Linear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别处于极其不平衡的状态, 一般需要先对数据做平衡处理。</li><li data-pid="C0g1JLOY">Linear SVM依赖数据表达的距离测度，所以需要对数据先做标准化；LR不受其影响</li><li data-pid="Oq9fthnZ">Linear SVM依赖惩罚项的系数，实验中需要做<a href="https://zhuanlan.zhihu.com/p/32627500" class="internal">交叉验证</a></li><li data-pid="X9dDv6yg">Linear SVM和LR的执行都会受到异常值的影响，其敏感程度而言，谁更好很难下明确结论。</li><li data-pid="R7LcxjFq">Linear SVM和LR损失函数不同, LR为logloss, SVM为hinge loss. 而SVM中的<span class="ztext-math" data-eeimg="1" data-tex="f(x) = \max(0, 1-x)"><span></span><span><script type="math/tex;mode=inline">f(x) = \max(0, 1-x)</script><span class="tex2jax_ignore math-holder">f(x) = \max(0, 1-x)</span></span></span>称为<b>hinge loss</b>。</li></ul><h2><b>七、朴素贝叶斯</b></h2><p data-pid="XPAli87_"><a href="https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6069267.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">推荐学习博客</a> </p><h2><b>八、XGBoost</b></h2><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-f44a62ba70adf36e5a966fbe0e4054e0_b.jpg" data-caption="" data-size="normal" data-rawwidth="1020" data-rawheight="567" class="origin_image zh-lightbox-thumb" width="1020" data-original="https://pic1.zhimg.com/v2-f44a62ba70adf36e5a966fbe0e4054e0_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1020' height='567'></svg>" data-caption="" data-size="normal" data-rawwidth="1020" data-rawheight="567" class="origin_image zh-lightbox-thumb lazy" width="1020" data-original="https://pic1.zhimg.com/v2-f44a62ba70adf36e5a966fbe0e4054e0_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-f44a62ba70adf36e5a966fbe0e4054e0_b.jpg"></div></figure><p data-pid="hpjtBV5f"><a href="https://link.zhihu.com/?target=https%3A//blog.csdn.net/a819825294/article/details/51206410" class=" wrap external" target="_blank" rel="nofollow noreferrer">XGBoost</a></p><h2><b>九、利用 sklearn 进行实战</b></h2><h2><b>参考资料</b></h2><p data-pid="SDT5ReF6"><a href="https://link.zhihu.com/?target=https%3A//book.douban.com/subject/10590856/" class=" wrap external" target="_blank" rel="nofollow noreferrer">《统计学习方法》李航</a> </p><p data-pid="7n5aNeoL"><a href="https://link.zhihu.com/?target=https%3A//cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">《机器学习》周志华</a> </p><p class="ztext-empty-paragraph"><br></p><blockquote data-pid="B1xzfAPe"><b>学习建议：多考虑各模型之间的优缺点，区别和适用范围。不要只知道用了哪些模型，而且知道为什么用这个模型。</b></blockquote><p data-pid="quclsr-D"><b>最后祝各位面试成功！并以下面一句话来勉励你我。</b></p><p data-pid="d2-Yz98X"><b>                                                                            路漫漫其修远兮，吾将上下而求索。</b></p><p class="ztext-empty-paragraph"><br></p><h2><b>竞赛社区（</b>数据竞赛的一站式服务<b>）</b></h2><p data-pid="rfVbOzFh">近期我们公众号和国内的开源组织Datawhale还有杰少一起成立了一个数据竞赛知识星球，并且邀请了国内的很多知名实战高手和赛圈的大佬，在推出的三天中也已经有了<b>500</b>多的用户报名，如果你<b>真的对实战感兴趣而且希望好好学习的话</b>，欢迎通过扫描下面的二维码进行报名，这样可以帮助您省下9元的报名费用，</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-fc123ff57408b2278d88f9448dbef70c_b.jpg" data-caption="" data-size="normal" data-rawwidth="1488" data-rawheight="874" class="origin_image zh-lightbox-thumb" width="1488" data-original="https://pic1.zhimg.com/v2-fc123ff57408b2278d88f9448dbef70c_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1488' height='874'></svg>" data-caption="" data-size="normal" data-rawwidth="1488" data-rawheight="874" class="origin_image zh-lightbox-thumb lazy" width="1488" data-original="https://pic1.zhimg.com/v2-fc123ff57408b2278d88f9448dbef70c_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-fc123ff57408b2278d88f9448dbef70c_b.jpg"></div></figure><h2>知识星球嘉宾（部分）</h2><p data-pid="qAfSsWMr">范晶晶：开源组织Datawhale创始人</p><p data-pid="85Diq4RS">张 杰：南京大学LAMDA硕士，天池数据科学家，KDD2019全球亚军</p><p data-pid="k5la0bHt">谈志旋：北京大学硕士，社交app算法负责人</p><p data-pid="2ABxJoRX">刘 洋：在读博士，IJCAI/KDD/ICME等顶会比赛前三，天池数据科学家</p><p data-pid="lgDuIxnk">钱 乾：资深算法工程师，Kaggle Grand Master</p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-4517eea81d4193aed7c42942589602f4_b.jpg" data-caption="" data-size="normal" data-rawwidth="750" data-rawheight="1020" class="origin_image zh-lightbox-thumb" width="750" data-original="https://pic1.zhimg.com/v2-4517eea81d4193aed7c42942589602f4_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='1020'></svg>" data-caption="" data-size="normal" data-rawwidth="750" data-rawheight="1020" class="origin_image zh-lightbox-thumb lazy" width="750" data-original="https://pic1.zhimg.com/v2-4517eea81d4193aed7c42942589602f4_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-4517eea81d4193aed7c42942589602f4_b.jpg"></div></figure><p class="ztext-empty-paragraph"><br></p><h2><b>数据竞赛群</b></h2><p data-pid="4koZ8uvU"><b>为了将热爱机器学习的大家聚在一起，推荐大家一个“数据竞赛”交流学习群，进群可与行业top级人物交流，可获得很强势的各方资源，大家有需要的可以进群哦</b></p><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-273c54a25d20b912d440dadb4d9cdc50_b.jpg" data-caption="" data-size="normal" data-rawwidth="956" data-rawheight="1265" class="origin_image zh-lightbox-thumb" width="956" data-original="https://pic1.zhimg.com/v2-273c54a25d20b912d440dadb4d9cdc50_r.jpg"/></noscript><div><img src="data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='956' height='1265'></svg>" data-caption="" data-size="normal" data-rawwidth="956" data-rawheight="1265" class="origin_image zh-lightbox-thumb lazy" width="956" data-original="https://pic1.zhimg.com/v2-273c54a25d20b912d440dadb4d9cdc50_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-273c54a25d20b912d440dadb4d9cdc50_b.jpg"></div></figure><p data-pid="RFoSwOaM"><b>一年半的竞赛经历，收获了两冠四亚一季的成绩。在这一年半，不仅坚持比赛，同时也坚持不断的分享。在我看来，分享是一个自我总结的一个过程。当然，这也是我与更多选手交流的一个平台，是一个相互学习提升的机会。愿我的分享能够帮助到你。</b></p><p data-pid="aK6JYsj8"><b>知乎专栏目的传播更多机器学习干货，数据竞赛方法。欢迎投稿！</b></p><div class="RichText-LinkCardContainer"><a target="_blank" href="https://zhuanlan.zhihu.com/DataAI" data-draft-node="block" data-draft-type="link-card" data-text="ML理论&amp;实践" class="LinkCard new css-1wr1m8" data-image="https://pic3.zhimg.com/v2-77acd3125654ffe9bfd9784c30d422fa_ipico.jpg" data-image-width="438" data-image-height="438"><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span><span class="LinkCard-image LinkCard-image--default"></span></a></div><div class="RichText-LinkCardContainer"><a target="_blank" href="https://zhuanlan.zhihu.com/shuma" data-draft-node="block" data-draft-type="link-card" data-text="数与码" class="LinkCard new css-1wr1m8" data-image="https://pic2.zhimg.com/v2-f250325aeb589be7eff01a3e2d67dfbd_ipico.jpg" data-image-width="200" data-image-height="200"><span class="LinkCard-contents"><span class="LinkCard-title loading" data-text="true"></span><span class="LinkCard-desc loading"></span></span><span class="LinkCard-image LinkCard-image--default"></span></a></div><p data-pid="1jVpoK5l"><b>路漫漫其修远兮，吾将上下而求索。</b></p></div></div></div></div><div role="button" tabindex="0" class="ContentItem-time">编辑于 2019-08-24 00:00</div><div class="Reward"><div><div class="Reward-tagline">「你的支持，是我创作的最大动力，欢迎赞赏！」</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19559450" target="_blank"><div class="css-1gomreu">机器学习</div></a></span></div><div class="Tag Topic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19553534&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/19553534" target="_blank"><div class="css-1gomreu">数据挖掘</div></a></span></div><div class="Tag Topic css-1s3a4zw" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20003862&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="//www.zhihu.com/topic/20003862" target="_blank"><div class="css-1gomreu">Kaggle</div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 414.6px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;32877396&quot;}}}"><span><button aria-label="赞同 925 " aria-live="polite" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor"><path fill-rule="evenodd" d="M13.792 3.681c-.781-1.406-2.803-1.406-3.584 0l-7.79 14.023c-.76 1.367.228 3.046 1.791 3.046h15.582c1.563 0 2.55-1.68 1.791-3.046l-7.79-14.023Z" clip-rule="evenodd"></path></svg></span>赞同 925</button><button aria-label="反对" aria-live="polite" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg width="10" height="10" viewBox="0 0 24 24" class="Zi Zi--TriangleDown" fill="currentColor"><path fill-rule="evenodd" d="M13.792 20.319c-.781 1.406-2.803 1.406-3.584 0L2.418 6.296c-.76-1.367.228-3.046 1.791-3.046h15.582c1.563 0 2.55 1.68 1.791 3.046l-7.79 14.023Z" clip-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Comment Button-zi" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>47 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Share Button-zi" fill="currentColor"><path d="M19.47 1.914a.8.8 0 0 1 1.204.778l-1.872 16.386a.9.9 0 0 1-1.204.743l-4.615-1.692a.7.7 0 0 0-.831.28l-1.927 3.02c-.43.674-1.474.369-1.474-.43v-3.865a.8.8 0 0 1 .179-.504l5.808-7.148a.595.595 0 0 0-.897-.781l-5.93 6.354a1.1 1.1 0 0 1-1.258.252L2.57 13.46a.8.8 0 0 1-.08-1.415l16.98-10.13Z"></path></svg></span>分享</button></div></div><button aria-live="polite" type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Heart Button-zi" fill="currentColor"><path fill-rule="evenodd" d="M12.004 4.934c1.015-.944 2.484-1.618 3.98-1.618 3.48 0 6.53 3.265 6.15 7.614-.11 1.254-.686 2.55-1.458 3.753-.778 1.215-1.79 2.392-2.845 3.419-1.054 1.028-2.168 1.923-3.161 2.566a9.96 9.96 0 0 1-1.41.777c-.418.182-.862.32-1.268.32s-.848-.137-1.267-.317a9.918 9.918 0 0 1-1.407-.771c-.992-.64-2.103-1.53-3.156-2.555-1.052-1.024-2.062-2.2-2.84-3.417-.77-1.208-1.346-2.51-1.456-3.775-.38-4.349 2.67-7.614 6.15-7.614 1.484 0 2.983.673 3.988 1.618Z" clip-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Star Button-zi" fill="currentColor"><path d="M10.484 3.307c.673-1.168 2.358-1.168 3.032 0l2.377 4.122a.25.25 0 0 0 .165.12l4.655.987c1.319.28 1.84 1.882.937 2.884l-3.186 3.535a.25.25 0 0 0-.063.193l.5 4.733c.142 1.34-1.222 2.33-2.453 1.782l-4.346-1.938a.25.25 0 0 0-.204 0l-4.346 1.938c-1.231.549-2.595-.442-2.453-1.782l.5-4.733a.25.25 0 0 0-.064-.193L2.35 11.42c-.903-1.002-.382-2.604.937-2.884l4.655-.987a.25.25 0 0 0 .164-.12l2.378-4.122Z"></path></svg></span>收藏</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Deliver Button-zi" fill="currentColor"><g fill-rule="evenodd" clip-rule="evenodd"><path d="M7.821 12a.75.75 0 0 1 .75-.75h6.857a.75.75 0 0 1 0 1.5H8.571a.75.75 0 0 1-.75-.75ZM8.965 8a.75.75 0 0 1 .75-.75h4.571a.75.75 0 0 1 0 1.5H9.715a.75.75 0 0 1-.75-.75Z"></path><path d="M7.527 3.15a2.35 2.35 0 0 0-2.309 1.91L3.165 15.84a.85.85 0 0 0-.015.16v2.5a2.35 2.35 0 0 0 2.35 2.35h13a2.35 2.35 0 0 0 2.35-2.35V16a.848.848 0 0 0-.015-.16L18.78 5.06a2.35 2.35 0 0 0-2.308-1.91H7.527Zm0 1.7a.65.65 0 0 0-.639.528l-1.88 9.872h13.984l-1.88-9.872a.65.65 0 0 0-.64-.528H7.528Z"></path></g></svg></span>申请转载</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="Zi Zi--Dots Button-zi" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></span></button></div></div></div></div></div><div class="Sticky--holder" style="position: static; inset: auto auto 0px 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div style="overflow: unset;" data-za-detail-view-path-module="CommentList" data-za-extra-module="{}"><div class="Comments-container css-plbgu"><div class="css-79elbk"><div><div class="css-1fo89v5"><img class="Avatar css-1oi6kgx" src="https://picd.zhimg.com/f1fbab298af0760f5e2c63554e086a15_l.jpg?source=32738c0c" srcset="https://picd.zhimg.com/f1fbab298af0760f5e2c63554e086a15_l.jpg?source=32738c0c 2x"><div class="css-x0pxoz"><div class="css-i6bazn"><div class="css-0"><div class="InputLike css-ip4bff Editable"><div class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext" style="min-height: 38px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-25ons" style="white-space: pre-wrap;">评论千万条，友善第一条</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-25ons" class="notranslate public-DraftEditor-content" contenteditable="true" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;" role="textbox"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="25ons" data-offset-key="v82m-0-0"><div data-offset-key="v82m-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="v82m-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><div></div><input multiple="" type="file" accept="image/webp,image/jpg,image/jpeg,image/png,image/gif" style="display: none;"></div></div></div></div></div></div><div class="css-z07uxh"><div class="css-we6n55"><div class="css-vpssrj"><div class="css-1k10w8f">47 条评论</div></div><div class="css-59erns"></div><div class="css-1hnxfhy"><div class="css-gjiv4z">默认</div><div class="css-1v9si9f">最新</div></div></div><div class="css-840pn3"><div class="css-1frn93x"><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/cae1fce57608b18d04ffb6384d0e0398" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/f2645c3ecd3e5cc4c44fd363b4675da2_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/f2645c3ecd3e5cc4c44fd363b4675da2_l.jpg?source=06d4cd63 2x" alt="三文鱼" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/cae1fce57608b18d04ffb6384d0e0398" target="_blank" class="css-1rd0h6f">三文鱼</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">和我目前的学习计划一模一样😊</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-02-16</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>3</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/fdbd74da0adb2d0759eadbea7b5c2ee7" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pica.zhimg.com/c0bf3db28bb37e9f5f8ef2fbc9bf50f0_l.jpg?source=06d4cd63" srcset="https://pica.zhimg.com/c0bf3db28bb37e9f5f8ef2fbc9bf50f0_l.jpg?source=06d4cd63 2x" alt="三怒" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/fdbd74da0adb2d0759eadbea7b5c2ee7" target="_blank" class="css-1rd0h6f">三怒</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>一行一行看完，写的非常到位。跟我工作学到的基本一致。 点赞！</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-02-26</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>1</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/084712877622939d9dfcaa6c58ea9bc4" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=06d4cd63 2x" alt="托马斯" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/084712877622939d9dfcaa6c58ea9bc4" target="_blank" class="css-1rd0h6f">托马斯</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">这个需要自己造轮子建模吗？还是直接sklearn调包？</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2020-05-04</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div><div><div class="css-8j5fyx"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/9985922041555e02bf909df460729888" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-11ee11ad05ddd9c3aa657420f51dd8c8_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-11ee11ad05ddd9c3aa657420f51dd8c8_l.jpg?source=06d4cd63 2x" alt="毛利大人" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/9985922041555e02bf909df460729888" target="_blank" class="css-1rd0h6f">毛利大人</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div><img src="https://pica.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae" class="css-1pj8qrb"></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>我感觉大部分时候sklearn就行了，自己写的会很慢，而且可能会有逻辑错误</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2021-03-29</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/a434241390bf0fb2e5a27f7119f6da44" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-d9d8cc0f21b32848f9e6135aec468802_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-d9d8cc0f21b32848f9e6135aec468802_l.jpg?source=06d4cd63 2x" alt="临渊" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/a434241390bf0fb2e5a27f7119f6da44" target="_blank" class="css-1rd0h6f">临渊</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>写的非常到位，面试打法就靠它了</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2020-02-09</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/6bc661e0fa7fd39e4fa50de819a6ad9a" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/21cf706e5541a83b75aadad74fb53f39_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/21cf706e5541a83b75aadad74fb53f39_l.jpg?source=06d4cd63 2x" alt="优秀的e小调" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/6bc661e0fa7fd39e4fa50de819a6ad9a" target="_blank" class="css-1rd0h6f">优秀的e小调</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>提个我个人认为有问题的说法（无伤大雅那种）：我觉得svm不是判别模型，因为svm不属于概率解释框架下的模型，所以无从谈起判别和生成的区别</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-06-25</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/323d73c07f20338024e8ffb901c14ac0" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/272a1fc723a5afcf90122f3ddd958da0_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/272a1fc723a5afcf90122f3ddd958da0_l.jpg?source=06d4cd63 2x" alt="喵帕斯" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/323d73c07f20338024e8ffb901c14ac0" target="_blank" class="css-1rd0h6f">喵帕斯</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">结合自己笔记看完了，明天去面试</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-06-10</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/a1960b7b3f10355a656de80b0915bdc0" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-e8adc94b07bc8a7dc8ec793441bc6e87_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-e8adc94b07bc8a7dc8ec793441bc6e87_l.jpg?source=06d4cd63 2x" alt="Nemo" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/a1960b7b3f10355a656de80b0915bdc0" target="_blank" class="css-1rd0h6f">Nemo</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>总结的真好</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-05-18</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/5bc4f4e77b951a948da5f7cdeb0425cb" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-108e5146f95d8180da500ef056f4319f_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-108e5146f95d8180da500ef056f4319f_l.jpg?source=06d4cd63 2x" alt="ipine" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/5bc4f4e77b951a948da5f7cdeb0425cb" target="_blank" class="css-1rd0h6f">ipine</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>赞</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-05-14</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/397be97736e4d5c9c717d9ce46dd3542" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pica.zhimg.com/v2-f12de74b3e1f0d6c7b2c5b146c463993_l.jpg?source=06d4cd63" srcset="https://pica.zhimg.com/v2-f12de74b3e1f0d6c7b2c5b146c463993_l.jpg?source=06d4cd63 2x" alt="Flash" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/397be97736e4d5c9c717d9ce46dd3542" target="_blank" class="css-1rd0h6f">Flash</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>怎么理解“Gini 指数更偏向于连续属性，熵更偏向于离散属性”呢？</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-04-06</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d6a9b761f6dc0ce864e87188574f8d19" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-1b28ec649aa11285b96c61f256f242f1_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-1b28ec649aa11285b96c61f256f242f1_l.jpg?source=06d4cd63 2x" alt="积极废人" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d6a9b761f6dc0ce864e87188574f8d19" target="_blank" class="css-1rd0h6f">积极废人</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>答主签了哪里</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-03-27</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div><div><div class="css-8j5fyx"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d6a9b761f6dc0ce864e87188574f8d19" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-1b28ec649aa11285b96c61f256f242f1_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-1b28ec649aa11285b96c61f256f242f1_l.jpg?source=06d4cd63 2x" alt="积极废人" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d6a9b761f6dc0ce864e87188574f8d19" target="_blank" class="css-1rd0h6f">积极废人</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div><svg width="12" height="12" viewBox="0 0 16 16" class="ZDI ZDI--ArrowRightAlt16 css-gx7lzm" fill="currentColor"><path d="M10.727 7.48a.63.63 0 0 1 0 1.039l-4.299 2.88c-.399.268-.926-.028-.926-.519V5.12c0-.491.527-.787.926-.52l4.299 2.881Z"></path></svg><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-1rd0h6f">鱼遇雨欲语与余</a></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">好的，谢谢哈。主要是实验室发一篇C类论文才能毕业</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-03-27</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-8j5fyx"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=06d4cd63 2x" alt="鱼遇雨欲语与余" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-1rd0h6f">鱼遇雨欲语与余</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div><span class="css-h9ndtl" href="">作者</span></div></div><svg width="12" height="12" viewBox="0 0 16 16" class="ZDI ZDI--ArrowRightAlt16 css-gx7lzm" fill="currentColor"><path d="M10.727 7.48a.63.63 0 0 1 0 1.039l-4.299 2.88c-.399.268-.926-.028-.926-.519V5.12c0-.491.527-.787.926-.52l4.299 2.881Z"></path></svg><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d6a9b761f6dc0ce864e87188574f8d19" target="_blank" class="css-1rd0h6f">积极废人</a></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">论文如果可以发些比较好的期刊的话，可以优先论文，如果压根没戏的话，就打比赛。目前很多大公司很重视科研能力，会尝试更多新的东西。所以我认为主论，辅比赛</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-03-27</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><button type="button" class="Button css-1p04wnp">展开其他 2 条回复<span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="ZDI ZDI--ArrowRightSmall24" fill="currentColor"><path fill-rule="evenodd" d="m13.248 12-4.025 3.78a.684.684 0 0 0 0 1.01.796.796 0 0 0 1.075 0l4.42-4.15a.867.867 0 0 0 0-1.28l-4.42-4.15a.796.796 0 0 0-1.075 0 .684.684 0 0 0 0 1.01L13.248 12Z" clip-rule="evenodd"></path></svg></span></button></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d4a67d15f1a4d825a51d7ec666c876c7" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pica.zhimg.com/v2-7d29be608f1dc573390c36a991385e7e_l.jpg?source=06d4cd63" srcset="https://pica.zhimg.com/v2-7d29be608f1dc573390c36a991385e7e_l.jpg?source=06d4cd63 2x" alt="登苍苍睬" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/d4a67d15f1a4d825a51d7ec666c876c7" target="_blank" class="css-1rd0h6f">登苍苍睬</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">楼主好文！帮大忙了<br>线性回归那里，用正态分布加极大似然估计是可以导出最小二乘法的。所以线性回归和逻辑回归实质上都是MLE</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-02-24</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/53d4ec7bae090ba01baf991513ea0ba9" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-fca0600882b37515ba3303cca0c062b8_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-fca0600882b37515ba3303cca0c062b8_l.jpg?source=06d4cd63 2x" alt="fwj7" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/53d4ec7bae090ba01baf991513ea0ba9" target="_blank" class="css-1rd0h6f">fwj7</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>5.3里的g(w)的最后一项是不是少乘了一个Xi ？</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-02-16</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/7811f30ac5f4077b3fe7228fe70c4c8b" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pica.zhimg.com/10ce24f1ce00ae2964e1b021d19c4713_l.jpg?source=06d4cd63" srcset="https://pica.zhimg.com/10ce24f1ce00ae2964e1b021d19c4713_l.jpg?source=06d4cd63 2x" alt="向前冲冲冲" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/7811f30ac5f4077b3fe7228fe70c4c8b" target="_blank" class="css-1rd0h6f">向前冲冲冲</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">答主现在一个月能拿多少了啊</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-01-05</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div><div><div class="css-8j5fyx"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/cd0ae642ce8242621819ffad01406310" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-dfac86de0aa6e0e94136c3552720e10c_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-dfac86de0aa6e0e94136c3552720e10c_l.jpg?source=06d4cd63 2x" alt="龙哥在思考" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/cd0ae642ce8242621819ffad01406310" target="_blank" class="css-1rd0h6f">龙哥在思考</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div><img src="https://pic1.zhimg.com/v2-4812630bc27d642f7cafcd6cdeca3d7a.jpg?source=88ceefae" class="css-1pj8qrb"></div></div><svg width="12" height="12" viewBox="0 0 16 16" class="ZDI ZDI--ArrowRightAlt16 css-gx7lzm" fill="currentColor"><path d="M10.727 7.48a.63.63 0 0 1 0 1.039l-4.299 2.88c-.399.268-.926-.028-.926-.519V5.12c0-.491.527-.787.926-.52l4.299 2.881Z"></path></svg><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-1rd0h6f">鱼遇雨欲语与余</a></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">那鱼佬睡前估计50 w了</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-07-19</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-8j5fyx"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=06d4cd63 2x" alt="鱼遇雨欲语与余" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-1rd0h6f">鱼遇雨欲语与余</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div><span class="css-h9ndtl" href="">作者</span></div></div><svg width="12" height="12" viewBox="0 0 16 16" class="ZDI ZDI--ArrowRightAlt16 css-gx7lzm" fill="currentColor"><path d="M10.727 7.48a.63.63 0 0 1 0 1.039l-4.299 2.88c-.399.268-.926-.028-.926-.519V5.12c0-.491.527-.787.926-.52l4.299 2.881Z"></path></svg><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/e08346b00ad9dd39adf74542c2d39664" target="_blank" class="css-1rd0h6f">Qichao Tang</a></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>某东</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-04-14</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><button type="button" class="Button css-1p04wnp">展开其他 3 条回复<span style="display: inline-flex; align-items: center;">​<svg width="24" height="24" viewBox="0 0 24 24" class="ZDI ZDI--ArrowRightSmall24" fill="currentColor"><path fill-rule="evenodd" d="m13.248 12-4.025 3.78a.684.684 0 0 0 0 1.01.796.796 0 0 0 1.075 0l4.42-4.15a.867.867 0 0 0 0-1.28l-4.42-4.15a.796.796 0 0 0-1.075 0 .684.684 0 0 0 0 1.01L13.248 12Z" clip-rule="evenodd"></path></svg></span></button></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/ee48b1a0a5a333fb9d751440dad34278" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-52c29d2a2da35ebff4e445ca5a2fdc31_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-52c29d2a2da35ebff4e445ca5a2fdc31_l.jpg?source=06d4cd63 2x" alt="阿拉蕾" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/ee48b1a0a5a333fb9d751440dad34278" target="_blank" class="css-1rd0h6f">阿拉蕾</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>假设已将输入空间划分为<img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1">个单元<img src="https://www.zhihu.com/equation?tex=R_1%2CR_2%2C...%2CR_M" alt="R_1,R_2,...,R_M" eeimg="1">，即<img src="https://www.zhihu.com/equation?tex=M" alt="M" eeimg="1">个特征。这里理理解不能理解为M个特征吧？</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2019-01-03</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/12f03ea3aa173bc00b910e64f4f1ae59" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-91c6823964b72e806121c0939add23de_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-91c6823964b72e806121c0939add23de_l.jpg?source=06d4cd63 2x" alt="喵喵爱睡觉" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/12f03ea3aa173bc00b910e64f4f1ae59" target="_blank" class="css-1rd0h6f">喵喵爱睡觉</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>哎，这些资料虽好，但是时间就是不够用。。</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-11-23</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/62b4e238ca3494d4b7ffb6f61ef59490" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-d8b01ad20c5f462b77f8eb92b33fea20_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-d8b01ad20c5f462b77f8eb92b33fea20_l.jpg?source=06d4cd63 2x" alt="ChainReaction" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/62b4e238ca3494d4b7ffb6f61ef59490" target="_blank" class="css-1rd0h6f">ChainReaction</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>mark</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-11-03</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/b030de69a62e3a4964cc951dfc8f3e56" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pica.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=06d4cd63" srcset="https://pica.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=06d4cd63 2x" alt="Rnanprince" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/b030de69a62e3a4964cc951dfc8f3e56" target="_blank" class="css-1rd0h6f">Rnanprince</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>随机森林列采样也是有放回吗</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-08-23</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/80ef7d9ab9cb58b6ed365e13de99c5b4" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-8afe48ac164f19fd71d6f14c5102513a_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-8afe48ac164f19fd71d6f14c5102513a_l.jpg?source=06d4cd63 2x" alt="艾哈迈德买买提" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/80ef7d9ab9cb58b6ed365e13de99c5b4" target="_blank" class="css-1rd0h6f">艾哈迈德买买提</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>图挂了呀</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-08-18</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/b79b3387e0d20b79f63882458ed431ae" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=06d4cd63" srcset="https://pic1.zhimg.com/v2-abed1a8c04700ba7d72b45195223e0ff_l.jpg?source=06d4cd63 2x" alt="知乎用户cst62l" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/b79b3387e0d20b79f63882458ed431ae" target="_blank" class="css-1rd0h6f">知乎用户cst62l</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>ID3 仅仅适用于二分类问题？？</p><p>为啥？</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-08-16</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div><div><div class="css-8j5fyx"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=06d4cd63 2x" alt="鱼遇雨欲语与余" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/09972d575614baad89afdaf4aa135905" target="_blank" class="css-1rd0h6f">鱼遇雨欲语与余</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div><span class="css-h9ndtl" href="">作者</span></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8"><p>感谢提问，再次思考了下，ID3也是适合多分类的，每次考虑一类就行。已修改</p></div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-08-16</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div></div><div><div class="css-194v73m"><div class="css-1jll2aj"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/1bab038b51f738281c9c97299f85b541" target="_blank" class="css-y06ck6"><img class="Avatar css-1s1htbw" src="https://picd.zhimg.com/v2-179620148c70eeae8346f0dcbd9eb08c_l.jpg?source=06d4cd63" srcset="https://picd.zhimg.com/v2-179620148c70eeae8346f0dcbd9eb08c_l.jpg?source=06d4cd63 2x" alt="Phen0barbita1" loading="lazy"></a></div></div><div class="css-14nvvry"><div class="css-1besdh8"><div class="css-swj9d4"><div class="css-1tww9qq"><div class="css-1gomreu"><a href="https://www.zhihu.com/people/1bab038b51f738281c9c97299f85b541" target="_blank" class="css-1rd0h6f">Phen0barbita1</a></div><div class="css-1qe0v6x"><div class="css-1ssbn0c"></div></div></div></div><svg width="16" height="16" viewBox="0 0 24 24" class="ZDI ZDI--Dots24 css-zgte1c" fill="currentColor"><path d="M5.34 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM13.665 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0ZM21.99 12a1.665 1.665 0 1 1-3.33 0 1.665 1.665 0 0 1 3.33 0Z"></path></svg></div><div class="CommentContent css-1ygdre8">感谢！</div><div class="css-dbby2p"><div class="css-82fsyv"><div class="css-12cl38p">2018-07-06</div></div><div class="css-18opwoy"><button type="button" class="Button  css-1o56bgb"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ChatBubbleFill24 css-15ro776" fill="currentColor"><path fill-rule="evenodd" d="M12 2.75a9.25 9.25 0 1 0 4.737 17.197l2.643.817a1 1 0 0 0 1.25-1.25l-.8-2.588A9.25 9.25 0 0 0 12 2.75Z" clip-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button css-h1yvwn" style="transform: none;"><span style="display: inline-flex; align-items: center;">​<svg width="1.2em" height="1.2em" viewBox="0 0 24 24" class="ZDI ZDI--ThumbFill24 css-15ro776" fill="currentColor"><path d="M8.5 4.078c0-1.834 1.986-2.979 3.573-2.06a4.826 4.826 0 0 1 2.379 4.71l-.114 1.022h3.581c2.53 0 4.334 2.454 3.58 4.868l-1.823 5.833a3.784 3.784 0 0 1-3.848 2.64c-2.372-.147-6.042-.341-8.828-.341H4.5A1.75 1.75 0 0 1 2.75 19V9.5c0-.967.784-1.75 1.75-1.75h.637a3.418 3.418 0 0 0 3.19-2.191c.115-.296.173-.611.173-.928v-.553Z"></path></svg></span>赞</button></div></div></div></div></div><div></div></div></div><div class="css-1yeqy9h"><div class="css-vurnku">点击查看全部评论</div><svg width="24" height="24" viewBox="0 0 24 24" class="ZDI ZDI--ArrowRightSmall24" fill="currentColor"><path fill-rule="evenodd" d="m13.248 12-4.025 3.78a.684.684 0 0 0 0 1.01.796.796 0 0 0 1.075 0l4.42-4.15a.867.867 0 0 0 0-1.28l-4.42-4.15a.796.796 0 0 0-1.075 0 .684.684 0 0 0 0 1.01L13.248 12Z" clip-rule="evenodd"></path></svg></div></div><div class="css-l8iyjs"></div><div class="css-805ti0" style="position: relative; box-shadow: 0px 0px;"><div><div class="css-59erns"><img class="Avatar css-1oi6kgx" src="https://picd.zhimg.com/f1fbab298af0760f5e2c63554e086a15_l.jpg?source=32738c0c" srcset="https://picd.zhimg.com/f1fbab298af0760f5e2c63554e086a15_l.jpg?source=32738c0c 2x"><div class="css-x0pxoz"><div class="css-i6bazn"><div class="css-1ryozpy"><div class="InputLike css-ip4bff Editable"><div class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext" style="min-height: 38px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-d4de4" style="white-space: pre-wrap;">评论千万条，友善第一条</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-d4de4" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="d4de4" data-offset-key="4patc-0-0"><div data-offset-key="4patc-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="4patc-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><div></div><input multiple="" type="file" accept="image/webp,image/jpg,image/jpeg,image/png,image/gif" style="display: none;"></div></div></div></div></div></div></div><div></div></div></div></div><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/DataAI"><div class="css-1gomreu"><img class="Avatar css-1any501" src="https://picd.zhimg.com/v2-77acd3125654ffe9bfd9784c30d422fa_l.jpg?source=172ae18b" srcset="https://picd.zhimg.com/v2-77acd3125654ffe9bfd9784c30d422fa_l.jpg?source=172ae18b 2x" alt="机器学习理论与数据竞赛实战"></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/DataAI"><div class="css-1gomreu">机器学习理论与数据竞赛实战</div></a></span></h2><div class="ContentItem-meta">公众号【Coggle数据科学】专注算法竞赛实战分享</div></div></div></div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/leemoo"><div class="css-1gomreu"><img class="Avatar css-1any501" src="https://pic1.zhimg.com/v2-8456af8026d9b1d75719eab1971a3662_l.jpg?source=172ae18b" srcset="https://pic1.zhimg.com/v2-8456af8026d9b1d75719eab1971a3662_l.jpg?source=172ae18b 2x" alt="每天都要机器学习哦"></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/leemoo"><div class="css-1gomreu">每天都要机器学习哦</div></a></span></h2><div class="ContentItem-meta">文章首发公众号“每天都要机器学习”</div></div></div></div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/Ryuyanshequ"><div class="css-1gomreu"><img class="Avatar css-1any501" src="https://pica.zhimg.com/v2-92f42b355edc10af5750445aa9897f58_l.jpg?source=172ae18b" srcset="https://pica.zhimg.com/v2-92f42b355edc10af5750445aa9897f58_l.jpg?source=172ae18b 2x" alt="中国R语言社区"></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/Ryuyanshequ"><div class="css-1gomreu">中国R语言社区</div></a></span></h2><div class="ContentItem-meta">公众号：R语言中文社区｜QQ群：612885032</div></div></div></div><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="//www.zhihu.com/column/c_1317403986237448192"><div class="css-1gomreu"><img class="Avatar css-1any501" src="https://pic1.zhimg.com/4b70deef7_l.jpg?source=172ae18b" srcset="https://pic1.zhimg.com/4b70deef7_l.jpg?source=172ae18b 2x" alt="算法工程师面试汇总"></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><span><a class="ColumnLink ColumnItem-Title" href="//www.zhihu.com/column/c_1317403986237448192"><div class="css-1gomreu">算法工程师面试汇总</div></a></span></h2><div class="ContentItem-meta">智力题、数据结构、机器学习、深度学习、面经等</div></div></div></div></ul></div><div role="complementary" aria-label="推荐阅读" class="Recommendations-Main" style="width: 1519px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled="" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg width="40" height="40" viewBox="0 0 24 24" fill="#d3d3d3" class="Zi Zi--ArrowLeft"><path fill-rule="evenodd" d="m10.752 12 4.025-3.78a.684.684 0 0 0 0-1.01.796.796 0 0 0-1.075 0l-4.42 4.15a.866.866 0 0 0 0 1.28l4.42 4.15a.796.796 0 0 0 1.075 0 .684.684 0 0 0 0-1.01L10.752 12Z" clip-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/35773847" class="PostItem"><div><h1 class="PostItem-Title">机器学习面试题精讲（二）</h1><p class="PostItem-Summary">上一讲：机器学习面试题精讲（一） 4. GBDTGBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree）。是一种迭代的决策树算法。 4.1 DT：回归树 Regressio…</p><div class="PostItem-Footer"><span>七月在线 ...</span><span class="PostItem-FooterTitle">发表于从零学AI</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/361113718" class="PostItem"><div><img src="https://picd.zhimg.com/v2-9c7c1d2e9f278b00867ba9cc902f2cf4_250x0.jpg?source=172ae18b" srcset="https://picd.zhimg.com/v2-9c7c1d2e9f278b00867ba9cc902f2cf4_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="机器学习100道经典面试题库！"><h1 class="PostItem-Title">机器学习100道经典面试题库！</h1><div class="PostItem-Footer"><span>六十度灰</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/35048471" class="PostItem"><div><h1 class="PostItem-Title">机器学习面试题精讲（一）</h1><p class="PostItem-Summary">作者：de,light 来源：http://gitbook.cn/gitchat/activity/5a2f6e9ed8ff692ea2b577b6 环境说明：Python 2.7；Sklearn 0.19.0；graphviz 0.8.1 决策树可视化。 1. 决策树 1.1 原理顾名思义…</p><div class="PostItem-Footer"><span>七月在线 ...</span><span class="PostItem-FooterTitle">发表于从零学AI</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/59201904" class="PostItem"><div><img src="https://pic1.zhimg.com/v2-b9b9b26f0d1db71b87c53b8bb04ebde1_250x0.jpg?source=172ae18b" srcset="https://pic1.zhimg.com/v2-b9b9b26f0d1db71b87c53b8bb04ebde1_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="常见机器学习面试题"><h1 class="PostItem-Title">常见机器学习面试题</h1><div class="PostItem-Footer"><span>大鱼</span><span class="PostItem-FooterTitle"></span></div></div></a><button class="PagingButton PagingButton-Next" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg width="40" height="40" viewBox="0 0 24 24" fill="#d3d3d3" class="Zi Zi--ArrowRight"><path fill-rule="evenodd" d="m13.248 12-4.025 3.78a.684.684 0 0 0 0 1.01.796.796 0 0 0 1.075 0l4.42-4.15a.867.867 0 0 0 0-1.28l-4.42-4.15a.796.796 0 0 0-1.075 0 .684.684 0 0 0 0 1.01L13.248 12Z" clip-rule="evenodd"></path></svg></button></ul></div></div></div></main><div role="complementary"><div class="CornerButtons"><div class="CornerAnimayedFlex CornerAnimayedFlex--hidden"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton css-gdd4kf"><svg width="24" height="24" viewBox="0 0 24 24" aria-hidden="true" class="Zi Zi--BackToTop" fill="currentColor"><path fill-rule="evenodd" d="M13.204 3.107a1.75 1.75 0 0 0-2.408 0L3.806 9.73c-1.148 1.088-.378 3.02 1.204 3.02h2.24V20c0 .966.784 1.75 1.75 1.75h6A1.75 1.75 0 0 0 16.75 20v-7.25h2.24c1.582 0 2.353-1.932 1.204-3.02l-6.99-6.623Z" clip-rule="evenodd"></path></svg></button></div></div></div></div></div><script id="js-clientConfig" type="text/json">{"fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com","vzuu":"https:\u002F\u002Fv.vzuu.com","openapi":"https:\u002F\u002Fopenapi.zhihu.com","svip":"https:\u002F\u002Fsvip.zhihu.com"},"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","videoHost":"video.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","allowSignUp":true,"refreshValidityPeriod":"30","refreshLimit":"0.8"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false,"article\u002FloadPostSearchEntity\u002F":false}},"entities":{"users":{"5b6cbee5d01bbbae90228c8baf07c3aa":{"uid":615253332370133000,"userType":"people","id":"5b6cbee5d01bbbae90228c8baf07c3aa"},"wang-he-13-93":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5.jpg?source=172ae18b","uid":"555381879923224576","userType":"people","isFollowing":false,"urlToken":"wang-he-13-93","id":"09972d575614baad89afdaf4aa135905","description":"一对一学习指导，可私信我。\n数据算法竞赛爱好者，国内竞赛方案最佳分享者，目前已获得五冠六亚一季的成绩。\n2022，WSDM-xmRec cup，亚军\n2020，腾讯广告算法大赛，冠军\n2020，TIANCHI-数字中国创新大赛-智慧海洋建设，冠军\n2019，TIANCHI-全球数据智能大赛【赛场二】，亚军\n2019，TIANCHI-安泰杯--跨境电商智能算法大赛，冠军\n2019，腾讯广告算法大赛，冠军\n2019，KDD Cup: Context-Aware Multi-Modal Transportation Recommendation，亚军\n2018，科大讯飞营销算法大赛，冠军\n2019，TIANCHI-OGeek算法挑战赛，亚军\n2019，JDATA-用户对品类下店铺的购买预测，亚军\n2019，第四届魔镜杯大赛数据应用大赛，亚军\n2019，TIANCHI-全球城市AI挑战赛，季军","name":"鱼遇雨欲语与余","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F09972d575614baad89afdaf4aa135905","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b","isOrg":false,"type":"people","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"1502692650108690432","medalName":"科学超新星 2022","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-dd2e3c787773d194fb3a9e1b7f3fc5c8_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-dd2e3c787773d194fb3a9e1b7f3fc5c8_l.png?source=172ae18b","description":"「向科学要答案 2022」活动期间，在相关领域内发布优质图文内容即可获得","medalAvatarFrame":""}}},"questions":{},"answers":{},"articles":{"32877396":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fcontent_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=BiBUKF0xBSkqGGFUAGV9Al12j8NIusj-094=&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"entityWords":[{"name":"信息增益","mention":"信息增益","matchorder":1,"begin":4863,"end":4867,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJZCgzkv6Hmga\u002Flop7nm4oSCU90aGVyVGVybRj\u002FJSCDJigBNQAAAAA6B2FydGljbGVAAEgAUiRiZDQzMmQ3Ny01NzgyLTRiOTEtODkxZS0zNTU3ZGRmNmRjMWE=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"information gain","mention":"information gain","matchorder":1,"begin":6403,"end":6419,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJdChBpbmZvcm1hdGlvbiBnYWluEglPdGhlclRlcm0YgzIgkzIoATUAAAAAOgdhcnRpY2xlQABIAFIkYmQ0MzJkNzctNTc4Mi00YjkxLTg5MWUtMzU1N2RkZjZkYzFh","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"逻辑回归模型","mention":"逻辑回归模型","matchorder":2,"begin":25073,"end":25079,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Math","score":0,"attachedInfoBytes":"sgJcChLpgLvovpHlm57lvZLmqKHlnosSBE1hdGgY8cMBIPfDASgCNQAAAAA6B2FydGljbGVAAEgAUiRiZDQzMmQ3Ny01NzgyLTRiOTEtODkxZS0zNTU3ZGRmNmRjMWE=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"决策树模型","mention":"决策树模型","matchorder":1,"begin":16407,"end":16412,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Computer","score":0,"attachedInfoBytes":"sgJdCg\u002FlhrPnrZbmoJHmqKHlnosSCENvbXB1dGVyGJeAASCcgAEoATUAAAAAOgdhcnRpY2xlQABIAFIkYmQ0MzJkNzctNTc4Mi00YjkxLTg5MWUtMzU1N2RkZjZkYzFh","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"决策树算法","mention":"决策树算法","matchorder":1,"begin":4812,"end":4817,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Computer","score":0,"attachedInfoBytes":"sgJbCg\u002FlhrPnrZbmoJHnrpfms5USCENvbXB1dGVyGMwlINElKAE1AAAAADoHYXJ0aWNsZUAASABSJGJkNDMyZDc3LTU3ODItNGI5MS04OTFlLTM1NTdkZGY2ZGMxYQ==","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"指数损失函数","mention":"指数损失函数","matchorder":2,"begin":18624,"end":18630,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Math","score":0,"attachedInfoBytes":"sgJcChLmjIfmlbDmjZ\u002FlpLHlh73mlbASBE1hdGgYwJEBIMaRASgCNQAAAAA6B2FydGljbGVAAEgAUiRiZDQzMmQ3Ny01NzgyLTRiOTEtODkxZS0zNTU3ZGRmNmRjMWE=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"递归返回","mention":"递归返回","matchorder":1,"begin":4569,"end":4573,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Math","score":0,"attachedInfoBytes":"sgJUCgzpgJLlvZLov5Tlm54SBE1hdGgY2SMg3SMoATUAAAAAOgdhcnRpY2xlQABIAFIkYmQ0MzJkNzctNTc4Mi00YjkxLTg5MWUtMzU1N2RkZjZkYzFh","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"互信息","mention":"互信息","matchorder":1,"begin":7206,"end":7209,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"OtherTerm","score":0,"attachedInfoBytes":"sgJWCgnkupLkv6Hmga8SCU90aGVyVGVybRimOCCpOCgBNQAAAAA6B2FydGljbGVAAEgAUiRiZDQzMmQ3Ny01NzgyLTRiOTEtODkxZS0zNTU3ZGRmNmRjMWE=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"属性测试","mention":"属性测试","matchorder":1,"begin":4014,"end":4018,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Computer","score":0,"attachedInfoBytes":"sgJYCgzlsZ7mgKfmtYvor5USCENvbXB1dGVyGK4fILIfKAE1AAAAADoHYXJ0aWNsZUAASABSJGJkNDMyZDc3LTU3ODItNGI5MS04OTFlLTM1NTdkZGY2ZGMxYQ==","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"极大似然估计法","mention":"极大似然估计法","matchorder":1,"begin":25141,"end":25148,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Math","score":0,"attachedInfoBytes":"sgJfChXmnoHlpKfkvLznhLbkvLDorqHms5USBE1hdGgYtcQBILzEASgBNQAAAAA6B2FydGljbGVAAEgAUiRiZDQzMmQ3Ny01NzgyLTRiOTEtODkxZS0zNTU3ZGRmNmRjMWE=","isOnAB":false,"isNatural":1,"isDelete":false},{"name":"弱分类器","mention":"弱分类器","matchorder":1,"begin":18206,"end":18210,"entityid":0,"isBookMark":false,"link":{"linkType":0,"linkUrl":"","docType":"","topicToken":""},"entityClass":"Computer","score":0,"attachedInfoBytes":"sgJaCgzlvLHliIbnsbvlmagSCENvbXB1dGVyGJ6OASCijgEoATUAAAAAOgdhcnRpY2xlQABIAFIkYmQ0MzJkNzctNTc4Mi00YjkxLTg5MWUtMzU1N2RkZjZkYzFh","isOnAB":false,"isNatural":1,"isDelete":false}],"id":32877396,"title":"机器学习面试干货精讲","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32877396","imageUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-379d54a5d9d4f5065efec52438a8a901_720w.jpg?source=172ae18b","titleImage":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-379d54a5d9d4f5065efec52438a8a901_720w.jpg?source=172ae18b","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6611c71ca64d987c8bc698730219f489_200x112.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"404\" data-watermark=\"\" data-original-src=\"\" data-watermark-src=\"\" data-private-watermark-src=\"\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6611c71ca64d987c8bc698730219f489_r.jpg\" class=\"origin_image inline-img zh-lightbox-thumb\"\u002F\u003E写于二零一八年二月十五日，农历大年三十 \u003Cb\u003E整体内容划分\u003C\u002Fb\u003E：推荐书单、公开课面试核心内容提纲本内容涉及模型核心数学公式，把本人面试中常被问到问题以及模型知识点的总结，起到提纲挈领作用，在准备的过程中抓住每个模型的重点。 \u003Cb\u003E面试核心内容目录：\u003C\u002Fb\u003E决策树随…","created":1518706344,"updated":1566576048,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5.jpg?source=172ae18b","uid":"555381879923224576","userType":"people","isFollowing":false,"urlToken":"wang-he-13-93","id":"09972d575614baad89afdaf4aa135905","description":"一对一学习指导，可私信我。\n数据算法竞赛爱好者，国内竞赛方案最佳分享者，目前已获得五冠六亚一季的成绩。\n2022，WSDM-xmRec cup，亚军\n2020，腾讯广告算法大赛，冠军\n2020，TIANCHI-数字中国创新大赛-智慧海洋建设，冠军\n2019，TIANCHI-全球数据智能大赛【赛场二】，亚军\n2019，TIANCHI-安泰杯--跨境电商智能算法大赛，冠军\n2019，腾讯广告算法大赛，冠军\n2019，KDD Cup: Context-Aware Multi-Modal Transportation Recommendation，亚军\n2018，科大讯飞营销算法大赛，冠军\n2019，TIANCHI-OGeek算法挑战赛，亚军\n2019，JDATA-用户对品类下店铺的购买预测，亚军\n2019，第四届魔镜杯大赛数据应用大赛，亚军\n2019，TIANCHI-全球城市AI挑战赛，季军","name":"鱼遇雨欲语与余","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F09972d575614baad89afdaf4aa135905","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b","isOrg":false,"type":"people","badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"1502692650108690432","medalName":"科学超新星 2022","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-dd2e3c787773d194fb3a9e1b7f3fc5c8_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-dd2e3c787773d194fb3a9e1b7f3fc5c8_l.png?source=172ae18b","description":"「向科学要答案 2022」活动期间，在相关领域内发布优质图文内容即可获得","medalAvatarFrame":""}},"commentPermission":"all","copyrightPermission":"need_review","state":"published","ipInfo":"","imageWidth":1920,"imageHeight":1080,"content":"\u003Cp data-pid=\"6Q5oGChn\"\u003E写于二零一八年二月十五日，农历大年三十\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"TafwsDRU\"\u003E\u003Cb\u003E整体内容划分\u003C\u002Fb\u003E：\u003C\u002Fblockquote\u003E\u003Cul\u003E\u003Cli data-pid=\"i1ym-Y7B\"\u003E推荐书单、公开课\u003C\u002Fli\u003E\u003Cli data-pid=\"a2XrswCs\"\u003E面试核心内容提纲\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"VfNJcPq1\"\u003E本内容涉及模型核心数学公式，把本人面试中常被问到问题以及模型知识点的总结，起到提纲挈领作用，在准备的过程中抓住每个模型的重点。\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"aCcfWcJo\"\u003E\u003Cb\u003E面试核心内容目录：\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cul\u003E\u003Cli data-pid=\"eXWdb57M\"\u003E决策树\u003C\u002Fli\u003E\u003Cli data-pid=\"fQKmPpX9\"\u003E随机森林\u003C\u002Fli\u003E\u003Cli data-pid=\"4oDG2rUJ\"\u003EAdaboost\u003C\u002Fli\u003E\u003Cli data-pid=\"iAh3Ccmo\"\u003EGBDT\u003C\u002Fli\u003E\u003Cli data-pid=\"xIcZgzFD\"\u003Elogistic 回归\u003C\u002Fli\u003E\u003Cli data-pid=\"CbrDKRwK\"\u003ESVM支持向量机\u003C\u002Fli\u003E\u003Cli data-pid=\"-Rt-yPHO\"\u003E朴素贝叶斯\u003C\u002Fli\u003E\u003Cli data-pid=\"_HCBG1IJ\"\u003Exgboost\u003C\u002Fli\u003E\u003Cli data-pid=\"Pvn-FAGp\"\u003Elightgbm\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Chr\u002F\u003E\u003Ch2\u003E        推荐书单、公开课\u003C\u002Fh2\u003E\u003Cp data-pid=\"6Dfee-tf\"\u003E\u003Cb\u003E网络公开课：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"4CBel4mV\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fopen.163.com\u002Fspecial\u002Fopencourse\u002Fdaishu.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E麻省理工公开课 线性代数\u003C\u002Fa\u003E——学习矩阵理论及线性代数的基本知识，推荐笔记\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F28277072\" class=\"internal\"\u003EMIT线性代数课程精细笔记by忆瑧\u003C\u002Fa\u003E。\u003C\u002Fli\u003E\u003Cli data-pid=\"7mX8dnlL\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.csie.ntu.edu.tw\u002F~htlin\u002Fmooc\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E台大机器学习公开课\u003C\u002Fa\u003E——授课人林轩田，课程分为机器学习基石和机器学习技法两部分。\u003C\u002Fli\u003E\u003Cli data-pid=\"qLW2n-aE\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.coursera.org\u002Fspecializations\u002Fmachine-learning\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E华盛顿大学机器学习公开课\u003C\u002Fa\u003E——华盛顿大学在Coursera开的机器学习专项课，共有四个部分，这个课直接从应用案例开始讲起，对于回归，分类，协同过滤和情感分析等都会具体去讲怎么实现应用，并且会告诉你如何在Python中利用网上一些现有的库来实现特定的功能，也就是说基本上在课程的第一部分你就可以全面的知道机器学习能够在现实生活中的应用，以及简单方式去实现一些功能。\u003C\u002Fli\u003E\u003Cli data-pid=\"-fIRaoAS\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fopen.163.com\u002Fspecial\u002Fopencourse\u002Fmachinelearning.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E斯坦福大学公开课 机器学习\u003C\u002Fa\u003E——Andrew Ng（吴恩达）在斯坦福开设的CS229，难度远高于Coursera上面的课程。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"3zaXQB-j\"\u003E\u003Cb\u003E书单：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"AS6gV_Dk\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F26708119\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《机器学习》\u003C\u002Fa\u003Eby 周志华，这是一本中国无数Machine Learning热爱者的启蒙教材，它非常合适没有任何背景的初学者看，每一个概念的来龙去脉讲的都很细致，是一本大而全的教材。\u003C\u002Fli\u003E\u003Cli data-pid=\"lNEx7Xuc\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F10590856\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《统计学习方法》\u003C\u002Fa\u003Eby 李航，这本书主要偏优化和推倒，推倒相应算法的时候可以参考这本书。虽然只是薄薄的一本，但全是精华内容。\u003C\u002Fli\u003E\u003Cli data-pid=\"9DvyXjHx\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F24703171\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《机器学习实战\u003C\u002Fa\u003E》by Peter Harrington，可以对应《统计学习方法》进行实现代码。\u003C\u002Fli\u003E\u003Cli data-pid=\"sV5E-TJ4\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.rmki.kfki.hu\u002F~banmi\u002Felte\u002FBishop%2520-%2520Pattern%2520Recognition%2520and%2520Machine%2520Learning.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《Pattern Recognition And Machine Learning》\u003C\u002Fa\u003E by Christopher Bishop，属于机器学习进阶书籍，内容全，建议首先完成以上三本书籍，再看这本。\u003C\u002Fli\u003E\u003Cli data-pid=\"90yL1DMB\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F25779298\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《利用Python进行数据分析》\u003C\u002Fa\u003E——Python常用的库学习（numpy，pandas）\u003C\u002Fli\u003E\u003Cli data-pid=\"iqr1dXAT\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F25910559\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《剑指offer》\u003C\u002Fa\u003E——常见面试题，面试必备。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"Xj8hsNoF\"\u003E最后推荐一个网站，收集了进阶的机器学习各种资源\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fgithub.com\u002FJustFollowUs\u002FMachine-Learning%23learning_route\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EGithub机器学习Machine-Learning\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Chr\u002F\u003E\u003Ch2\u003E面试核心内容提纲\u003C\u002Fh2\u003E\u003Ch2\u003E\u003Cb\u003E一、决策树\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch2\u003E\u003Cb\u003E1.1 原理\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"AjC6dee6\"\u003E决策树是一种基本的分类与回归方法，其模型就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如\u003Cb\u003ECART 只能是二叉树\u003C\u002Fb\u003E），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。根节点包含整个样本集，每个叶节都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。\u003C\u002Fp\u003E\u003Cp data-pid=\"9VgLCru0\"\u003E举个例子：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6611c71ca64d987c8bc698730219f489_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb\" width=\"573\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6611c71ca64d987c8bc698730219f489_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;573&#39; height=&#39;404&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"573\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6611c71ca64d987c8bc698730219f489_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-6611c71ca64d987c8bc698730219f489_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"ubUBryU1\"\u003E就像上面这个例子，训练集由三个特征：outlook(天气)，humidity（度），windy（是否有风）。那么我们该如何选择特征对训练集进行划分？连续型特征（比如湿度）划分的阈值又是如何确定的？\u003Cbr\u002F\u003E       决策树的生成就是不断的选择最优的特征对训练集进行划分，是一个递归的过程。递归返回的条件有三种：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"Mgp3MA2s\"\u003E当前节点包含的样本属于同一类别，无需划分；\u003C\u002Fli\u003E\u003Cli data-pid=\"GqPfKaXM\"\u003E当前属性集为空，或所有样本在属性集上取值相同，无法划分；\u003C\u002Fli\u003E\u003Cli data-pid=\"MlYro6X6\"\u003E当前节点包含样本集合为空，无法划分。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E1.2 ID3、C4.5、CART\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"_iDr2cOe\"\u003E这三个是非常著名的决策树算法。简单粗暴来说:\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"338bffOJ\"\u003EID3 使用信息增益作为选择特征的准则；\u003C\u002Fli\u003E\u003Cli data-pid=\"4Fo26RZD\"\u003EC4.5 使用信息增益比作为选择特征的准则；\u003C\u002Fli\u003E\u003Cli data-pid=\"qdgfyKGX\"\u003ECART 使用 Gini 指数作为选择特征的准则。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cblockquote data-pid=\"Q749oO9M\"\u003E\u003Cb\u003E熵(entropy)\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"lnHnfXmH\"\u003E在信息论与概率论中，熵用于表示\u003Cb\u003E随机变量不确定性的度量\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"tUZJl8xs\"\u003E设X是一个有限状态的离散型随机变量，其概率分布为:\u003C\u002Fp\u003E\u003Cp data-pid=\"rdJHclFZ\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28X+%3D+x_i%29+%3D+p_i%2C%5C+i%3D1%2C2%2C%5Ccdots%2Cn\" alt=\"P(X = x_i) = p_i,\\ i=1,2,\\cdots,n\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"7_7mikZb\"\u003E则随机变量\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X\" alt=\"X\" eeimg=\"1\"\u002F\u003E的熵定义为\u003C\u002Fp\u003E\u003Cp data-pid=\"Oz-evZ_g\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28X%29%3D+-+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+p_%7Bi%7Dlog%28p_i%29\" alt=\"H(X)= - \\sum_{i=1}^{n} p_{i}log(p_i)\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"p6qBJZPL\"\u003E熵越大，则随机变量的不确定性越大。\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"7tkmd4Jo\"\u003E\u003Cb\u003E条件熵(conditional entropy)\u003C\u002Fb\u003E \u003C\u002Fblockquote\u003E\u003Cp data-pid=\"-t9u42Cp\"\u003E随机变量\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X\" alt=\"X\" eeimg=\"1\"\u002F\u003E给定的条件下，随机变量\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Y\" alt=\"Y\" eeimg=\"1\"\u002F\u003E的条件熵\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28Y%7CX%29\" alt=\"H(Y|X)\" eeimg=\"1\"\u002F\u003E定义为：\u003C\u002Fp\u003E\u003Cp data-pid=\"Zv7baPnU\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28Y%7CX%29+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dp_i+H%28Y%7CX%3Dx_i%29\" alt=\"H(Y|X) = \\sum_{i=1}^{n}p_i H(Y|X=x_i)\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"J-pa3Exy\"\u003E其中，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p_i+%3D+P%28X+%3D+x_i%29\" alt=\"p_i = P(X = x_i)\" eeimg=\"1\"\u002F\u003E。\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"yFDAZJlc\"\u003E\u003Cb\u003E信息增益(information gain)\u003C\u002Fb\u003E \u003C\u002Fblockquote\u003E\u003Cp data-pid=\"wwnCl_aq\"\u003E信息增益表示的是：\u003Cb\u003E\u003Cu\u003E得知特征X的信息而使得类Y的信息的不确定性减少的程度\u003C\u002Fu\u003E\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp data-pid=\"GI1Pk5KB\"\u003E具体定义如下。\u003C\u002Fp\u003E\u003Cp data-pid=\"JegxCKQc\"\u003E特征A对训练数据集D的信息增益\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g%28D%2CA%29\" alt=\"g(D,A)\" eeimg=\"1\"\u002F\u003E定义为集合D的经验熵\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28D%29\" alt=\"H(D)\" eeimg=\"1\"\u002F\u003E与特征A给定条件下D的经验条件熵\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28D%7CA%29\" alt=\"H(D|A)\" eeimg=\"1\"\u002F\u003E之差，即\u003C\u002Fp\u003E\u003Cp data-pid=\"VxJ3RNZV\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g%28D%2CA%29%3DH%28D%29-H%28D%7CA%29\" alt=\"g(D,A)=H(D)-H(D|A)\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"VoFUfJEM\"\u003E一般地，熵\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28Y%29\" alt=\"H(Y)\" eeimg=\"1\"\u002F\u003E与条件熵\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H%28Y%7CX%29\" alt=\"H(Y|X)\" eeimg=\"1\"\u002F\u003E之差称为互信息(mutual information).\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"vMP2G280\"\u003E\u003Cb\u003EID3\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"I0vZVjRE\"\u003E熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。\u003C\u002Fp\u003E\u003Cp data-pid=\"tAgYRsOi\"\u003E\u003Cb\u003E信息增益 = 划分前熵 - 划分后熵\u003C\u002Fb\u003E。信息增益越大，则意味着使用属性a来进行划分所获得的 “纯度提升” 越大 。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"TSs5gyDr\"\u003EID3优点是理论清晰、方法简单、学习能力较强，但也存在一些缺点：\u003C\u002Fblockquote\u003E\u003Cul\u003E\u003Cli data-pid=\"NIFuBxgQ\"\u003E只能处理分类属性的数据，不能处理连续的数据；\u003C\u002Fli\u003E\u003Cli data-pid=\"xvBX1KZD\"\u003E划分过程会由于子集规模过小而造成统计特征不充分而停止；\u003C\u002Fli\u003E\u003Cli data-pid=\"8LeTX_nX\"\u003EID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cblockquote data-pid=\"QF__nDtv\"\u003E\u003Cb\u003EC4.5\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"ibZKtU8C\"\u003EC4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。\u003Cb\u003E信息增益比 = 信息增益 \u002F 划分前熵\u003C\u002Fb\u003E  选择信息增益比最大的作为最优特征。公式：\u003C\u002Fp\u003E\u003Cp data-pid=\"BevVAm9n\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g_R%28D%2CA%29%3D%5Cfrac%7Bg%28D%2CA%29%7D%7BH_A%28D%29%7D\" alt=\"g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"K43-S490\"\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=H_A%28D%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7Dlog_2%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D%7D\" alt=\"H_A(D)=-\\sum_{i=1}^{n}{\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}}\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E 是特征 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A\" alt=\"A\" eeimg=\"1\"\u002F\u003E 取值的个数\u003C\u002Fp\u003E\u003Cp data-pid=\"2P_hSX9_\"\u003EC4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"g7Rfk7iW\"\u003E\u003Cb\u003ECART\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"0eNJwrF1\"\u003ECART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。\u003C\u002Fp\u003E\u003Cp data-pid=\"jvSRE0HS\"\u003ECART 的全称是分类与回归树（classification and regression tree）。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"BtJ7X2e7\"\u003E回归树\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"3ftgdjCO\"\u003E使用平方误差最小化准则来选择特征并进行划分。每一个叶子节点给出的预测值，是划分到该叶子节点的所有样本目标值的均值，这样只是在给定划分的情况下最小化了平方误差。\u003C\u002Fp\u003E\u003Cp data-pid=\"xDR2eHNX\"\u003E要确定最优化分，还需要遍历所有属性，以及其所有的取值来分别尝试划分并计算在此种划分情况下的最小平方误差，选取最小的作为此次划分的依据。由于回归树生成使用平方误差最小化准则，所以又叫做最小二乘回归树。\u003C\u002Fp\u003E\u003Cp data-pid=\"w15DlBCS\"\u003E具体为，假设已将输入空间划分为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=M\" alt=\"M\" eeimg=\"1\"\u002F\u003E 个单元\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_1%2CR_2%2C...%2CR_M\" alt=\"R_1,R_2,...,R_M\" eeimg=\"1\"\u002F\u003E，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=M\" alt=\"M\" eeimg=\"1\"\u002F\u003E 个特征，并且在每个单元\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R_m\" alt=\"R_m\" eeimg=\"1\"\u002F\u003E上有一个固定的输出值\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_m\" alt=\"c_m\" eeimg=\"1\"\u002F\u003E，于是回归树可以表示为\u003C\u002Fp\u003E\u003Cp data-pid=\"pFaCE8YC\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28x%29%3D%5Csum_%7Bm%3D1%7D%5E%7BM%7Dc_%7Bm%7DI%28x%5Cin+R_m%29\" alt=\"f(x)=\\sum_{m=1}^{M}c_{m}I(x\\in R_m)\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"IGRlgI1Q\"\u003E当输入空间的划分确定时，可以用平方误差\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csum_%7Bx_i+%5Cin+R_m%7D%28y_i+-+f%28x_i%29%29%5E2\" alt=\"\\sum_{x_i \\in R_m}(y_i - f(x_i))^2\" eeimg=\"1\"\u002F\u003E来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优值。\u003C\u002Fp\u003E\u003Cp data-pid=\"myiLqFpf\"\u003E\u003Cb\u003E选择最优切分特征 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 和切分点 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E ,具体做法为:\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"iGIr3IXJ\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmin_%7Bj%2Cs%7D%5B%5Cmin_%7Bc_1%7D%5Csum_%7Bx_i%5Cin+R_1%28j%2Cs%29%7D%7B%28y_i-c_1%29%5E2%7D%2B%5Cmin_%7Bc_2%7D%5Csum_%7Bx_i%5Cin+R_2%28j%2Cs%29%7D%7B%28y_i-c_2%29%5E2%7D%5D\" alt=\"\\min_{j,s}[\\min_{c_1}\\sum_{x_i\\in R_1(j,s)}{(y_i-c_1)^2}+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}{(y_i-c_2)^2}]\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"VQQLyyMz\"\u003E\u003Cb\u003E首先遍历特征 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E ,对固定的切分特征 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=j\" alt=\"j\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E 扫描切分点 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=s\" alt=\"s\" eeimg=\"1\"\u002F\u003E\u003Cb\u003E ,选择使上式达到最小值的对 \u003C\u002Fb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28j%2Cs%29\" alt=\"(j,s)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"6V2y2rs1\"\u003E分类树\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"9S5ylX6w\"\u003E使用 Gini 指数最小化准则来选择特征并进行划分；Gini 指数表示集合的不确定性，或者是不纯度。基尼指数越大，集合不确定性越高，不纯度也越大。这一点和熵类似。另一种理解基尼指数的思路是，基尼指数是为了最小化误分类的概率。\u003C\u002Fp\u003E\u003Cp data-pid=\"o7DuGkgi\"\u003E假设有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K\" alt=\"K\" eeimg=\"1\"\u002F\u003E 个类别，样本点属于第\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=k\" alt=\"k\" eeimg=\"1\"\u002F\u003E类的概率为\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p_k\" alt=\"p_k\" eeimg=\"1\"\u002F\u003E,则概率分布的基尼指数定义为：\u003C\u002Fp\u003E\u003Cp data-pid=\"OZpxUkx4\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Gini%28p%29%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7Dp_%7Bk%7D%281-p_k%29%3D1-%5Csum_%7Bk%3D1%7D%5E%7BK%7Dp_%7Bk%7D%5E%7B2%7D\" alt=\"Gini(p)=\\sum_{k=1}^{K}p_{k}(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E1.3 信息增益 vs 信息增益比\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"BL_fAzIP\"\u003E之所以引入了信息增益比，是由于信息增益的一个缺点。那就是：信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E1.4 Gini 指数 vs 熵\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"yizDAQx_\"\u003E既然这两个都可以\u003Cb\u003E表示数据的不确定性，不纯度\u003C\u002Fb\u003E。那么这两个有什么区别那？\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"0WXbYbFg\"\u003EGini 指数的计算不需要对数运算，更加高效；\u003C\u002Fli\u003E\u003Cli data-pid=\"A-YAG_yz\"\u003EGini 指数更偏向于连续属性，熵更偏向于离散属性。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E1.5 剪枝\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"fou8-sdo\"\u003E决策树算法很容易过拟合（overfitting），剪枝算法就是用来防止决策树过拟合，提高泛华性能的方法，剪枝分为预剪枝与后剪枝。\u003C\u002Fp\u003E\u003Cp data-pid=\"vspY71cV\"\u003E\u003Cb\u003E预剪枝\u003C\u002Fb\u003E是指在决策树的生成过程中，对每个节点在划分前先进行评估，若当前的划分不能带来泛化性能的提升，则停止划分，并将当前节点标记为叶节点。\u003C\u002Fp\u003E\u003Cp data-pid=\"kAkWlsZJ\"\u003E\u003Cb\u003E后剪枝\u003C\u002Fb\u003E是指先从训练集生成一颗完整的决策树，然后自底向上对非叶节点进行考察，若将该节点对应的子树替换为叶节点，能带来泛化性能的提升，则将该子树替换为叶节点。\u003C\u002Fp\u003E\u003Cp data-pid=\"0bBWGutl\"\u003E那么怎么来判断是否带来泛化性能的提升呢？最简单的就是留出法，即预留一部分数据作为验证集来进行性能评估。\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32627500\" class=\"internal\"\u003E交叉验证\u003C\u002Fa\u003E \u003C\u002Fp\u003E\u003Cblockquote data-pid=\"g7Ufqmnh\"\u003E\u003Cb\u003E预剪枝和后剪枝的比较\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E1.6 总结\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"pdafh-Ck\"\u003E决策树算法主要包括三个部分：特征选择、树的生成、树的剪枝。常用算法有 ID3、C4.5、CART。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"U1pPxePK\"\u003E特征选择。特征选择的目的是选取能够对训练集分类的特征。特征选择的关键是准则：信息增益、信息增益比、Gini 指数；\u003C\u002Fli\u003E\u003Cli data-pid=\"EPSQd9Gm\"\u003E决策树的生成。通常是利用信息增益最大、信息增益比最大、Gini 指数最小作为特征选择的准则。从根节点开始，递归的生成决策树。相当于是不断选取局部最优特征，或将训练集分割为基本能够正确分类的子集；\u003C\u002Fli\u003E\u003Cli data-pid=\"LpTodN7C\"\u003E决策树的剪枝。决策树的剪枝是为了防止树的过拟合，增强其泛化能力。包括预剪枝和后剪枝。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E二、随机森林（Random Forest）\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"0P9NpTBD\"\u003E要说随机森林就要先说 Bagging，要说 Bagging 就要先说\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32412775\" class=\"internal\"\u003E集成学习\u003C\u002Fa\u003E。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"H1lmlo9V\"\u003E\u003Cb\u003E2.1 集成学习方法\u003C\u002Fb\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"CtI2dify\"\u003E集成学习（ensemble learning）通过构建并组合多个学习器来完成学习任务。集成学习将多个学习器进行结合，常获得比单一学习器显著优越的泛化性能。\u003C\u002Fp\u003E\u003Cp data-pid=\"Ramqtkgv\"\u003E个体学习器通常由一个现有的学习算法从训练数据产生（比如 C4.5,BP 等），此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是“同质”的，同质集成中的个体学习器称“基学习器”。当然也存在不同质的学习器存在于统一集成中，常称为“组件学习器”或直接称为个体学习器。\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32877396\u002Fedit\" class=\"internal\"\u003E《机器学习》周志华\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"lLz718Fk\"\u003E\u003Cb\u003E原则\u003C\u002Fb\u003E：要获得比单一学习器更好的性能，个体学习器应该好而不同。即个体学习器应该具有一定的准确性，不能差于弱学习器，并且具有多样性，即学习器之间有差异。\u003C\u002Fp\u003E\u003Cp data-pid=\"Vwyy3Iap\"\u003E根据个体学习器的生成方式，目前集成学习分为两大类：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"lxHUASwp\"\u003E个体学习器之间存在强依赖关系、必须串行生成的序列化方法。代表是 Boosting；\u003C\u002Fli\u003E\u003Cli data-pid=\"e_hOUHVE\"\u003E个体学习器之间不存在强依赖关系、可同时生成的并行化方法。代表是 Bagging 和随机森林（Random Forest）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E2.2 Bagging\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0178314df8b6fa4a2754ae07890a82db_b.jpg\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"559\" class=\"content_image\" width=\"420\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;420&#39; height=&#39;559&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"559\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0178314df8b6fa4a2754ae07890a82db_b.jpg\"\u002F\u003E\u003Cfigcaption\u003Ebagging平均法\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"eOKsi2Vg\"\u003E前面提到，想要集成算法获得性能的提升，个体学习器应该具有独立性。虽然 “独立” 在现实生活中往往无法做到，但是可以设法让基学习器尽可能的有较大的差异。\u003C\u002Fp\u003E\u003Cp data-pid=\"FG0g1_KG\"\u003EBagging 给出的做法就是对训练集进行采样，产生出若干个不同的子集，再从每个训练子集中训练一个基学习器。由于训练数据不同，我们的基学习器可望具有较大的差异。\u003C\u002Fp\u003E\u003Cp data-pid=\"zPtfebCA\"\u003EBagging 是并行式集成学习方法的代表，采样方法是自助采样法（bootstrap），用的是有放回的采样。初始训练集中大约有 63.2% 的数据出现在采样集中。\u003C\u002Fp\u003E\u003Cp data-pid=\"U36k_C34\"\u003EBagging 在预测输出进行结合时，对于分类问题，采用简单投票法；对于回归问题，采用简单平均法。\u003C\u002Fp\u003E\u003Cp data-pid=\"PvtyZs31\"\u003EBagging 优点：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"2rwyxHYh\"\u003E高效。Bagging 集成与直接训练基学习器的复杂度同阶；\u003C\u002Fli\u003E\u003Cli data-pid=\"t9AnFyty\"\u003EBagging 能不经修改的适用于多分类、回归任务；\u003C\u002Fli\u003E\u003Cli data-pid=\"07rFRKTA\"\u003E包外估计。使用剩下的样本作为验证集进行包外估计（out-of-bag estimate）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cblockquote data-pid=\"_617iz3h\"\u003EBagging 主要关注降低方差。（low variance）\u003C\u002Fblockquote\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b9355fcccc2be171253913eda7c6fd0c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b9355fcccc2be171253913eda7c6fd0c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;600&#39; height=&#39;567&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b9355fcccc2be171253913eda7c6fd0c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-b9355fcccc2be171253913eda7c6fd0c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cblockquote data-pid=\"K3JzYg5O\"\u003E\u003Cb\u003E偏差（bias）：\u003C\u002Fb\u003E描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如上图第二行所示。\u003Cbr\u002F\u003E\u003Cb\u003E方差（variance）：\u003C\u002Fb\u003E描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如上图右列所示。\u003C\u002Fblockquote\u003E\u003Ch2\u003E\u003Cb\u003E2.3 随机森林（Random Forest）\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"LPD9tYbJ\"\u003E\u003Cb\u003E2.3.1 原理\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"s-4dWSZE\"\u003E随机森林（Random Forest）是 Bagging 的一个变体。Ramdon Forest 在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入随机属性选择。\u003C\u002Fp\u003E\u003Cp data-pid=\"rqZ03VE1\"\u003E原来决策树从所有特征中，选择最优特征。Ramdom Forest 的每一颗决策树中的每一个节点，先从该节点的特征集中通过bootstrap的方法随机选择 K 个特征的子集，然后从这个特征子集中通过决策树算法选择最优特征进行划分。（\u003Cb\u003E注意\u003C\u002Fb\u003E：除了随机获取特征以外，还能随机获取样本集，使用这些随机抽取的样本得到不同的决策树）\u003C\u002Fp\u003E\u003Cp data-pid=\"nqkn3M3R\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K\" alt=\"K\" eeimg=\"1\"\u002F\u003E 控制了随机性的引入程度，是一个重要的超参数。\u003C\u002Fp\u003E\u003Cp data-pid=\"AT9_8Fmb\"\u003E\u003Cb\u003E预测 \u003C\u002Fb\u003E：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"ShKRmtCJ\"\u003E分类：简单投票法；\u003C\u002Fli\u003E\u003Cli data-pid=\"65a4MtR0\"\u003E回归：简单平均法。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"39JmLo7F\"\u003E\u003Cb\u003E2.3.2 具体构造过程\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"P5SfT1di\"\u003E从原始训练集中使用bootstrap方法随机有放回采样选出m个样本，共进行n_tree次采样，生成n_tree个训练集\u003C\u002Fli\u003E\u003Cli data-pid=\"NjH3vZlX\"\u003E对于n_tree个训练集，我们分别训练n_tree个决策树模型\u003C\u002Fli\u003E\u003Cli data-pid=\"G8C3bP4m\"\u003E对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益\u002F信息增益比\u002F基尼指数选择最好的特征进行分裂\u003C\u002Fli\u003E\u003Cli data-pid=\"tB0YMUey\"\u003E每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类。在决策树的分裂过程中不需要剪枝\u003C\u002Fli\u003E\u003Cli data-pid=\"AQsXke1G\"\u003E将生成的多棵决策树组成随机森林。对于分类问题，按多棵树分类器投票决定最终分类结果；对于回归问题，由多棵树预测值的均值决定最终预测结果\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"2xm592hE\"\u003E\u003Cb\u003E2.3.3 优缺点\u003C\u002Fb\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"W9kcqMcg\"\u003E\u003Cb\u003E优点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"-RcKWldE\"\u003E由于每次不再考虑全部的属性，而是一个属性子集，所以相比于 Bagging 计算开销更小，训练效率更高；\u003C\u002Fli\u003E\u003Cli data-pid=\"aC3-tMEq\"\u003E由于增加了属性的扰动，随机森林中基学习器的性能降低，使得在随机森林在起始时候性能较差，但是随着基学习器的增多，随机森林通常会收敛于更低的泛化误差，相比于 Bagging；\u003C\u002Fli\u003E\u003Cli data-pid=\"0P21IrtN\"\u003E两个随机性的引入，使得随机森林不容易陷入过拟合，具有很好的抗噪声能力；\u003C\u002Fli\u003E\u003Cli data-pid=\"yorXO4W0\"\u003E对数据的适应能力强，可以处理离散和连续的，无需要规范化；\u003C\u002Fli\u003E\u003Cli data-pid=\"Lswx77Xa\"\u003E可以得到变量的重要性， 基于oob错误率（袋外错误率out-of-bag error）和基于 Gini 系数的变化。\u003C\u002Fli\u003E\u003Cli data-pid=\"hWmitJPr\"\u003E不同决策树可以由不同主机并行训练生成，效率很高\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"fdHoIc26\"\u003E\u003Cb\u003E缺点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"mPj1d1h6\"\u003E在噪声较大的时候容易过拟合。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E三、AdaBoost\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"N1sFfulz\"\u003EAdaBoost 是 Boosting 的代表，前面我们提到 Boosting 是集成学习中非常重要的一类串行化学习方法。\u003C\u002Fblockquote\u003E\u003Ch2\u003E\u003Cb\u003E3.1 Boosting\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"xTI8FOir\"\u003EBoosting 是指个体学习器之间存在强依赖关系，必须串行序列化生成的集成学习方法。他的思想来源是三个臭皮匠顶个诸葛亮。Boosting 意为提升，意思是希望将每个弱学习器提升为强学习器。\u003C\u002Fp\u003E\u003Cp data-pid=\"V8KX4oDH\"\u003E\u003Cb\u003E工作机制如下：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"s2IXkf8H\"\u003E先从初始训练集中学习一个基学习器；\u003C\u002Fli\u003E\u003Cli data-pid=\"EnIpAeXT\"\u003E根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续收到更多关注；\u003C\u002Fli\u003E\u003Cli data-pid=\"3z3e_fW2\"\u003E基于调整后的样本分布来训练下一个基学习器；\u003C\u002Fli\u003E\u003Cli data-pid=\"G6USMBEd\"\u003E如此反复，直到基学习器数目达到 T，最终将这 T 个基学习器进行加权结合。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"xTa-DY8J\"\u003E对训练样本分布调整，主要是通过增加误分类样本的权重，降低正确分类样本的权重。\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"2RImJpb-\"\u003EBoosting 关注的主要是降低偏差。（low bias）\u003C\u002Fblockquote\u003E\u003Ch2\u003E\u003Cb\u003E3.2 AdaBoost 原理\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"adrhY_tA\"\u003E\u003Cb\u003E面临两个问题：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"THaDEC_Y\"\u003E在每一轮，如何改变训练数据的概率分布或者权值分布。（也可以重采样，但是 AdaBoost 没这么做）；\u003C\u002Fli\u003E\u003Cli data-pid=\"mQ6yKHzj\"\u003E如何将弱分类器组合成强分类器。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cp data-pid=\"yviKjghn\"\u003E\u003Cb\u003EAdaBoost 的做法：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"YwX_2t9j\"\u003E提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类的样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮弱分类器的关注；\u003C\u002Fli\u003E\u003Cli data-pid=\"rfczEgqy\"\u003E采用加权多数表决。具体的，加大分类错误率低的分类器的权值，使其在表决中起较大作用，减少分类误差率大的弱分类器的权值，使其在表决中起较小作用。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cblockquote data-pid=\"djIIkkjl\"\u003E弱分类器被线性组合成为一个强分类器。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"Nv7if9BV\"\u003E\u003Cb\u003E训练目标：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"h9E183UY\"\u003E最小化指数损失函数。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"Qq83I1bL\"\u003E\u003Cb\u003E三部分组成：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"UZvMoVVw\"\u003E分类器权重更新公式；\u003C\u002Fli\u003E\u003Cli data-pid=\"MKB0VGto\"\u003E样本分布（也就是样本权重）更新公式；\u003C\u002Fli\u003E\u003Cli data-pid=\"Pu_QstaV\"\u003E加性模型。 最小化指数损失函数。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E3.3 AdaBoost 优缺点\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"4hc-3nrl\"\u003E\u003Cb\u003E优点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"xWZ5QS7b\"\u003EAdaboost作为分类器时，分类精度很高\u003C\u002Fli\u003E\u003Cli data-pid=\"eqA1BHJS\"\u003E在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。\u003C\u002Fli\u003E\u003Cli data-pid=\"szadCbp7\"\u003E作为简单的二元分类器时，构造简单，结果可理解。\u003C\u002Fli\u003E\u003Cli data-pid=\"56haZ8sl\"\u003E不容易发生过拟合\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"_PQo8r1I\"\u003E\u003Cb\u003E缺点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"QWpJui5m\"\u003E对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E四、GBDT\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"FL5VuGuj\"\u003EGBDT（Gradient Boosting Decision Tree）又叫 MART（Multiple Additive Regression Tree），是一种迭代的决策树算法。本小节从以下几个方面进行阐述：\u003C\u002Fblockquote\u003E\u003Col\u003E\u003Cli data-pid=\"JioRa3LU\"\u003ERegression Decision Tree(DT)；\u003C\u002Fli\u003E\u003Cli data-pid=\"bc5tTI4z\"\u003EGradient Boosting(GB)；\u003C\u002Fli\u003E\u003Cli data-pid=\"pBqODnlA\"\u003EShrinkage(算法的一个重要演进分支，目前大部分源码都是基于该版本实现)；\u003C\u002Fli\u003E\u003Cli data-pid=\"zl4LQyjw\"\u003EGBDT 适用范围；\u003C\u002Fli\u003E\u003Cli data-pid=\"Rsg2JYSm\"\u003E与随机森林的对比。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E4.1 DT：回归树 Regression Decision Tree\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"ffSH9WJa\"\u003EGBDT 中的树全部都是回归树，核心就是累加所有树的结果作为最终结果。只有回归树的结果累加起来才是有意义的，分类的结果累加是没有意义的。\u003C\u002Fp\u003E\u003Cp data-pid=\"49OrI3HR\"\u003EGBDT 调整之后可以用于分类问题，但是内部还是回归树。\u003C\u002Fp\u003E\u003Cp data-pid=\"HxNrmCDE\"\u003E这部分和决策树中的是一样的，无非就是特征选择。回归树用的是最小化均方误差，分类树是用的是最小化基尼指数（CART）\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E4.2 GB：梯度迭代 Gradient Boosting\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"UIBmiKYq\"\u003E首先 Boosting 是一种集成方法。通过对弱分类器的组合得到强分类器，他是串行的，几个弱分类器之间是依次训练的。GBDT 的核心就在于，每一颗树学习的是之前所有树结论和的残差。\u003C\u002Fp\u003E\u003Cp data-pid=\"XfMnkS0t\"\u003EGradient 体现在：无论前面一颗树的 cost function 是什么，是均方差还是均差，只要它以误差作为衡量标准，那么残差向量都是它的全局最优方向，这就是 Gradient。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E4.3 Shrinkage\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"-DTKh_bF\"\u003EShrinkage（缩减）是 GBDT 算法的一个重要演进分支，目前大部分的源码都是基于这个版本的。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"3shiqvBc\"\u003E核心思想在于：Shrinkage 认为每次走一小步来逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易防止过拟合。\u003C\u002Fp\u003E\u003Cp data-pid=\"rIU7B-dM\"\u003E也就是说，它不信任每次学习到的残差，它认为每棵树只学习到了真理的一小部分，累加的时候只累加一小部分，通过多学习几棵树来弥补不足。\u003C\u002Fp\u003E\u003Cp data-pid=\"1MkuDo_U\"\u003E具体的做法就是：仍然以残差作为学习目标，但是对于残差学习出来的结果，只累加一小部分（step* 残差）逐步逼近目标，step 一般都比较小 0.01-0.001, 导致各个树的残差是渐变而不是陡变的。\u003C\u002Fp\u003E\u003Cp data-pid=\"LjXTj5-Y\"\u003E本质上，Shrinkage 为每一颗树设置了一个 weight，累加时要乘以这个 weight，但和 Gradient 没有关系。\u003C\u002Fp\u003E\u003Cp data-pid=\"2bmrin5M\"\u003E这个 weight 就是 step。跟 AdaBoost 一样，Shrinkage 能减少过拟合也是经验证明的，目前还没有理论证明。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E4.4 GBDT 适用范围\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli data-pid=\"UdIJv9Gr\"\u003EGBDT 可以适用于回归问题（线性和非线性）；\u003C\u002Fli\u003E\u003Cli data-pid=\"ZMFPtrak\"\u003EGBDT 也可用于二分类问题（设定阈值，大于为正，否则为负）和多分类问题。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E4.5 GBDT 和随机森林区别（重点）\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli data-pid=\"-GMc8lxJ\"\u003EGBDT 和随机森林的相同点：\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"IDLyeqE7\"\u003E都是由多棵树组成；\u003C\u002Fli\u003E\u003Cli data-pid=\"iL7gxZRQ\"\u003E最终的结果都由多棵树共同决定。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cli data-pid=\"hASC177I\"\u003EGBDT 和随机森林的不同点：\u003C\u002Fli\u003E\u003Cul\u003E\u003Cli data-pid=\"XPUIQVtl\"\u003E组成随机森林的可以是分类树、回归树；组成 GBDT 只能是回归树；\u003C\u002Fli\u003E\u003Cli data-pid=\"Xujb2a6l\"\u003E组成随机森林的树可以并行生成（Bagging）；GBDT 只能串行生成（Boosting）；这两种模型都用到了Bootstrap的思想。\u003C\u002Fli\u003E\u003Cli data-pid=\"-XrOefUf\"\u003E对于最终的输出结果而言，随机森林使用多数投票或者简单平均；而 GBDT 则是将所有结果累加起来，或者加权累加起来；\u003C\u002Fli\u003E\u003Cli data-pid=\"skx5_IZX\"\u003E随机森林对异常值不敏感，GBDT 对异常值非常敏感；\u003C\u002Fli\u003E\u003Cli data-pid=\"rUQUUzI0\"\u003E随机森林对训练集一视同仁权值一样，GBDT 是基于权值的弱分类器的集成；\u003C\u002Fli\u003E\u003Cli data-pid=\"IduaYiS_\"\u003E随机森林通过减小模型的方差提高性能，GBDT 通过减少模型偏差提高性能。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003ETIP\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"hbJfbbIi\"\u003E\u003Cb\u003E1. GBDT 相比于决策树有什么优点\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"wipbgV-3\"\u003E泛化性能更好！GBDT 的最大好处在于，每一步的残差计算其实变相的增大了分错样本的权重，而已经分对的样本则都趋向于 0。这样后面就更加专注于那些分错的样本。\u003C\u002Fp\u003E\u003Cp data-pid=\"X3jVPllV\"\u003E\u003Cb\u003E2. Gradient 体现在哪里？\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"NyrwHLtO\"\u003E可以理解为残差是全局最优的绝对方向，类似于求梯度。\u003C\u002Fp\u003E\u003Cp data-pid=\"u2USmTcB\"\u003E\u003Cb\u003E3. re-sample\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"B-zFUjFV\"\u003EGBDT 也可以在使用残差的同时引入 Bootstrap re-sampling，GBDT 多数实现版本中引入了这个选项，但是是否一定使用有不同的看法。\u003C\u002Fp\u003E\u003Cp data-pid=\"FNYa8yBe\"\u003E原因在于 re-sample 导致的随机性，使得模型不可复现，对于评估提出一定的挑战，比如很难确定性能的提升是由于 feature 的原因还是 sample 的随机因素。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E五、Logistic 回归（熟悉推导过程）\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Col\u003E\u003Cli data-pid=\"AyGmqzKg\"\u003ELR 原理；\u003C\u002Fli\u003E\u003Cli data-pid=\"0mvsiOoi\"\u003E参数估计；\u003C\u002Fli\u003E\u003Cli data-pid=\"W4PaEy9x\"\u003ELR 的正则化；\u003C\u002Fli\u003E\u003Cli data-pid=\"yMnW22wc\"\u003E为什么 LR 能比线性回归好？\u003C\u002Fli\u003E\u003Cli data-pid=\"t3GUxooA\"\u003ELR 与 MaxEnt 的关系。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Ch2\u003E\u003Cb\u003E5.1 LR 模型原理\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"Qyw86nyx\"\u003E首先必须给出Logistic分布：\u003C\u002Fp\u003E\u003Cp data-pid=\"sr7j-Cux\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 是位置参数，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 是形状参数。对称点是 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28%5Cmu%EF%BC%8C%5Cfrac%7B1%7D%7B2%7D%29\" alt=\"(\\mu，\\frac{1}{2})\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"\u002F\u003E 越小，函数在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"\u002F\u003E 附近越陡峭。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f5b7494c719d064bc2dd908734c5687_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"618\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"618\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f5b7494c719d064bc2dd908734c5687_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;618&#39; height=&#39;128&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"618\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"618\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f5b7494c719d064bc2dd908734c5687_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-0f5b7494c719d064bc2dd908734c5687_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c5035b34e00929a3940feb6cbbbd5b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1414\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb\" width=\"1414\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c5035b34e00929a3940feb6cbbbd5b5_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1414&#39; height=&#39;536&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1414\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1414\" data-original=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c5035b34e00929a3940feb6cbbbd5b5_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8c5035b34e00929a3940feb6cbbbd5b5_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"Y8thVq84\"\u003E然后，二分类 LR 模型，是参数化的 logistic 分布，使用条件概率来表示：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-587fdf891b6be074712676c8e8f2a922_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-587fdf891b6be074712676c8e8f2a922_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;532&#39; height=&#39;236&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-587fdf891b6be074712676c8e8f2a922_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-587fdf891b6be074712676c8e8f2a922_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"t8_ddZwW\"\u003E然后，一个事件的几率（odds）：指该事件发生与不发生的概率比值：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7aeed809a148ccdb68b3598a431c3310_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7aeed809a148ccdb68b3598a431c3310_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;427&#39; height=&#39;100&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7aeed809a148ccdb68b3598a431c3310_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7aeed809a148ccdb68b3598a431c3310_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"RiuWljF6\"\u003E对数几率：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-65e6847355e4a5d5f23f9d5da458009f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"99\" class=\"content_image\" width=\"346\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;346&#39; height=&#39;99&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"99\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-65e6847355e4a5d5f23f9d5da458009f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"hNU3hw0-\"\u003E那么对于逻辑回归而言， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Y%3D1\" alt=\"Y=1\" eeimg=\"1\"\u002F\u003E 的对数几率就是：\u003C\u002Fp\u003E\u003Cp data-pid=\"zUMccMbP\"\u003E最终，输出 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Y%3D1\" alt=\"Y=1\" eeimg=\"1\"\u002F\u003E 的对数几率是输入 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 的线性函数表示的模型，这就是逻辑回归模型。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E5.2 参数估计\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"dnzg4auF\"\u003E在统计学中，常常使用极大似然估计法来估计参数。即找到一组参数，使得在这组参数下，我们数据的似然度（概率）最大。\u003Cb\u003E(\u003C\u002Fb\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F26614750\" class=\"internal\"\u003E极大似然估计\u003C\u002Fa\u003E\u003Cb\u003E：就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值，即‘模型已定，参数未知’\u003C\u002Fb\u003E)\u003C\u002Fp\u003E\u003Cp data-pid=\"B1jAwX7a\"\u003E似然函数：\u003C\u002Fp\u003E\u003Cp data-pid=\"EDBaBGKr\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Beqnarray%2A%7D+L%28w%29%26%3D%26+%5Cprod_%7B%7D%5B%5Cpi%28x_i%29%5D%5E%7By_i%7D%5B1-%5Cpi%28x_i%29%5D%5E%7B1-y_i%7D%5Cend%7Beqnarray%2A%7D\" alt=\"\\begin{eqnarray*} L(w)&amp;=&amp; \\prod_{}[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}\\end{eqnarray*}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"8GRdsTtZ\"\u003E对数似然函数：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-df5d56c026b6c33f9e7f64f23883ceb0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-df5d56c026b6c33f9e7f64f23883ceb0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;740&#39; height=&#39;294&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-df5d56c026b6c33f9e7f64f23883ceb0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-df5d56c026b6c33f9e7f64f23883ceb0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cblockquote data-pid=\"1uhq1cbO\"\u003E\u003Cb\u003E如何得到逻辑回归的损失函数?\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"PGXfdCPl\"\u003E我们希望上式越大越好，换句话说，对于给定样本数量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=n\" alt=\"n\" eeimg=\"1\"\u002F\u003E ，我们希望 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=-%5Cfrac%7B1%7D%7Bn%7DlnL%28w%29\" alt=\"-\\frac{1}{n}lnL(w)\" eeimg=\"1\"\u002F\u003E 越小越好，这个也就是LogLoss。\u003C\u002Fp\u003E\u003Cp data-pid=\"s6V4QnvH\"\u003E对应的损失函数：\u003C\u002Fp\u003E\u003Cp data-pid=\"hhgWOTj4\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28W%29+%3D+-%5Cfrac%7B1%7D%7Bn%7DlnL%28w%29\" alt=\"J(W) = -\\frac{1}{n}lnL(w)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cblockquote data-pid=\"lbcPEk84\"\u003E\u003Cb\u003E为什么LR的输入特征一般是离散的而不是连续的？\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"XjVkm11I\"\u003E     在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：\u003C\u002Fp\u003E\u003Col\u003E\u003Cli data-pid=\"0LHJ1zD2\"\u003E离散特征的增加和减少都很容易，易于模型的快速迭代；\u003C\u002Fli\u003E\u003Cli data-pid=\"iwvxBIIH\"\u003E稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；\u003C\u002Fli\u003E\u003Cli data-pid=\"qz_X7aEh\"\u003E离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；\u003C\u002Fli\u003E\u003Cli data-pid=\"eSHpyMq0\"\u003E逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；\u003C\u002Fli\u003E\u003Cli data-pid=\"nAK6HuHL\"\u003E离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；\u003C\u002Fli\u003E\u003Cli data-pid=\"NHXDWQ7w\"\u003E特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；\u003C\u002Fli\u003E\u003Cli data-pid=\"4LgbcQhq\"\u003E特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cblockquote data-pid=\"ooZUo-1U\"\u003E李沐少帅指出，模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"Zhlw3ZPn\"\u003E\u003Cb\u003E5.3 最优化方法\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"Il1iFnxx\"\u003E逻辑回归模型的参数估计中，最后就是对 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28W%29\" alt=\"J(W)\" eeimg=\"1\"\u002F\u003E 求最小值。这种不带约束条件的最优化问题，常用梯度下降，牛顿法和拟牛顿法，共轭梯度法来解决。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"fCtGHJRK\"\u003E\u003Cb\u003E使用梯度下降法求解逻辑回归参数估计\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"CfUe84Dh\"\u003E求 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28W%29\" alt=\"J(W)\" eeimg=\"1\"\u002F\u003E 梯度： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=g%28w%29\" alt=\"g(w)\" eeimg=\"1\"\u002F\u003E :\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b21ae715c95063854a74311be9a8ef3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"616\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b21ae715c95063854a74311be9a8ef3f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;616&#39; height=&#39;456&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"616\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b21ae715c95063854a74311be9a8ef3f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-b21ae715c95063854a74311be9a8ef3f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"0UZfS0BW\"\u003E更新 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W_k\" alt=\"W_k\" eeimg=\"1\"\u002F\u003E ：\u003C\u002Fp\u003E\u003Cp data-pid=\"PYtK71R1\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W_%7Bk%2B1%7D+%3D+W_k+-+%5Clambda+g%28W_k%29\" alt=\"W_{k+1} = W_k - \\lambda g(W_k)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E5.4 正则化\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"X4_uTjkQ\"\u003E正则化为了解决过拟合问题。分为 L1 和 L2 正则化。主要通过修正损失函数，加入模型复杂性评估；\u003Cbr\u002F\u003E正则化是符合\u003Cb\u003E奥卡姆剃刀原理\u003C\u002Fb\u003E：在所有可能的模型中，能够很好的解释已知数据并且十分简单的才是最好的模型。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"8qK4qdi3\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=J%28w%29%5CRightarrow+J%28w%29+%2B+%5Clambda%7B%7C%7Cw%7C%7C%7D_p\" alt=\"J(w)\\Rightarrow J(w) + \\lambda{||w||}_p\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"mzxJZEk5\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p\" alt=\"p\" eeimg=\"1\"\u002F\u003E 表示范数， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%3D1\" alt=\"p=1\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%3D2\" alt=\"p=2\" eeimg=\"1\"\u002F\u003E 分别应用 L1 和 L2 正则。\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"XdZOgqzb\"\u003EL1 正则化。向量中各元素绝对值之和。又叫做稀疏规则算子（Lasso regularization）。关键在于能够实现特征的自动选择，参数稀疏可以避免非必要的特征引入的噪声；\u003C\u002Fli\u003E\u003Cli data-pid=\"btW1JTRZ\"\u003EL2 正则化。使得每个元素都尽可能的小，但是都不为零。在回归里面，有人把他的回归叫做岭回归（Ridge Regression），也有人叫他 “权值衰减”（weight decay）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-22ee181fbe6046ccec1dc443228ae176_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1160\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-22ee181fbe6046ccec1dc443228ae176_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1160&#39; height=&#39;564&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1160\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-22ee181fbe6046ccec1dc443228ae176_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-22ee181fbe6046ccec1dc443228ae176_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"u-4MN9dk\"\u003E一句话总结就是：L1 会趋向于产生少量的特征，而其他的特征都是 0，而 L2 会选择更多的特征，这些特征都会接近于 0.\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E5.5 逻辑回归 vs 线性回归\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"48ihljJ3\"\u003E首先，逻辑回归比线性回归要好。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"on1DHhoD\"\u003E两者都属于广义线性模型。\u003C\u002Fp\u003E\u003Cp data-pid=\"hgktOeVV\"\u003E线性回归优化目标函数用的最小二乘法，而逻辑回归用的是最大似然估计。逻辑回归只是在线性回归的基础上，将加权和通过 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=sigmoid\" alt=\"sigmoid\" eeimg=\"1\"\u002F\u003E 函数，映射到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=0-1\" alt=\"0-1\" eeimg=\"1\"\u002F\u003E 范围内空间。\u003C\u002Fp\u003E\u003Cp data-pid=\"3SvKL8ga\"\u003E线性回归在整个实数范围内进行预测，敏感度一致，而分类范围，需要在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5B0%2C1%5D\" alt=\"[0,1]\" eeimg=\"1\"\u002F\u003E 。而逻辑回归就是一种减小预测范围，将预测值限定为 [0,1] 间的一种回归模型。\u003C\u002Fp\u003E\u003Cp data-pid=\"Qew5UHtR\"\u003E逻辑曲线在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z%3D0\" alt=\"z=0\" eeimg=\"1\"\u002F\u003E 时，十分敏感，在 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z%3E%3E0\" alt=\"z&gt;&gt;0\" eeimg=\"1\"\u002F\u003E 或 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=z%3C%3C0\" alt=\"z&lt;&lt;0\" eeimg=\"1\"\u002F\u003E 处，都不敏感，将预测值限定为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%280%2C1%29\" alt=\"(0,1)\" eeimg=\"1\"\u002F\u003E 。逻辑回归的鲁棒性比线性回归要好。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E5.6 逻辑回归模型 vs 最大熵模型 MaxEnt\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"qBlAKkQY\"\u003E简单粗暴的说：逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应为二类时的特殊情况，也就是说，当逻辑回归扩展为多类别的时候，就是最大熵模型。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"5IC__BoG\"\u003E最大熵原理：学习概率模型的时候，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E5.7 最大熵模型\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"O2h_poCF\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.cnblogs.com\u002Fpinard\u002Fp\u002F6093948.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E最大熵模型推荐学习博客\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E六、SVM 支持向量机\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"1oTY5J1-\"\u003E虽然咱们的目标是尽可能的不涉及到公式，但是提到 SVM 就没有办法不涉及到公式推导，因为面试中只要问到 SVM，最基本也是最难的问题就是：SVM 的对偶问题数学公式推导。\u003Cbr\u002F\u003E所以想学好机器学习，还是要踏下心来，不仅仅要把原理的核心思想掌握了，公式推导也要好好学习才行。\u003C\u002Fblockquote\u003E\u003Ch2\u003E\u003Cb\u003E6.1 SVM 原理\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cblockquote data-pid=\"q9l3ayJJ\"\u003E简单粗暴的说：SVM 的思路就是找到一个超平面将数据集进行正确的分类。对于在现有维度不可分的数据，利用核函数映射到高纬空间使其线性可分。\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"xfLeWIyy\"\u003E支持向量机 SVM 是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。SVM 的学习策略是间隔最大化，可形式化为求解凸二次规划问题。\u003C\u002Fp\u003E\u003Cp data-pid=\"PSK7X9a1\"\u003ESVM 分为：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"qi6__Utm\"\u003E线性可分支持向量机。当训练数据线性可分时，通过硬间隔最大化，学习到的一个线性分类器；\u003C\u002Fli\u003E\u003Cli data-pid=\"rDrVbqCP\"\u003E线性支持向量机。当训练数据近似线性可分时，通过软间隔最大化，学习到的一个线性分类器；\u003C\u002Fli\u003E\u003Cli data-pid=\"z98PWFBx\"\u003E非线性支持向量机。当训练数据线性不可分，通过使用核技巧及软间隔最大化，学习非线性支持向量机。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-968c6fdab38d2c75be3cd81355993a3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"470\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-968c6fdab38d2c75be3cd81355993a3f_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;470&#39; height=&#39;407&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"470\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-968c6fdab38d2c75be3cd81355993a3f_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-968c6fdab38d2c75be3cd81355993a3f_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"V5ggx1G9\"\u003E上图中，X 表示负例，O 表示正例。此时的训练数据可分，线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线。\u003C\u002Fp\u003E\u003Cp data-pid=\"P0etBsMO\"\u003E\u003Cb\u003E6.1.1 支持向量与间隔\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"NWvnBtDR\"\u003E支持向量：在线性可分的情况下，训练数据样本集中的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。\u003C\u002Fp\u003E\u003Cp data-pid=\"m4Fj2P1R\"\u003E函数间隔定义如下：\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e8219bc42d97aef5bb70a8e93427928_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1207\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"1207\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e8219bc42d97aef5bb70a8e93427928_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1207&#39; height=&#39;378&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1207\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1207\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e8219bc42d97aef5bb70a8e93427928_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-0e8219bc42d97aef5bb70a8e93427928_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"kVVrv5CP\"\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"\u002F\u003E 表示目标值，取值为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%2B1\" alt=\"+1\" eeimg=\"1\"\u002F\u003E 、 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=-1\" alt=\"-1\" eeimg=\"1\"\u002F\u003E . 函数间隔虽然可以表示分类预测的准确性以及确信度。但是有个不好的性质：只要成倍的改变 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w\" alt=\"w\" eeimg=\"1\"\u002F\u003E 和 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b\" alt=\"b\" eeimg=\"1\"\u002F\u003E ，虽然此时的超平面并没有改变，但是函数间隔会变大。\u003C\u002Fp\u003E\u003Cp data-pid=\"sZutUtOq\"\u003E所以我们想到了对超平面的法向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=w\" alt=\"w\" eeimg=\"1\"\u002F\u003E 施加一些约束，比如规范化，使得间隔确定，这就引出了几何间隔：\u003C\u002Fp\u003E\u003Cp data-pid=\"tweXFAkn\"\u003E支持向量学习的基本思想就是求解能够正确划分训练数据集并且几何间隔最大的分类超平面。\u003C\u002Fp\u003E\u003Cp data-pid=\"yfnzMhty\"\u003E\u003Cb\u003E6.1.2 对偶问题\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"gJvVAMS2\"\u003E为了求解线性可分支持向量机的最优化问题：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4d75bb1dffffd99371c903fadf8c7a16_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"589\" data-rawheight=\"157\" class=\"origin_image zh-lightbox-thumb\" width=\"589\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4d75bb1dffffd99371c903fadf8c7a16_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;589&#39; height=&#39;157&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"589\" data-rawheight=\"157\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"589\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4d75bb1dffffd99371c903fadf8c7a16_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4d75bb1dffffd99371c903fadf8c7a16_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"pdNjkQdR\"\u003E将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解，这就是线性可分支持向量机的对偶算法。\u003C\u002Fp\u003E\u003Cp data-pid=\"hqBYu6lm\"\u003E本来的算法也可以求解 SVM，但是之所以要用对偶问题来求解，优点是：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"Pb0VSPzU\"\u003E一是对偶问题往往更容易求解；\u003C\u002Fli\u003E\u003Cli data-pid=\"vz4RB_L2\"\u003E二是自然引入核函数，进而推广到非线性分类问题。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"W0DBPKiq\"\u003E\u003Cb\u003E6.1.3 核函数\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"L8emVsTx\"\u003E对于在原始空间中不可分的问题，在高维空间中是线性可分的。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-84cf4a913ef8a47d9c0cc5574ca134be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1225\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"1225\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-84cf4a913ef8a47d9c0cc5574ca134be_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1225&#39; height=&#39;487&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1225\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1225\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-84cf4a913ef8a47d9c0cc5574ca134be_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-84cf4a913ef8a47d9c0cc5574ca134be_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"XLgQGRix\"\u003E对于线性不可分的问题，使用核函数可以从原始空间映射到高纬空间，使得问题变得线性可分。核函数还可以使得在高维空间计算的内积在低维空间中通过一个函数来完成。\u003C\u002Fp\u003E\u003Cp data-pid=\"Eb524iwX\"\u003E常用的核函数有：高斯核、线性核、多项式核。\u003C\u002Fp\u003E\u003Cp data-pid=\"E6gkSCOr\"\u003E\u003Cb\u003E6.1.4 软间隔\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"dG6WO75n\"\u003E线性可分问题的支持向量机学习方法，对现行不可分训练数据是不适用的。所以讲间隔函数修改为软间隔，对于函数间隔，在其上加上一个松弛变量，使其满足大于等于 1。约束条件变为：\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6844f79f791fa9a5f890ec3f60b23ec2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"314\" data-rawheight=\"73\" class=\"content_image\" width=\"314\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;314&#39; height=&#39;73&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"314\" data-rawheight=\"73\" class=\"content_image lazy\" width=\"314\" data-actualsrc=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6844f79f791fa9a5f890ec3f60b23ec2_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E\u003Cb\u003E6.2 优缺点\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"vvsqI4ny\"\u003E缺点：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"2ETeQ_99\"\u003E时空开销比较大，训练时间长；\u003C\u002Fli\u003E\u003Cli data-pid=\"Q7gRsSuK\"\u003E核函数的选取比较难，主要靠经验。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp data-pid=\"Wqfvngf7\"\u003E优点：\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli data-pid=\"_ch23kB5\"\u003E在小训练集上往往得到比较好的结果；\u003C\u002Fli\u003E\u003Cli data-pid=\"yPx3HMiy\"\u003E使用核函数避开了高纬空间的复杂性；\u003C\u002Fli\u003E\u003Cli data-pid=\"Iy0VHmdy\"\u003E泛化能力强。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E6.3 LR和SVM的区别\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cul\u003E\u003Cli data-pid=\"Y6oQekGV\"\u003ELinear SVM和LR都是线性分类器\u003C\u002Fli\u003E\u003Cli data-pid=\"o45MYUfb\"\u003ELinear SVM和LR都是\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32655097\" class=\"internal\"\u003E判别模型\u003C\u002Fa\u003E \u003C\u002Fli\u003E\u003Cli data-pid=\"VxtZbAOe\"\u003ELinear SVM不直接依赖数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别处于极其不平衡的状态, 一般需要先对数据做平衡处理。\u003C\u002Fli\u003E\u003Cli data-pid=\"C0g1JLOY\"\u003ELinear SVM依赖数据表达的距离测度，所以需要对数据先做标准化；LR不受其影响\u003C\u002Fli\u003E\u003Cli data-pid=\"Oq9fthnZ\"\u003ELinear SVM依赖惩罚项的系数，实验中需要做\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32627500\" class=\"internal\"\u003E交叉验证\u003C\u002Fa\u003E\u003C\u002Fli\u003E\u003Cli data-pid=\"X9dDv6yg\"\u003ELinear SVM和LR的执行都会受到异常值的影响，其敏感程度而言，谁更好很难下明确结论。\u003C\u002Fli\u003E\u003Cli data-pid=\"R7LcxjFq\"\u003ELinear SVM和LR损失函数不同, LR为logloss, SVM为hinge loss. 而SVM中的\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=f%28x%29+%3D+%5Cmax%280%2C+1-x%29\" alt=\"f(x) = \\max(0, 1-x)\" eeimg=\"1\"\u002F\u003E称为\u003Cb\u003Ehinge loss\u003C\u002Fb\u003E。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Ch2\u003E\u003Cb\u003E七、朴素贝叶斯\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"XPAli87_\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=http%3A\u002F\u002Fwww.cnblogs.com\u002Fpinard\u002Fp\u002F6069267.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E推荐学习博客\u003C\u002Fa\u003E \u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E八、XGBoost\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f44a62ba70adf36e5a966fbe0e4054e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1020\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb\" width=\"1020\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f44a62ba70adf36e5a966fbe0e4054e0_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1020&#39; height=&#39;567&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1020\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1020\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f44a62ba70adf36e5a966fbe0e4054e0_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-f44a62ba70adf36e5a966fbe0e4054e0_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"hpjtBV5f\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fblog.csdn.net\u002Fa819825294\u002Farticle\u002Fdetails\u002F51206410\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003EXGBoost\u003C\u002Fa\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E九、利用 sklearn 进行实战\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch2\u003E\u003Cb\u003E参考资料\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"SDT5ReF6\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fbook.douban.com\u002Fsubject\u002F10590856\u002F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《统计学习方法》李航\u003C\u002Fa\u003E \u003C\u002Fp\u003E\u003Cp data-pid=\"7n5aNeoL\"\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fcs.nju.edu.cn\u002Fzhouzh\u002Fzhouzh.files\u002Fpublication\u002FMLbook2016.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E《机器学习》周志华\u003C\u002Fa\u003E \u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cblockquote data-pid=\"B1xzfAPe\"\u003E\u003Cb\u003E学习建议：多考虑各模型之间的优缺点，区别和适用范围。不要只知道用了哪些模型，而且知道为什么用这个模型。\u003C\u002Fb\u003E\u003C\u002Fblockquote\u003E\u003Cp data-pid=\"quclsr-D\"\u003E\u003Cb\u003E最后祝各位面试成功！并以下面一句话来勉励你我。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"d2-Yz98X\"\u003E\u003Cb\u003E                                                                            路漫漫其修远兮，吾将上下而求索。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E竞赛社区（\u003C\u002Fb\u003E数据竞赛的一站式服务\u003Cb\u003E）\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"rfVbOzFh\"\u003E近期我们公众号和国内的开源组织Datawhale还有杰少一起成立了一个数据竞赛知识星球，并且邀请了国内的很多知名实战高手和赛圈的大佬，在推出的三天中也已经有了\u003Cb\u003E500\u003C\u002Fb\u003E多的用户报名，如果你\u003Cb\u003E真的对实战感兴趣而且希望好好学习的话\u003C\u002Fb\u003E，欢迎通过扫描下面的二维码进行报名，这样可以帮助您省下9元的报名费用，\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fc123ff57408b2278d88f9448dbef70c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1488\" data-rawheight=\"874\" class=\"origin_image zh-lightbox-thumb\" width=\"1488\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fc123ff57408b2278d88f9448dbef70c_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1488&#39; height=&#39;874&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1488\" data-rawheight=\"874\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1488\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fc123ff57408b2278d88f9448dbef70c_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-fc123ff57408b2278d88f9448dbef70c_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch2\u003E知识星球嘉宾（部分）\u003C\u002Fh2\u003E\u003Cp data-pid=\"qAfSsWMr\"\u003E范晶晶：开源组织Datawhale创始人\u003C\u002Fp\u003E\u003Cp data-pid=\"85Diq4RS\"\u003E张 杰：南京大学LAMDA硕士，天池数据科学家，KDD2019全球亚军\u003C\u002Fp\u003E\u003Cp data-pid=\"k5la0bHt\"\u003E谈志旋：北京大学硕士，社交app算法负责人\u003C\u002Fp\u003E\u003Cp data-pid=\"2ABxJoRX\"\u003E刘 洋：在读博士，IJCAI\u002FKDD\u002FICME等顶会比赛前三，天池数据科学家\u003C\u002Fp\u003E\u003Cp data-pid=\"lgDuIxnk\"\u003E钱 乾：资深算法工程师，Kaggle Grand Master\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4517eea81d4193aed7c42942589602f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1020\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4517eea81d4193aed7c42942589602f4_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;750&#39; height=&#39;1020&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1020\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4517eea81d4193aed7c42942589602f4_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-4517eea81d4193aed7c42942589602f4_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E\u003Cb\u003E数据竞赛群\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Cp data-pid=\"4koZ8uvU\"\u003E\u003Cb\u003E为了将热爱机器学习的大家聚在一起，推荐大家一个“数据竞赛”交流学习群，进群可与行业top级人物交流，可获得很强势的各方资源，大家有需要的可以进群哦\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-273c54a25d20b912d440dadb4d9cdc50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"1265\" class=\"origin_image zh-lightbox-thumb\" width=\"956\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-273c54a25d20b912d440dadb4d9cdc50_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;956&#39; height=&#39;1265&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"1265\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"956\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-273c54a25d20b912d440dadb4d9cdc50_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-273c54a25d20b912d440dadb4d9cdc50_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp data-pid=\"RFoSwOaM\"\u003E\u003Cb\u003E一年半的竞赛经历，收获了两冠四亚一季的成绩。在这一年半，不仅坚持比赛，同时也坚持不断的分享。在我看来，分享是一个自我总结的一个过程。当然，这也是我与更多选手交流的一个平台，是一个相互学习提升的机会。愿我的分享能够帮助到你。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp data-pid=\"aK6JYsj8\"\u003E\u003Cb\u003E知乎专栏目的传播更多机器学习干货，数据竞赛方法。欢迎投稿！\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002FDataAI\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-77acd3125654ffe9bfd9784c30d422fa_ipico.jpg\" data-image-width=\"438\" data-image-height=\"438\" class=\"internal\"\u003EML理论&amp;实践\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fshuma\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-f250325aeb589be7eff01a3e2d67dfbd_ipico.jpg\" data-image-width=\"200\" data-image-height=\"200\" class=\"internal\"\u003E数与码\u003C\u002Fa\u003E\u003Cp data-pid=\"1jVpoK5l\"\u003E\u003Cb\u003E路漫漫其修远兮，吾将上下而求索。\u003C\u002Fb\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19553534","type":"topic","id":"19553534","name":"数据挖掘"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20003862","type":"topic","id":"20003862","name":"Kaggle"}],"voteupCount":925,"voting":0,"heavyUpStatus":"allow_heavy_up","column":{"description":"公众号【Coggle数据科学】专注算法竞赛实战分享","canManage":false,"intro":"公众号【Coggle数据科学】专注算法竞赛实战分享","isFollowing":false,"urlToken":"DataAI","id":"DataAI","articlesCount":219,"acceptSubmission":true,"title":"机器学习理论与数据竞赛实战","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002FDataAI","commentPermission":"all","created":1514428685,"updated":1599148673,"imageUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-77acd3125654ffe9bfd9784c30d422fa_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5.jpg?source=172ae18b","uid":"555381879923224576","userType":"people","isFollowing":false,"urlToken":"wang-he-13-93","id":"09972d575614baad89afdaf4aa135905","description":"一对一学习指导，可私信我。\n数据算法竞赛爱好者，国内竞赛方案最佳分享者，目前已获得五冠六亚一季的成绩。\n2022，WSDM-xmRec cup，亚军\n2020，腾讯广告算法大赛，冠军\n2020，TIANCHI-数字中国创新大赛-智慧海洋建设，冠军\n2019，TIANCHI-全球数据智能大赛【赛场二】，亚军\n2019，TIANCHI-安泰杯--跨境电商智能算法大赛，冠军\n2019，腾讯广告算法大赛，冠军\n2019，KDD Cup: Context-Aware Multi-Modal Transportation Recommendation，亚军\n2018，科大讯飞营销算法大赛，冠军\n2019，TIANCHI-OGeek算法挑战赛，亚军\n2019，JDATA-用户对品类下店铺的购买预测，亚军\n2019，第四届魔镜杯大赛数据应用大赛，亚军\n2019，TIANCHI-全球城市AI挑战赛，季军","name":"鱼遇雨欲语与余","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F09972d575614baad89afdaf4aa135905","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":13621,"type":"column"},"commentCount":47,"contributions":[{"id":1109734,"state":"accepted","type":"first_publish","column":{"description":"公众号【Coggle数据科学】专注算法竞赛实战分享","canManage":false,"intro":"公众号【Coggle数据科学】专注算法竞赛实战分享","isFollowing":false,"urlToken":"DataAI","id":"DataAI","articlesCount":219,"acceptSubmission":true,"title":"机器学习理论与数据竞赛实战","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002FDataAI","commentPermission":"all","created":1514428685,"updated":1599148673,"imageUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-77acd3125654ffe9bfd9784c30d422fa_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5.jpg?source=172ae18b","uid":"555381879923224576","userType":"people","isFollowing":false,"urlToken":"wang-he-13-93","id":"09972d575614baad89afdaf4aa135905","description":"一对一学习指导，可私信我。\n数据算法竞赛爱好者，国内竞赛方案最佳分享者，目前已获得五冠六亚一季的成绩。\n2022，WSDM-xmRec cup，亚军\n2020，腾讯广告算法大赛，冠军\n2020，TIANCHI-数字中国创新大赛-智慧海洋建设，冠军\n2019，TIANCHI-全球数据智能大赛【赛场二】，亚军\n2019，TIANCHI-安泰杯--跨境电商智能算法大赛，冠军\n2019，腾讯广告算法大赛，冠军\n2019，KDD Cup: Context-Aware Multi-Modal Transportation Recommendation，亚军\n2018，科大讯飞营销算法大赛，冠军\n2019，TIANCHI-OGeek算法挑战赛，亚军\n2019，JDATA-用户对品类下店铺的购买预测，亚军\n2019，第四届魔镜杯大赛数据应用大赛，亚军\n2019，TIANCHI-全球城市AI挑战赛，季军","name":"鱼遇雨欲语与余","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F09972d575614baad89afdaf4aa135905","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":13621,"type":"column"}},{"id":1748966,"state":"accepted","type":"include","column":{"description":"","canManage":false,"intro":"文章首发公众号“每天都要机器学习”","isFollowing":false,"urlToken":"leemoo","id":"leemoo","articlesCount":53,"acceptSubmission":true,"title":"每天都要机器学习哦","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fleemoo","commentPermission":"all","created":1507449667,"updated":1603966688,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8456af8026d9b1d75719eab1971a3662_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7c367815a3f8d0fe48f2d5c3b2751d57.jpg?source=172ae18b","uid":"61977426657280","userType":"people","isFollowing":false,"urlToken":"wangle-nlp","id":"1b7326a564c30474efbc1f85c5af40a9","description":"","name":"王乐","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F1b7326a564c30474efbc1f85c5af40a9","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7c367815a3f8d0fe48f2d5c3b2751d57_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":3756,"type":"column"}},{"id":1856307,"state":"accepted","type":"include","column":{"description":"微信公众号《R语言中文社区》","canManage":false,"intro":"公众号：R语言中文社区｜QQ群：612885032","isFollowing":false,"urlToken":"Ryuyanshequ","id":"Ryuyanshequ","articlesCount":777,"acceptSubmission":true,"title":"中国R语言社区","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002FRyuyanshequ","commentPermission":"all","created":1493799657,"updated":1650680915,"imageUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-92f42b355edc10af5750445aa9897f58_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bee0fd670cf5ff5e6ae7622bca9baf4c.jpg?source=172ae18b","uid":"799378118749089792","userType":"people","isFollowing":false,"urlToken":"lancheo","id":"d66db5603030413f255cb4f52f1cdab1","description":"赋能商业决策，关注业务价值创造！V：huangxiaowei_data 邮箱：huangxiaowei_china@163.com","name":"黄小伟Yeah","isAdvertiser":false,"headline":"公众号：读数  | 脉脉@黄小伟","gender":1,"url":"\u002Fpeople\u002Fd66db5603030413f255cb4f52f1cdab1","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-bee0fd670cf5ff5e6ae7622bca9baf4c_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":10994,"type":"column"}},{"id":30411512,"state":"accepted","type":"first_publish","column":{"description":"","canManage":false,"intro":"智力题、数据结构、机器学习、深度学习、面经等","isFollowing":false,"urlToken":"c_1317403986237448192","id":"c_1317403986237448192","articlesCount":45,"acceptSubmission":true,"title":"算法工程师面试汇总","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1317403986237448192","commentPermission":"all","created":1606873586,"updated":1643600408,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736.jpg?source=172ae18b","uid":"748889366583074816","userType":"people","isFollowing":false,"urlToken":"Sing-F","id":"4c7d0fa5cfea8b2f3e201cc74b5d02e6","description":"GNN，推荐，广告，搜索, KG, NLP, CV","name":"郭达森","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F4c7d0fa5cfea8b2f3e201cc74b5d02e6","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":17,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"favlistsCount":3058,"isNormal":true,"status":0,"shareText":"机器学习面试干货精讲 - 来自知乎专栏「机器学习理论与数据竞赛实战」，作者: 鱼遇雨欲语与余 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32877396 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":51,"hasColumn":true,"republishers":[{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736.jpg?source=172ae18b","uid":"748889366583074816","userType":"people","isFollowing":false,"urlToken":"Sing-F","id":"4c7d0fa5cfea8b2f3e201cc74b5d02e6","description":"GNN，推荐，广告，搜索, KG, NLP, CV","name":"郭达森","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F4c7d0fa5cfea8b2f3e201cc74b5d02e6","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736_l.jpg?source=172ae18b","isOrg":false,"type":"people"}],"isNewLinkCard":true,"emojiReaction":{"cryFaceCount":0,"cryFaceHasSet":false,"hugCount":0,"hugHasSet":false,"likeCount":51,"likeHasSet":false,"onlookerCount":0,"onlookerHasSet":false},"abParam":{"shangyebiaoshi":"1"},"attachedInfo":"kgIhCgc1MzQwMzcxEggzMjg3NzM5NhgHIgpJTUFHRV9URVhU","shareGuide":{"hasPositiveBubble":false,"hasTimeBubble":false,"hitShareGuideCluster":false},"settings":{"tableOfContents":{"enabled":false}},"canReference":false,"reactionInstruction":{}}},"columns":{"DataAI":{"description":"公众号【Coggle数据科学】专注算法竞赛实战分享","canManage":false,"intro":"公众号【Coggle数据科学】专注算法竞赛实战分享","isFollowing":false,"urlToken":"DataAI","id":"DataAI","articlesCount":219,"acceptSubmission":true,"title":"机器学习理论与数据竞赛实战","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002FDataAI","commentPermission":"all","created":1514428685,"updated":1599148673,"imageUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-77acd3125654ffe9bfd9784c30d422fa_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5.jpg?source=172ae18b","uid":"555381879923224576","userType":"people","isFollowing":false,"urlToken":"wang-he-13-93","id":"09972d575614baad89afdaf4aa135905","description":"一对一学习指导，可私信我。\n数据算法竞赛爱好者，国内竞赛方案最佳分享者，目前已获得五冠六亚一季的成绩。\n2022，WSDM-xmRec cup，亚军\n2020，腾讯广告算法大赛，冠军\n2020，TIANCHI-数字中国创新大赛-智慧海洋建设，冠军\n2019，TIANCHI-全球数据智能大赛【赛场二】，亚军\n2019，TIANCHI-安泰杯--跨境电商智能算法大赛，冠军\n2019，腾讯广告算法大赛，冠军\n2019，KDD Cup: Context-Aware Multi-Modal Transportation Recommendation，亚军\n2018，科大讯飞营销算法大赛，冠军\n2019，TIANCHI-OGeek算法挑战赛，亚军\n2019，JDATA-用户对品类下店铺的购买预测，亚军\n2019，第四届魔镜杯大赛数据应用大赛，亚军\n2019，TIANCHI-全球城市AI挑战赛，季军","name":"鱼遇雨欲语与余","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F09972d575614baad89afdaf4aa135905","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-b4426a5857d5196b5b1ef16384ab4ed5_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":13621,"type":"column"},"leemoo":{"description":"","canManage":false,"intro":"文章首发公众号“每天都要机器学习”","isFollowing":false,"urlToken":"leemoo","id":"leemoo","articlesCount":53,"acceptSubmission":true,"title":"每天都要机器学习哦","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fleemoo","commentPermission":"all","created":1507449667,"updated":1603966688,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8456af8026d9b1d75719eab1971a3662_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7c367815a3f8d0fe48f2d5c3b2751d57.jpg?source=172ae18b","uid":"61977426657280","userType":"people","isFollowing":false,"urlToken":"wangle-nlp","id":"1b7326a564c30474efbc1f85c5af40a9","description":"","name":"王乐","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F1b7326a564c30474efbc1f85c5af40a9","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-7c367815a3f8d0fe48f2d5c3b2751d57_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":3756,"type":"column"},"Ryuyanshequ":{"description":"微信公众号《R语言中文社区》","canManage":false,"intro":"公众号：R语言中文社区｜QQ群：612885032","isFollowing":false,"urlToken":"Ryuyanshequ","id":"Ryuyanshequ","articlesCount":777,"acceptSubmission":true,"title":"中国R语言社区","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002FRyuyanshequ","commentPermission":"all","created":1493799657,"updated":1650680915,"imageUrl":"https:\u002F\u002Fpica.zhimg.com\u002Fv2-92f42b355edc10af5750445aa9897f58_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-bee0fd670cf5ff5e6ae7622bca9baf4c.jpg?source=172ae18b","uid":"799378118749089792","userType":"people","isFollowing":false,"urlToken":"lancheo","id":"d66db5603030413f255cb4f52f1cdab1","description":"赋能商业决策，关注业务价值创造！V：huangxiaowei_data 邮箱：huangxiaowei_china@163.com","name":"黄小伟Yeah","isAdvertiser":false,"headline":"公众号：读数  | 脉脉@黄小伟","gender":1,"url":"\u002Fpeople\u002Fd66db5603030413f255cb4f52f1cdab1","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-bee0fd670cf5ff5e6ae7622bca9baf4c_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":10994,"type":"column"},"c_1317403986237448192":{"description":"","canManage":false,"intro":"智力题、数据结构、机器学习、深度学习、面经等","isFollowing":false,"urlToken":"c_1317403986237448192","id":"c_1317403986237448192","articlesCount":45,"acceptSubmission":true,"title":"算法工程师面试汇总","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1317403986237448192","commentPermission":"all","created":1606873586,"updated":1643600408,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736.jpg?source=172ae18b","uid":"748889366583074816","userType":"people","isFollowing":false,"urlToken":"Sing-F","id":"4c7d0fa5cfea8b2f3e201cc74b5d02e6","description":"GNN，推荐，广告，搜索, KG, NLP, CV","name":"郭达森","isAdvertiser":false,"headline":"","gender":1,"url":"\u002Fpeople\u002F4c7d0fa5cfea8b2f3e201cc74b5d02e6","avatarUrl":"https:\u002F\u002Fpicd.zhimg.com\u002Fv2-25deab0d86c6a5c67cb5803494771736_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":17,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{},"zvideos":{},"zvideoContributions":{},"briefs":{},"eduCourses":{}},"currentUser":"5b6cbee5d01bbbae90228c8baf07c3aa","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false},"cardUserInfo":{"vipInfo":{}},"handleWidget":{},"widgetList":[],"userWidgetId":""},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{},"batchUsers":{},"profileInfinity":null},"env":{"ab":{"config":{"params":[{"id":"vessay_v2_sdk","type":"Int","value":"1","layerId":"Qtkm"},{"id":"pc_ppt_publish","type":"Int","value":"1","layerId":"pc_ppt_publish"},{"id":"helpcenter_pc","type":"Int","value":"1","layerId":"helpcenter_pc"},{"id":"pc_comment","type":"Int","value":"2","layerId":"EsOR"},{"id":"pc_follow","type":"Int","value":"1","layerId":"pc_follow"},{"id":"pc_player_rp","type":"Int","value":"0","layerId":"pc_player_rp"},{"id":"use_biz_comment","type":"Int","value":"1","layerId":"use_biz_comment"},{"id":"pc_pin","type":"Int","value":"0","layerId":"pc_pin"},{"id":"player_vendors","type":"Int","value":"0","layerId":"player_vendors"},{"id":"report_page","type":"Int","value":"0","layerId":"report_page"},{"id":"img_notwell","type":"Int","value":"0","layerId":"img_notwell"},{"id":"update_react","type":"Int","value":"0","layerId":"update_react"}],"experiments":[{"expId":"pc_ppt_publish-2_v2","expPrefix":"pc_ppt_publish","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"helpcenter_pc-2_v11","expPrefix":"helpcenter_pc","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"pc_comment-3_v4","expPrefix":"pc_comment","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"pc_follow-3_v5","expPrefix":"pc_follow","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"pc_player_rp-1_v8","expPrefix":"pc_player_rp","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"use_biz_comment-2_v5","expPrefix":"use_biz_comment","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"player_vendors-1_v8","expPrefix":"player_vendors","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"update_react-1_v3","expPrefix":"update_react","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false}],"chains":[{"chainId":"_all_"}],"encodedParams":"CsIBCAAbAD8ARwC0AGkBagF0ATsCzALXAtgCTwNQA6ADoQOiA7cD8wP0AzMEjASNBKYE1gQRBVEFiwWMBZ4FMAYxBusGJwd3B3gH2AfcB90HZwh0CHYIeQjaCD8JQglgCY0JwwnECcUJxgnHCcgJyQnKCcsJzAnRCfQJBApJCmUKawqYCqUKqQq+CsQK1ArdCu0K\u002FQr+CjsLPAtDC0YLcQt2C4ULhwuNC8AL1wvgC+UL5gssDDgMcQyPDKwMuQzDDMkM+AwSYQIAAAAAAgEAAQUAAAAAAAAAAAAAAAQEAAQAAQAAAQAAAAAAAAACAgQAAQYAAAEBAAAAAAAAAAAAAAAAAwAAAAABAAAAAQEAAAAAAQABAAAAAAAFAAIBAAAGAgYAAAECAAA="},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Quark":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F97.0.4692.71 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F32877396","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F32877396","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":true,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"baiduSearch":false,"googleSearch":false,"shenma":false,"miniProgram":false,"xiaomi":false},"theme":"light","appHeaderTheme":{"current":"normal","disable":true,"normal":{"bgColor":"GBK99A"},"custom":{"bgColor":"GBK99A"}},"enableShortcut":true,"referer":"","xUDId":"ADAftiQeYhWPTusQ0SNqNQTYqRvo8pEiKds=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"北京","countryName":"中国","regionName":"北京","countryCode":"CN"},"logged":true,"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{},"rights":[],"newRights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{},"videoSupport":{},"textBenefit":{},"mcnManage":{},"tasks":{},"newTasks":{"creatorTask":{"tasks":[],"des":[]}},"scoreInfo":{},"recentlyCreated":[],"announcement":{},"bannerList":[],"creatorsRecommendInfo":{},"menusShowControlByServer":{"bVipRecomend":false,"creationRelationship":false},"income":{"aggregation":{}},"editModal":{"status":false}},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"entityWords":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{},"concernedUpvoters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"room":{"meta":{},"isFetching":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}},"banner":{},"commercialBanner":{"show":false,"banner":{},"trackData":{}},"video":{"items":[],"next":null,"isLoading":false,"isDrained":false}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"DataAI","leemoo","Ryuyanshequ","c_1317403986237448192"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0,"infinity":0},"visible":false}},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"zvideos":{"campaignVideoList":{},"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"club":{},"qyActivityData":{},"talkActivityData":{},"party2022ActivityData":{},"batchVideos":{},"contribution":{"selectedContribution":null,"campaign":null,"configs":{},"contributionLists":{},"recommendQuestions":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]},"questionSearchResults":{"isLoading":true,"paging":{"isEnd":false,"isStart":true,"totals":0},"data":[]}},"creationReferences":{},"zvideoCollection":{},"zvideoGrant":{},"collectData":{"isFetching":false,"list":[]},"videoSource":{"isLoaded":false}},"republish":{},"commentPermission":{},"creatorRightStatus":{"list":[]},"adPromotion":{"answer":{},"article":{}}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script crossorigin="" src="https://static.zhihu.com/heifetz/vendor.a49f6a20b8521c39871a.js"></script><script crossorigin="" src="https://unpkg.zhimg.com/react@16.14.0/umd/react.production.min.js"></script><script crossorigin="" src="https://unpkg.zhimg.com/react-dom@16.14.0/umd/react-dom.production.min.js"></script><script crossorigin="" src="https://unpkg.zhimg.com/react-dom@16.14.0/umd/react-dom-server.browser.production.min.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.lib_0e5ce61e.e15f8b3774e01ad26c97.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.lib_6efc30be.7e5f573d64f5b04f2b42.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.lib_b3b2c8c3.50b0bd326567de8e3b6a.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.lib_330004dc.9697633a4900d4a3e8e1.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.lib_79b5cf47.ceab09413d36430dcdc1.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.lib_29107295.0229f4908688d743e90b.js"></script><script crossorigin="" src="https://static.zhihu.com/heifetz/column.app.480d902224a19d50436e.js"></script><script defer="" src="https://static.zhihu.com/event/wza/31035/aria.js?appid=a3637ace5dc3a347f6863b0bac487599"></script><script src="https://hm.baidu.com/hm.js?98beee57fd2ef70ccdd5ca52b9740c49" async=""></script><script crossorigin="" src="https://unpkg.zhimg.com/za-js-sdk@4.0.0/dist/zap.js"></script><div><div style="display: none;"><i>想来知乎工作？请发送邮件到 jobs@zhihu.com</i></div></div><script src="https://zz.bdstatic.com/linksubmit/push.js"></script><div><div><div class="css-8pdeid"></div></div></div><script crossorigin="" src="https://pay.zhihu.com/api/js"></script><script crossorigin="" src="https://unpkg.zhimg.com/@cfe/emoticon@1.2.4/lib/emoticon.js"></script><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete7-0" id="Popover6-toggle" aria-haspopup="true" aria-owns="Popover6-content" class="Input" placeholder="选择语言" value=""><svg width="24" height="24" viewBox="0 0 24 24" fill="#afbdcf" class="Zi Zi--Select"><path fill-rule="evenodd" d="M12.53 3.47a.75.75 0 0 0-1.06 0l-5 5a.75.75 0 0 0 1.06 1.06L12 5.06l4.47 4.47a.75.75 0 1 0 1.06-1.06l-5-5Zm-5 11a.75.75 0 0 0-1.06 1.06l5 5a.75.75 0 0 0 1.06 0l5-5a.75.75 0 1 0-1.06-1.06L12 18.94l-4.47-4.47Z" clip-rule="evenodd"></path></svg></label></div></div></div></div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete9-0" id="Popover8-toggle" aria-haspopup="true" aria-owns="Popover8-content" class="Input" placeholder="选择语言" value=""><svg width="24" height="24" viewBox="0 0 24 24" fill="#afbdcf" class="Zi Zi--Select"><path fill-rule="evenodd" d="M12.53 3.47a.75.75 0 0 0-1.06 0l-5 5a.75.75 0 0 0 1.06 1.06L12 5.06l4.47 4.47a.75.75 0 1 0 1.06-1.06l-5-5Zm-5 11a.75.75 0 0 0-1.06 1.06l5 5a.75.75 0 0 0 1.06 0l5-5a.75.75 0 1 0-1.06-1.06L12 18.94l-4.47-4.47Z" clip-rule="evenodd"></path></svg></label></div></div></div></div></div></body></html>